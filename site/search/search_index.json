{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Welcome to the Amazon EKS SSP Quickstart documentation site. Amazon EKS SSP Quickstart provides a framework and methodology that makes it easy for customers build Shared Services Platform (SSP) on top of Amazon EKS . What is an SSP? \u00b6 A Shared Services Platform (SSP) is an interenal development platform that abstracts the complexities of cloud infrastrucuture from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise. What can I do with the Quickstart? \u00b6 Customers can use Amazon EKS SSP Quickstart to: Deploy batteries included EKS clusters across multiple accounts and regions. Manage configuration for all of your cluster from a single Git repository. Manage the set of addons that are provisioned in each cluster. Leverage Gitops-based workflows to onboard and manage workloads. Define teams, namespaces, and their associated access permissions. Integrate cluster access with IAM or OIDC provider of your choosing.","title":"Overview"},{"location":"#overview","text":"Welcome to the Amazon EKS SSP Quickstart documentation site. Amazon EKS SSP Quickstart provides a framework and methodology that makes it easy for customers build Shared Services Platform (SSP) on top of Amazon EKS .","title":"Overview"},{"location":"#what-is-an-ssp","text":"A Shared Services Platform (SSP) is an interenal development platform that abstracts the complexities of cloud infrastrucuture from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise.","title":"What is an SSP?"},{"location":"#what-can-i-do-with-the-quickstart","text":"Customers can use Amazon EKS SSP Quickstart to: Deploy batteries included EKS clusters across multiple accounts and regions. Manage configuration for all of your cluster from a single Git repository. Manage the set of addons that are provisioned in each cluster. Leverage Gitops-based workflows to onboard and manage workloads. Define teams, namespaces, and their associated access permissions. Integrate cluster access with IAM or OIDC provider of your choosing.","title":"What can I do with the Quickstart?"},{"location":"ci-cd/","text":"CI/CD \u00b6 IaC Pipeline \u00b6 (work in progress) Example of IaC self-mutating pipeline based on CodePipeline can be found in the lib/pipelineStack.ts .","title":"CI/CD"},{"location":"ci-cd/#cicd","text":"","title":"CI/CD"},{"location":"ci-cd/#iac-pipeline","text":"(work in progress) Example of IaC self-mutating pipeline based on CodePipeline can be found in the lib/pipelineStack.ts .","title":"IaC Pipeline"},{"location":"cluster-management/","text":"// TODO","title":"Cluster Management"},{"location":"core-concepts/","text":"","title":"Core Concepts"},{"location":"getting-started/","text":"Getting Started \u00b6 This getting started guide will walk you through setting up a new CDK project which leverages the cdk-eks-blueprint NPM module to deploy a simple SSP. Project setup \u00b6 The quickstart leverages AWS Cloud Development Kit (CDK) . Install CDK via the following. npm install -g aws-cdk Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap aws://<AWS_ACCOUNT_ID>/<AWS_REGION> Deploy a Blueprint EKS Cluster \u00b6 Install the cdk-eks-blueprint NPM package via the following. npm i @shapirov/cdk-eks-blueprint Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBlueprint , AddOns } from '@shapirov/cdk-eks-blueprint' ; const addOns : Array < ClusterAddOn > = [ new AddOns . ArgoCDAddOn , ]; const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy Congratulations! You have deployed your first EKS cluster with cdk-eks-blueprint . Onboard a Team \u00b6 Now that we have our cluster deployed, it is time to onboard our first team. In its simplest form, a team is represented in a Kubernetes cluster by a namespace. In order to create a new team and namespace within your cluster do the following: Create a new file called team-awesome . touch team-awesome.tsx Paste the following code the file. import { ClusterInfo , ApplicationTeam } from '../../stacks/eks-blueprint-stack' ; export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } Replace the contents of bin/main.tsx with the following: import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBluepint , AddOns , Team } from '@shapirov/cdk-eks-blueprint' ; import { TeamAwesome } from '../teams/team-awesome' const addOns : Array < ClusterAddOn > = [ new AddOns . ArgoCDAddOn , ]; const teams : Array < Team > = [ new TeamAwesome ] const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns , teams } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy To confirm your team has been added to the cluster, run: kubectl get ns team-awesome Deploy workloads with ArgoCD \u00b6 Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads arcoss multiple namespaces. The sample app of apps repository that we use in this walkthrough can be found here . Install ArgoCD CLI \u00b6 Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd : v2 . 0.1 + 33 eaf11 . dirty Exposing ArgoCD \u00b6 To access the ArgoCD running in your Kubernetes cluster, we simply need leverage port-forwarding. To do so, first capture the service name in an environment variable. export ARGO_SERVER =$ ( kubectl get svc - n argocd - l app . kubernetes . io / name = argocd - server - o name ) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the Argo login screen. Logging Into ArgoCD \u00b6 ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD =$ ( kubectl - n argocd get secret argocd - initial - admin - secret - o jsonpath = \"{.data.password}\" | base64 - d ) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD Deploy workloads to your cluster \u00b6 Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/kcoleman731/argo-apps.git Create the application within Argo by running the following command argocd app create sample-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/kcoleman731/argo-apps.git \\ --path \".\" Sync the apps by running the following command argocd app sync sample-apps Validate deployments. \u00b6 To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-burnham . kubectl port-forward svc/guestbook-ui -n team-burnham 4040:80 Open up localhost:4040 in your browser and you should see the application.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This getting started guide will walk you through setting up a new CDK project which leverages the cdk-eks-blueprint NPM module to deploy a simple SSP.","title":"Getting Started"},{"location":"getting-started/#project-setup","text":"The quickstart leverages AWS Cloud Development Kit (CDK) . Install CDK via the following. npm install -g aws-cdk Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap aws://<AWS_ACCOUNT_ID>/<AWS_REGION>","title":"Project setup"},{"location":"getting-started/#deploy-a-blueprint-eks-cluster","text":"Install the cdk-eks-blueprint NPM package via the following. npm i @shapirov/cdk-eks-blueprint Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBlueprint , AddOns } from '@shapirov/cdk-eks-blueprint' ; const addOns : Array < ClusterAddOn > = [ new AddOns . ArgoCDAddOn , ]; const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy Congratulations! You have deployed your first EKS cluster with cdk-eks-blueprint .","title":"Deploy a Blueprint EKS Cluster"},{"location":"getting-started/#onboard-a-team","text":"Now that we have our cluster deployed, it is time to onboard our first team. In its simplest form, a team is represented in a Kubernetes cluster by a namespace. In order to create a new team and namespace within your cluster do the following: Create a new file called team-awesome . touch team-awesome.tsx Paste the following code the file. import { ClusterInfo , ApplicationTeam } from '../../stacks/eks-blueprint-stack' ; export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } Replace the contents of bin/main.tsx with the following: import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBluepint , AddOns , Team } from '@shapirov/cdk-eks-blueprint' ; import { TeamAwesome } from '../teams/team-awesome' const addOns : Array < ClusterAddOn > = [ new AddOns . ArgoCDAddOn , ]; const teams : Array < Team > = [ new TeamAwesome ] const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns , teams } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy To confirm your team has been added to the cluster, run: kubectl get ns team-awesome","title":"Onboard a Team"},{"location":"getting-started/#deploy-workloads-with-argocd","text":"Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads arcoss multiple namespaces. The sample app of apps repository that we use in this walkthrough can be found here .","title":"Deploy workloads with ArgoCD"},{"location":"getting-started/#install-argocd-cli","text":"Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd : v2 . 0.1 + 33 eaf11 . dirty","title":"Install ArgoCD CLI"},{"location":"getting-started/#exposing-argocd","text":"To access the ArgoCD running in your Kubernetes cluster, we simply need leverage port-forwarding. To do so, first capture the service name in an environment variable. export ARGO_SERVER =$ ( kubectl get svc - n argocd - l app . kubernetes . io / name = argocd - server - o name ) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the Argo login screen.","title":"Exposing ArgoCD"},{"location":"getting-started/#logging-into-argocd","text":"ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD =$ ( kubectl - n argocd get secret argocd - initial - admin - secret - o jsonpath = \"{.data.password}\" | base64 - d ) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD","title":"Logging Into ArgoCD"},{"location":"getting-started/#deploy-workloads-to-your-cluster","text":"Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/kcoleman731/argo-apps.git Create the application within Argo by running the following command argocd app create sample-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/kcoleman731/argo-apps.git \\ --path \".\" Sync the apps by running the following command argocd app sync sample-apps","title":"Deploy workloads to your cluster"},{"location":"getting-started/#validate-deployments","text":"To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-burnham . kubectl port-forward svc/guestbook-ui -n team-burnham 4040:80 Open up localhost:4040 in your browser and you should see the application.","title":"Validate deployments."},{"location":"teams/","text":"Teams \u00b6 The cdk-eks-blueprint framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team . ApplicationTeam \u00b6 To create an ApplicationTeam for your cluster, simplye implement a class that extends ApplicationTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace. PlatformTeam \u00b6 To create an PlatformTeam for your cluster, simplye implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRole : Role.fromRoleArn ( ` ${ YOUR_ROLE_ARN } ` ); }) DefaultTeamRoles \u00b6 The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services Team Benefits \u00b6 By managing teams via infrastrucutre as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access. Cluster Access ( kubectl ) \u00b6 The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${ teamname } teamrole arn:aws:iam:: ${ account } :role/west-dev- ${ teamname } AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam:: ${ account } :role/west-dev- ${ platform - team - name } AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam:: ${ account } :role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam:: ${ account } :role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example. Console Access \u00b6 Provided that each team has recieved the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team. Examples \u00b6 There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Teams"},{"location":"teams/#teams","text":"The cdk-eks-blueprint framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team .","title":"Teams"},{"location":"teams/#applicationteam","text":"To create an ApplicationTeam for your cluster, simplye implement a class that extends ApplicationTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace.","title":"ApplicationTeam"},{"location":"teams/#platformteam","text":"To create an PlatformTeam for your cluster, simplye implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRole : Role.fromRoleArn ( ` ${ YOUR_ROLE_ARN } ` ); })","title":"PlatformTeam"},{"location":"teams/#defaultteamroles","text":"The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services","title":"DefaultTeamRoles"},{"location":"teams/#team-benefits","text":"By managing teams via infrastrucutre as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access.","title":"Team Benefits"},{"location":"teams/#cluster-access-kubectl","text":"The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${ teamname } teamrole arn:aws:iam:: ${ account } :role/west-dev- ${ teamname } AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam:: ${ account } :role/west-dev- ${ platform - team - name } AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam:: ${ account } :role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam:: ${ account } :role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example.","title":"Cluster Access (kubectl)"},{"location":"teams/#console-access","text":"Provided that each team has recieved the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team.","title":"Console Access"},{"location":"teams/#examples","text":"There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Examples"},{"location":"addons/cluster-autoscaler/","text":"Cluster Autoscaler Add-on \u00b6 Usage \u00b6 import { ClusterAutoScaler } from '@shapirov/cdk-eks-blueprint' ; readonly myClusterAutoscaler = new ClusterAutoscaler ( \"v1.19.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Functionality \u00b6 Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances.","title":"Cluster Autoscaler"},{"location":"addons/cluster-autoscaler/#cluster-autoscaler-add-on","text":"","title":"Cluster Autoscaler Add-on"},{"location":"addons/cluster-autoscaler/#usage","text":"import { ClusterAutoScaler } from '@shapirov/cdk-eks-blueprint' ; readonly myClusterAutoscaler = new ClusterAutoscaler ( \"v1.19.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/cluster-autoscaler/#functionality","text":"Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances.","title":"Functionality"},{"location":"addons/container-insights/","text":"Container Insights Addon \u00b6 The ContainerInsights addon adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing. Usage \u00b6 Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import { AddOns } from '@shapirov/cdk-eks-blueprint' ; const myClusterAutoscaler = new AddOns . ContainerInsights (); const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Next run cdk deploy to update your CDK stack. Prerequisites \u00b6 Once the Container Insights add-on has been installed to your cluster check to see that the CloudWatch Agent and the FluentD daemons are running. Run kubectl get all -n amazon-cloudwatch and you should see the following output. NAME READY STATUS RESTARTS AGE pod/cloudwatch-agent-k8wxl 1/1 Running 0 105s pod/fluentd-cloudwatch-78zv4 1/1 Running 0 105s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cloudwatch-agent 1 1 1 1 1 <none> 107s daemonset.apps/fluentd-cloudwatch 1 1 1 1 1 <none> 106s To enable or disable control plane logs with the console, please run the following command in your terminal - aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command - aws eks describe-update \\ --region <region-code>\\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } View metrics for cluster and workloads \u00b6 Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance. View cluster level logs \u00b6 After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane. View workload level logs \u00b6 In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax. View containers via that container map in container insights. \u00b6 In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"Container Insights"},{"location":"addons/container-insights/#container-insights-addon","text":"The ContainerInsights addon adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing.","title":"Container Insights Addon"},{"location":"addons/container-insights/#usage","text":"Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import { AddOns } from '@shapirov/cdk-eks-blueprint' ; const myClusterAutoscaler = new AddOns . ContainerInsights (); const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Next run cdk deploy to update your CDK stack.","title":"Usage"},{"location":"addons/container-insights/#prerequisites","text":"Once the Container Insights add-on has been installed to your cluster check to see that the CloudWatch Agent and the FluentD daemons are running. Run kubectl get all -n amazon-cloudwatch and you should see the following output. NAME READY STATUS RESTARTS AGE pod/cloudwatch-agent-k8wxl 1/1 Running 0 105s pod/fluentd-cloudwatch-78zv4 1/1 Running 0 105s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cloudwatch-agent 1 1 1 1 1 <none> 107s daemonset.apps/fluentd-cloudwatch 1 1 1 1 1 <none> 106s To enable or disable control plane logs with the console, please run the following command in your terminal - aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command - aws eks describe-update \\ --region <region-code>\\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } }","title":"Prerequisites"},{"location":"addons/container-insights/#view-metrics-for-cluster-and-workloads","text":"Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance.","title":"View metrics for cluster and workloads"},{"location":"addons/container-insights/#view-cluster-level-logs","text":"After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane.","title":"View cluster level logs"},{"location":"addons/container-insights/#view-workload-level-logs","text":"In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax.","title":"View workload level logs"},{"location":"addons/container-insights/#view-containers-via-that-container-map-in-container-insights","text":"In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"View containers via that container map in container insights."},{"location":"addons/overview/","text":"AddOns \u00b6 Supported AddOns \u00b6 AddOn Description AppMeshAddOn Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddOn Adds an ArgoCD controller CalicoAddOn Adds the Calico 1.7.1 CNI/Network policy engine ContainerInsightsAddOn Adds Container Insights support integrating monitoring with CloudWatch ClusterAutoscalerAddOn Adds the standard cluster autoscaler ( Karpenter is coming) MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddOn Adds NGINX ingress controller","title":"Overview"},{"location":"addons/overview/#addons","text":"","title":"AddOns"},{"location":"addons/overview/#supported-addons","text":"AddOn Description AppMeshAddOn Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddOn Adds an ArgoCD controller CalicoAddOn Adds the Calico 1.7.1 CNI/Network policy engine ContainerInsightsAddOn Adds Container Insights support integrating monitoring with CloudWatch ClusterAutoscalerAddOn Adds the standard cluster autoscaler ( Karpenter is coming) MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddOn Adds NGINX ingress controller","title":"Supported AddOns"},{"location":"cluster-providers/ec2-cluster-provider/","text":"EC2 Cluster Provider \u00b6 Stack Configuration \u00b6 Supports context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.instance-type : (defaulted to \"m5.large\") Type of instance for the EKS cluster, must be a valid instance type like t3.medium eks.default.vpc : Specifies whether to use an existing VPC (if specified) or create a new one if not specified. eks.default.min-size : Min cluster size, must be positive integer greater than 0 (default 1). eks.default.max-size : Max cluster size, must be greater than minSize (default 3). eks.default.desired-size : Desired cluster size, must be greater or equal to minSize (default min-size ). eks.default.vpc supports an option to look up default VPC in your account/region if the value is set to default . Note, there should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass EC2ProviderClusterProps to each cluster provider. Upgrading Worker Nodes \u00b6 Upgrading Kubernetes version at the cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, including worker nodes use the following configuration: const props : EC2ProviderClusterProps = { version : KubernetesVersion . V1_20 , instanceTypes : [ new InstanceType ( 't3.large' )], amiType : NodegroupAmiType . AL2_X86_64 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20 . 4 } const myClusterProvider = new EC2ClusterProvider ( props ); new EksBlueprint ( app , { id : \"test-cluster-provider\" , clusterProvider : myClusterProvider }); Note: consult the official EKS documentation for the information of the AMI release version that matches your Kubernetes version. Creating Clusters with Spot Capacity Type \u00b6 You can specify capacity type with the cluster configuration options: const props : EC2ProviderClusterProps = { nodeGroupCapacityType : CapacityType . SPOT , version : KubernetesVersion . V1_20 , instanceTypes : [ new InstanceType ( 't3.large' ), new InstanceType ( 'm5.large' )], amiType : NodegroupAmiType . AL2_X86_64 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20 . 4 } Note two attributes in this configuration that are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability.","title":"EC2 Cluster Provider"},{"location":"cluster-providers/ec2-cluster-provider/#ec2-cluster-provider","text":"","title":"EC2 Cluster Provider"},{"location":"cluster-providers/ec2-cluster-provider/#stack-configuration","text":"Supports context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.instance-type : (defaulted to \"m5.large\") Type of instance for the EKS cluster, must be a valid instance type like t3.medium eks.default.vpc : Specifies whether to use an existing VPC (if specified) or create a new one if not specified. eks.default.min-size : Min cluster size, must be positive integer greater than 0 (default 1). eks.default.max-size : Max cluster size, must be greater than minSize (default 3). eks.default.desired-size : Desired cluster size, must be greater or equal to minSize (default min-size ). eks.default.vpc supports an option to look up default VPC in your account/region if the value is set to default . Note, there should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass EC2ProviderClusterProps to each cluster provider.","title":"Stack Configuration"},{"location":"cluster-providers/ec2-cluster-provider/#upgrading-worker-nodes","text":"Upgrading Kubernetes version at the cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, including worker nodes use the following configuration: const props : EC2ProviderClusterProps = { version : KubernetesVersion . V1_20 , instanceTypes : [ new InstanceType ( 't3.large' )], amiType : NodegroupAmiType . AL2_X86_64 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20 . 4 } const myClusterProvider = new EC2ClusterProvider ( props ); new EksBlueprint ( app , { id : \"test-cluster-provider\" , clusterProvider : myClusterProvider }); Note: consult the official EKS documentation for the information of the AMI release version that matches your Kubernetes version.","title":"Upgrading Worker Nodes"},{"location":"cluster-providers/ec2-cluster-provider/#creating-clusters-with-spot-capacity-type","text":"You can specify capacity type with the cluster configuration options: const props : EC2ProviderClusterProps = { nodeGroupCapacityType : CapacityType . SPOT , version : KubernetesVersion . V1_20 , instanceTypes : [ new InstanceType ( 't3.large' ), new InstanceType ( 'm5.large' )], amiType : NodegroupAmiType . AL2_X86_64 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20 . 4 } Note two attributes in this configuration that are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability.","title":"Creating Clusters with Spot Capacity Type"},{"location":"cluster-providers/fargate-cluster-provider/","text":"","title":"Fargate Cluster Provider"},{"location":"cluster-providers/outpost-cluster-provider/","text":"","title":"Outpost Cluster Provider"},{"location":"internal/readme-internal/","text":"Description \u00b6 This is an internal readme for development processes that should be followed for this repository. Local Development \u00b6 This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build. Publishing \u00b6 At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): npm run build (compile) npm publish (this will require credentials to npm) Submitting Changes \u00b6 For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Description"},{"location":"internal/readme-internal/#description","text":"This is an internal readme for development processes that should be followed for this repository.","title":"Description"},{"location":"internal/readme-internal/#local-development","text":"This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build.","title":"Local Development"},{"location":"internal/readme-internal/#publishing","text":"At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): npm run build (compile) npm publish (this will require credentials to npm)","title":"Publishing"},{"location":"internal/readme-internal/#submitting-changes","text":"For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Submitting Changes"}]}
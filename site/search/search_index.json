{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Welcome to the Amazon EKS SSP Quick Start documentation site. Amazon EKS SSP Quick Start provides AWS customers with a framework and methodology that makes it easy to build a Shared Services Platform (SSP) on top of Amazon EKS . What is an SSP? \u00b6 A Shared Services Platform (SSP) is an internal development platform that abstracts the complexities of cloud infrastructure from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise. What can I do with this QuickStart? \u00b6 Customers can use this Quick Start to easily architect and deploy a multi-tenant SSP built on EKS. Specifically, customers can leverage the cdk-eks-blueprint module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including addons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure. Leverage GitOps-based workflows for onboarding and managing workloads for your teams. Examples \u00b6 To view a library of examples for how you can leverage the cdk-eks-blueprint , please see our SSP Patterns Repository .","title":"Overview"},{"location":"#overview","text":"Welcome to the Amazon EKS SSP Quick Start documentation site. Amazon EKS SSP Quick Start provides AWS customers with a framework and methodology that makes it easy to build a Shared Services Platform (SSP) on top of Amazon EKS .","title":"Overview"},{"location":"#what-is-an-ssp","text":"A Shared Services Platform (SSP) is an internal development platform that abstracts the complexities of cloud infrastructure from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise.","title":"What is an SSP?"},{"location":"#what-can-i-do-with-this-quickstart","text":"Customers can use this Quick Start to easily architect and deploy a multi-tenant SSP built on EKS. Specifically, customers can leverage the cdk-eks-blueprint module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including addons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure. Leverage GitOps-based workflows for onboarding and managing workloads for your teams.","title":"What can I do with this QuickStart?"},{"location":"#examples","text":"To view a library of examples for how you can leverage the cdk-eks-blueprint , please see our SSP Patterns Repository .","title":"Examples"},{"location":"cluster-management/","text":"// TODO","title":"Cluster management"},{"location":"core-concepts/","text":"","title":"Core Concepts"},{"location":"getting-started/","text":"Getting Started \u00b6 This getting started guide will walk you through setting up a new CDK project which leverages the cdk-eks-blueprint NPM module to deploy a simple SSP. Project setup \u00b6 To use the cdk-eks-blueprint module, you must have the AWS Cloud Development Kit (CDK) installed. Install CDK via the following. npm install -g aws-cdk@1.104.0 Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript Deploy a Blueprint EKS Cluster \u00b6 Install the cdk-eks-blueprint NPM package via the following. npm i @shapirov/cdk-eks-blueprint Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import * as ssp from '@shapirov/cdk-eks-blueprint' ; const app = new cdk . App (); const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . NginxAddOn , new ssp . addons . ArgoCDAddOn , new ssp . addons . CalicoAddOn , new ssp . addons . MetricsServerAddOn , new ssp . addons . ClusterAutoScalerAddOn , new ssp . addons . ContainerInsightsAddOn , new ssp . addons . AwsLoadBalancerControllerAddOn () ]; const opts = { id : 'east-test-1' , addOns } new ssp . EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXXX' , region : 'us-east-1' }, }); Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap Deploy the stack using the following command. This command will take roughly 20 minutes to complete. cdk deploy Congratulations! You have deployed your first EKS cluster with cdk-eks-blueprint . The above code will provision the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. Nginx into your cluster to serve as a reverse proxy for your workloads. ArgoCD into your cluster to support GitOps deployments. Calico into your cluster to support Network policies. Metrics Server into your cluster to support metrics collection. AWS and Kubernetes resources needed to support Cluster Autoscaler . AWS and Kubernetes resources needed to forward logs and metrics to Container Insights . AWS and Kubernetes resources needed to support AWS Load Balancer Controller . AWS and Kubernetes resources needed to support AWS X-Ray . Cluster Access \u00b6 Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: east-test-1.easttest1ClusterName8D8E5E5E = east-test-1 east-test-1.easttest1ConfigCommand25ABB520 = aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> east-test-1.easttest1GetTokenCommand337FE3DD = aws eks get-token --cluster-name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Stack ARN: arn:aws:cloudformation:us-east-1:115717706081:stack/east-test-1/e1b9e6a0-d5f6-11eb-8498-0a374cd00e27 To update your Kubernetes config for you new cluster, copy and run the east-test-1.easttest1ConfigCommand25ABB520 command (the second command) in your terminal. aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Validate that you now have kubectl access to your cluster via the following: kubectl get namespace You should see output that lists all namespaces in your cluster. Deploy workloads with ArgoCD \u00b6 Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads across multiple namespaces. The sample app of apps repository that we use in this getting started guide can be found here . Install ArgoCD CLI \u00b6 Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd: v2.0.1+33eaf11.dirty Exposing ArgoCD \u00b6 To access ArgoCD running in your Kubernetes cluster, we can leverage Kubernetes Port Forwarding . To do so, first capture the ArgoCD service name in an environment variable. export ARGO_SERVER=$(kubectl get svc -n argocd -l app.kubernetes.io/name=argocd-server -o name) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the ArgoCD login screen. Logging Into ArgoCD \u00b6 ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD You can also login to the ArgoCD UI with generated password and the username admin . echo $ARGO_PASSWORD Deploy workloads to your cluster \u00b6 Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/aws-samples/ssp-eks-workloads.git Create the application within Argo by running the following command argocd app create dev-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/aws-samples/ssp-eks-workloads.git \\ --path \"envs/dev\" Sync the apps by running the following command argocd app sync dev-apps Validate deployments. \u00b6 To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-burnham . kubectl port-forward svc/guestbook-ui -n team-burnham 4040:80 Open up localhost:4040 in your browser and you should see the application. Next Steps \u00b6 For information on Onboarding teams to your clusters, see Team documentation . For information on setting up Continuous Delivery pipelines for your infrastructure, see Pipelines documentation . For information on supported Add-ons, see Add-on documentation For information on Onboarding and managing workloads in your clusters, see Workload documentation .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This getting started guide will walk you through setting up a new CDK project which leverages the cdk-eks-blueprint NPM module to deploy a simple SSP.","title":"Getting Started"},{"location":"getting-started/#project-setup","text":"To use the cdk-eks-blueprint module, you must have the AWS Cloud Development Kit (CDK) installed. Install CDK via the following. npm install -g aws-cdk@1.104.0 Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript","title":"Project setup"},{"location":"getting-started/#deploy-a-blueprint-eks-cluster","text":"Install the cdk-eks-blueprint NPM package via the following. npm i @shapirov/cdk-eks-blueprint Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import * as ssp from '@shapirov/cdk-eks-blueprint' ; const app = new cdk . App (); const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . NginxAddOn , new ssp . addons . ArgoCDAddOn , new ssp . addons . CalicoAddOn , new ssp . addons . MetricsServerAddOn , new ssp . addons . ClusterAutoScalerAddOn , new ssp . addons . ContainerInsightsAddOn , new ssp . addons . AwsLoadBalancerControllerAddOn () ]; const opts = { id : 'east-test-1' , addOns } new ssp . EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXXX' , region : 'us-east-1' }, }); Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap Deploy the stack using the following command. This command will take roughly 20 minutes to complete. cdk deploy Congratulations! You have deployed your first EKS cluster with cdk-eks-blueprint . The above code will provision the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. Nginx into your cluster to serve as a reverse proxy for your workloads. ArgoCD into your cluster to support GitOps deployments. Calico into your cluster to support Network policies. Metrics Server into your cluster to support metrics collection. AWS and Kubernetes resources needed to support Cluster Autoscaler . AWS and Kubernetes resources needed to forward logs and metrics to Container Insights . AWS and Kubernetes resources needed to support AWS Load Balancer Controller . AWS and Kubernetes resources needed to support AWS X-Ray .","title":"Deploy a Blueprint EKS Cluster"},{"location":"getting-started/#cluster-access","text":"Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: east-test-1.easttest1ClusterName8D8E5E5E = east-test-1 east-test-1.easttest1ConfigCommand25ABB520 = aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> east-test-1.easttest1GetTokenCommand337FE3DD = aws eks get-token --cluster-name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Stack ARN: arn:aws:cloudformation:us-east-1:115717706081:stack/east-test-1/e1b9e6a0-d5f6-11eb-8498-0a374cd00e27 To update your Kubernetes config for you new cluster, copy and run the east-test-1.easttest1ConfigCommand25ABB520 command (the second command) in your terminal. aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Validate that you now have kubectl access to your cluster via the following: kubectl get namespace You should see output that lists all namespaces in your cluster.","title":"Cluster Access"},{"location":"getting-started/#deploy-workloads-with-argocd","text":"Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads across multiple namespaces. The sample app of apps repository that we use in this getting started guide can be found here .","title":"Deploy workloads with ArgoCD"},{"location":"getting-started/#install-argocd-cli","text":"Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd: v2.0.1+33eaf11.dirty","title":"Install ArgoCD CLI"},{"location":"getting-started/#exposing-argocd","text":"To access ArgoCD running in your Kubernetes cluster, we can leverage Kubernetes Port Forwarding . To do so, first capture the ArgoCD service name in an environment variable. export ARGO_SERVER=$(kubectl get svc -n argocd -l app.kubernetes.io/name=argocd-server -o name) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the ArgoCD login screen.","title":"Exposing ArgoCD"},{"location":"getting-started/#logging-into-argocd","text":"ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD You can also login to the ArgoCD UI with generated password and the username admin . echo $ARGO_PASSWORD","title":"Logging Into ArgoCD"},{"location":"getting-started/#deploy-workloads-to-your-cluster","text":"Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/aws-samples/ssp-eks-workloads.git Create the application within Argo by running the following command argocd app create dev-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/aws-samples/ssp-eks-workloads.git \\ --path \"envs/dev\" Sync the apps by running the following command argocd app sync dev-apps","title":"Deploy workloads to your cluster"},{"location":"getting-started/#validate-deployments","text":"To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-burnham . kubectl port-forward svc/guestbook-ui -n team-burnham 4040:80 Open up localhost:4040 in your browser and you should see the application.","title":"Validate deployments."},{"location":"getting-started/#next-steps","text":"For information on Onboarding teams to your clusters, see Team documentation . For information on setting up Continuous Delivery pipelines for your infrastructure, see Pipelines documentation . For information on supported Add-ons, see Add-on documentation For information on Onboarding and managing workloads in your clusters, see Workload documentation .","title":"Next Steps"},{"location":"pipelines/","text":"Pipelines \u00b6 While it is convenient to leverage the CDK command line tool to deploy your first cluster, we recommend setting up automated pipelines that will be responsible for deploying and updating your EKS infrastructure. To accomplish this, the EKS SSP - Reference Solution leverages the Pipelines CDK module. This module makes it trivial to create Continuous Delivery (CD) pipelines via CodePipeline that are responsible for deploying and updating your infrastructure. Additionally, the EKS SSP - Reference Solution leverages the GitHub integration that the Pipelines CDK module provides in order to integrate our pipelines with GitHub. The end result is that any new configuration pushed to a GitHub repository containing our CDK will be automatically deployed. Creating a pipeline \u00b6 We can create a new CodePipeline resource via the following. import * as ssp from '@shapirov/cdk-eks-blueprint' const pipeline = ssp . CodePipeline . build ({ name : 'blueprint-pipeline' , owner : '<REPO_OWNER>' , repo : '<REPO_NAME>' , branch : 'main' , secretKey : '<SECRET_KEY>' , scope : scope }) Creating stages \u00b6 Once our pipeline is created, we need to define a stage for the pipeline. To do so, we can wrap our EksBlueprint stack in a cdk.Stage object. import * as ssp from '@shapirov/cdk-eks-blueprint' import * as team from 'path/to/teams' export class ClusterStage extends cdk . Stage { constructor ( scope : cdk.Stack , id : string , props? : cdk.StageProps ) { super ( scope , id , props ); // Setup platform team const accountID = props ? . env ? . account const platformTeam = new team . TeamPlatform ( accountID ! ) const teams : Array < ssp . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . NginxAddOn , new ssp . ArgoCDAddOn , new ssp . CalicoAddOn , new ssp . MetricsServerAddOn , new ssp . ClusterAutoScalerAddOn , new ssp . ContainerInsightsAddOn , ]; new ssp . EksBlueprint ( this , { id : 'blueprint-cluster' , addOns , teams }, props ); } } Adding stages to the pipeline. \u00b6 Once a stage is created, we simply add it to the pipeline. const dev = new ClusterStage ( this , 'blueprint-stage-dev' ) pipeline . addApplicationStage ( dev ); Adding stages to deploy multiple pipelines is trivial. const dev = new ClusterStage ( this , 'blueprint-stage-dev' ) pipeline . addApplicationStage ( dev ); const test = new ClusterStage ( this , 'blueprint-stage-test' ) pipeline . addApplicationStage ( test ); We can also add manual approvals for production stages. const prod = new ClusterStage ( this , 'blueprint-stage-prod' ) pipeline . addApplicationStage ( prod , { manualApprovals : true }); Putting it all together \u00b6 The below code block contains the complete implementation of a CodePipeline that is responsible for deploying three different clusters across three different accounts by using three different pipeline stages. import * as cdk from '@aws-cdk/core' ; // SSP Lib import * as ssp from '@shapirov/cdk-eks-blueprint' // Team implementations import * as team from 'path/to/teams' export class PipelineStack extends cdk . Stack { constructor ( scope : cdk.Construct , id : string , props? : cdk.StackProps ) { super ( scope , id ) const pipeline = this . buildPipeline ( scope ) const dev = new ClusterStage ( this , 'dev-stage' , { env : { account : 'XXXXXXXXXXX' , region : 'us-west-1' , } }) pipeline . addApplicationStage ( dev ) const test = new ClusterStage ( this , 'test-stage' , { env : { account : 'XXXXXXXXXXX' , region : 'us-west-1' , } ) pipeline . addApplicationStage ( test ) // Manual approvals for Prod deploys. const prod = new ClusterStage ( this , 'prod-stage' ,{ env : { account : 'XXXXXXXXXXX' , region : 'us-west-1' , } }) pipeline . addApplicationStage ( prod , { manualApprovals : true }) } buildPipeline = ( scope : cdk.Stack ) => { return ssp . CodePipeline . build ({ name : 'blueprint-pipeline' , owner : '<REPO_OWNER>' , repo : '<REPO_NAME>' , branch : 'main' , secretKey : '<SECRET_KEY>' , scope : scope }) } } export class ClusterStage extends cdk . Stage { constructor ( scope : cdk.Stack , id : string , props? : cdk.StageProps ) { super ( scope , id , props ); // Setup platform team const accountID = props ? . env ? . account const platformTeam = new team . TeamPlatform ( accountID ! ) const teams : Array < ssp . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . NginxAddOn , new ssp . ArgoCDAddOn , new ssp . CalicoAddOn , new ssp . MetricsServerAddOn , new ssp . ClusterAutoScalerAddOn , new ssp . ContainerInsightsAddOn , ]; new ssp . EksBlueprint ( this , { id : 'blueprint-cluster' , addOns , teams }, props ); } }","title":"Pipelines"},{"location":"pipelines/#pipelines","text":"While it is convenient to leverage the CDK command line tool to deploy your first cluster, we recommend setting up automated pipelines that will be responsible for deploying and updating your EKS infrastructure. To accomplish this, the EKS SSP - Reference Solution leverages the Pipelines CDK module. This module makes it trivial to create Continuous Delivery (CD) pipelines via CodePipeline that are responsible for deploying and updating your infrastructure. Additionally, the EKS SSP - Reference Solution leverages the GitHub integration that the Pipelines CDK module provides in order to integrate our pipelines with GitHub. The end result is that any new configuration pushed to a GitHub repository containing our CDK will be automatically deployed.","title":"Pipelines"},{"location":"pipelines/#creating-a-pipeline","text":"We can create a new CodePipeline resource via the following. import * as ssp from '@shapirov/cdk-eks-blueprint' const pipeline = ssp . CodePipeline . build ({ name : 'blueprint-pipeline' , owner : '<REPO_OWNER>' , repo : '<REPO_NAME>' , branch : 'main' , secretKey : '<SECRET_KEY>' , scope : scope })","title":"Creating a pipeline"},{"location":"pipelines/#creating-stages","text":"Once our pipeline is created, we need to define a stage for the pipeline. To do so, we can wrap our EksBlueprint stack in a cdk.Stage object. import * as ssp from '@shapirov/cdk-eks-blueprint' import * as team from 'path/to/teams' export class ClusterStage extends cdk . Stage { constructor ( scope : cdk.Stack , id : string , props? : cdk.StageProps ) { super ( scope , id , props ); // Setup platform team const accountID = props ? . env ? . account const platformTeam = new team . TeamPlatform ( accountID ! ) const teams : Array < ssp . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . NginxAddOn , new ssp . ArgoCDAddOn , new ssp . CalicoAddOn , new ssp . MetricsServerAddOn , new ssp . ClusterAutoScalerAddOn , new ssp . ContainerInsightsAddOn , ]; new ssp . EksBlueprint ( this , { id : 'blueprint-cluster' , addOns , teams }, props ); } }","title":"Creating stages"},{"location":"pipelines/#adding-stages-to-the-pipeline","text":"Once a stage is created, we simply add it to the pipeline. const dev = new ClusterStage ( this , 'blueprint-stage-dev' ) pipeline . addApplicationStage ( dev ); Adding stages to deploy multiple pipelines is trivial. const dev = new ClusterStage ( this , 'blueprint-stage-dev' ) pipeline . addApplicationStage ( dev ); const test = new ClusterStage ( this , 'blueprint-stage-test' ) pipeline . addApplicationStage ( test ); We can also add manual approvals for production stages. const prod = new ClusterStage ( this , 'blueprint-stage-prod' ) pipeline . addApplicationStage ( prod , { manualApprovals : true });","title":"Adding stages to the pipeline."},{"location":"pipelines/#putting-it-all-together","text":"The below code block contains the complete implementation of a CodePipeline that is responsible for deploying three different clusters across three different accounts by using three different pipeline stages. import * as cdk from '@aws-cdk/core' ; // SSP Lib import * as ssp from '@shapirov/cdk-eks-blueprint' // Team implementations import * as team from 'path/to/teams' export class PipelineStack extends cdk . Stack { constructor ( scope : cdk.Construct , id : string , props? : cdk.StackProps ) { super ( scope , id ) const pipeline = this . buildPipeline ( scope ) const dev = new ClusterStage ( this , 'dev-stage' , { env : { account : 'XXXXXXXXXXX' , region : 'us-west-1' , } }) pipeline . addApplicationStage ( dev ) const test = new ClusterStage ( this , 'test-stage' , { env : { account : 'XXXXXXXXXXX' , region : 'us-west-1' , } ) pipeline . addApplicationStage ( test ) // Manual approvals for Prod deploys. const prod = new ClusterStage ( this , 'prod-stage' ,{ env : { account : 'XXXXXXXXXXX' , region : 'us-west-1' , } }) pipeline . addApplicationStage ( prod , { manualApprovals : true }) } buildPipeline = ( scope : cdk.Stack ) => { return ssp . CodePipeline . build ({ name : 'blueprint-pipeline' , owner : '<REPO_OWNER>' , repo : '<REPO_NAME>' , branch : 'main' , secretKey : '<SECRET_KEY>' , scope : scope }) } } export class ClusterStage extends cdk . Stage { constructor ( scope : cdk.Stack , id : string , props? : cdk.StageProps ) { super ( scope , id , props ); // Setup platform team const accountID = props ? . env ? . account const platformTeam = new team . TeamPlatform ( accountID ! ) const teams : Array < ssp . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . NginxAddOn , new ssp . ArgoCDAddOn , new ssp . CalicoAddOn , new ssp . MetricsServerAddOn , new ssp . ClusterAutoScalerAddOn , new ssp . ContainerInsightsAddOn , ]; new ssp . EksBlueprint ( this , { id : 'blueprint-cluster' , addOns , teams }, props ); } }","title":"Putting it all together"},{"location":"teams/","text":"Teams \u00b6 The cdk-eks-blueprint framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team . ApplicationTeam \u00b6 To create an ApplicationTeam for your cluster, simplye implement a class that extends ApplicationTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace. PlatformTeam \u00b6 To create an PlatformTeam for your cluster, simplye implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRole : Role.fromRoleArn ( ` ${ YOUR_ROLE_ARN } ` ); }) DefaultTeamRoles \u00b6 The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services Team Benefits \u00b6 By managing teams via infrastrucutre as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access. Cluster Access ( kubectl ) \u00b6 The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${teamname}teamrole arn:aws:iam::${account}:role/west-dev-${teamname}AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam::${account}:role/west-dev-${platform-team-name}AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam::${account}:role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam::${account}:role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example. Console Access \u00b6 Provided that each team has recieved the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team. Examples \u00b6 There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Teams"},{"location":"teams/#teams","text":"The cdk-eks-blueprint framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team .","title":"Teams"},{"location":"teams/#applicationteam","text":"To create an ApplicationTeam for your cluster, simplye implement a class that extends ApplicationTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace.","title":"ApplicationTeam"},{"location":"teams/#platformteam","text":"To create an PlatformTeam for your cluster, simplye implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRole : Role.fromRoleArn ( ` ${ YOUR_ROLE_ARN } ` ); })","title":"PlatformTeam"},{"location":"teams/#defaultteamroles","text":"The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services","title":"DefaultTeamRoles"},{"location":"teams/#team-benefits","text":"By managing teams via infrastrucutre as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access.","title":"Team Benefits"},{"location":"teams/#cluster-access-kubectl","text":"The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${teamname}teamrole arn:aws:iam::${account}:role/west-dev-${teamname}AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam::${account}:role/west-dev-${platform-team-name}AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam::${account}:role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam::${account}:role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example.","title":"Cluster Access (kubectl)"},{"location":"teams/#console-access","text":"Provided that each team has recieved the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team.","title":"Console Access"},{"location":"teams/#examples","text":"There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Examples"},{"location":"addons/","text":"Add-ons \u00b6 The cdk-eks-blueprint framework leverages a modular approach to managing AddOns that run within the context of a Kubernetes cluster. Customers are free to select the AddOns that run in each of their blueprint clusters. Within the context of the cdk-eks-blueprint framework, an add-on is simply an class, and the implementation of an add-on can do whatever is necessary to support the desired add-on functionality. This can include applying manifests to a Kubernetes cluster or calling AWS APIs to provision new resources. Supported AddOns \u00b6 The framework currently supports the following add-ons. Add-on Description AppMeshAddOn Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddOn Provisions Argo CD into your cluster. AWS Load Balancer Controller Provisions the AWS Load Balancer Controller into your cluster. CalicoAddOn Adds the Calico 1.7.1 CNI/Network policy engine ContainerInsightsAddOn Adds Container Insights support integrating monitoring with CloudWatch ClusterAutoscalerAddOn Adds the standard cluster autoscaler ( Karpenter is coming) MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddOn Adds NGINX ingress controller XrayAddOn (./xray) Adds XRay Daemon to the EKS Cluster","title":"Overview"},{"location":"addons/#add-ons","text":"The cdk-eks-blueprint framework leverages a modular approach to managing AddOns that run within the context of a Kubernetes cluster. Customers are free to select the AddOns that run in each of their blueprint clusters. Within the context of the cdk-eks-blueprint framework, an add-on is simply an class, and the implementation of an add-on can do whatever is necessary to support the desired add-on functionality. This can include applying manifests to a Kubernetes cluster or calling AWS APIs to provision new resources.","title":"Add-ons"},{"location":"addons/#supported-addons","text":"The framework currently supports the following add-ons. Add-on Description AppMeshAddOn Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddOn Provisions Argo CD into your cluster. AWS Load Balancer Controller Provisions the AWS Load Balancer Controller into your cluster. CalicoAddOn Adds the Calico 1.7.1 CNI/Network policy engine ContainerInsightsAddOn Adds Container Insights support integrating monitoring with CloudWatch ClusterAutoscalerAddOn Adds the standard cluster autoscaler ( Karpenter is coming) MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddOn Adds NGINX ingress controller XrayAddOn (./xray) Adds XRay Daemon to the EKS Cluster","title":"Supported AddOns"},{"location":"addons/app-mesh/","text":"AWS App Mesh AddOn \u00b6 AWS App Mesh is a service mesh that makes it easy to monitor and control services.The App Mesh addon provisions the necessary AWS resources and Helm charts into an EKS cluster that are needed to support App Mesh for EKS workloads. Full documentation on using App Mesh with EKS can be found here . Usage \u00b6 import { AppMeshAddOn , ClusterAddOn , EksBlueprint } from '@shapirov/cdk-eks-blueprint' ; const appMeshAddOn = new AppMeshAddOn (); const addOns : Array < ClusterAddOn > = [ appMeshAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); App Mesh Sidecar Injection \u00b6 You can configure certain namespaces for automatic injection of App Mesh sidecar (Envoy) proxy. This will enable handling cross-cutting aspects such as service to service communication, resiliency patterns (Circuit breaker/retries) as well handle ingress and egress for the workloads running in the namespace. Here is an example of a team with a namespace configured for automatic sidecar injection: export class TeamBurnhamSetup extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), namespaceAnnotations : { \"appmesh.k8s.aws/sidecarInjectorWebhook\" : \"enabled\" } }); } } Tracing Integration \u00b6 App Mesh integrates with a number of tracing providers for distributed tracing support. At the moment it supports AWS X-Ray, Jaeger and Datadog providers. X-Ray integration at present requires either a managed node group or a self-managed auto-scaling group backed by EC2. Fargate is not supported. Enabling integration: const appMeshAddOn = new ssp . AppMeshAddOn ({ enableTracing : true , tracingProvider : \"x-ray\" }), When configured, App Mesh will automatically inject an XRay sidecar to handle tracing which enables troubleshooting latency issues. App Mesh and XRay Integration Example \u00b6 team-burnham sample workload repository is configured with an example workload that demonstrates meshified workloads with SSP. After workload is bootstrapped with ArgoCD or applied directly to the cluster in team-burnham namespace it will create a DJ application similar to the one used for the EKS Workshop . It was adapted for GitOps integration with SSP and relies on automatic sidecar injection as well as tracing integration with App Mesh. After the workload is deployed you can generate some traffic to populated traces: $ export DJ_POD_NAME = $( kubectl get pods -n team-burnham -l app = dj -o jsonpath = '{.items[].metadata.name}' ) $ kubectl -n team-burnham exec -it ${ DJ_POD_NAME } -c dj bash $ while true ; do curl http://jazz.team-burnham.svc.cluster.local:9080/ echo curl http://metal.team-burnham.svc.cluster.local:9080/ echo done The above script will start producing traces with XRay. Once traces are produced (for a minute or more) you can navigate to the AWS XRay console and click on Service Map. You will see a screenshot similar to this: Functionality \u00b6 Creates an App Mesh IAM service account. Adds both AWSCloudMapFullAccess and AWSAppMeshFullAccess roles to the service account. Adds AWSXRayDaemonWriteAccess to the instance role if XRay integration is enabled. Creates the appmesh-system namespace. Deploys the appmesh-controller Helm chart into the cluster.","title":"App Mesh"},{"location":"addons/app-mesh/#aws-app-mesh-addon","text":"AWS App Mesh is a service mesh that makes it easy to monitor and control services.The App Mesh addon provisions the necessary AWS resources and Helm charts into an EKS cluster that are needed to support App Mesh for EKS workloads. Full documentation on using App Mesh with EKS can be found here .","title":"AWS App Mesh AddOn"},{"location":"addons/app-mesh/#usage","text":"import { AppMeshAddOn , ClusterAddOn , EksBlueprint } from '@shapirov/cdk-eks-blueprint' ; const appMeshAddOn = new AppMeshAddOn (); const addOns : Array < ClusterAddOn > = [ appMeshAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/app-mesh/#app-mesh-sidecar-injection","text":"You can configure certain namespaces for automatic injection of App Mesh sidecar (Envoy) proxy. This will enable handling cross-cutting aspects such as service to service communication, resiliency patterns (Circuit breaker/retries) as well handle ingress and egress for the workloads running in the namespace. Here is an example of a team with a namespace configured for automatic sidecar injection: export class TeamBurnhamSetup extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), namespaceAnnotations : { \"appmesh.k8s.aws/sidecarInjectorWebhook\" : \"enabled\" } }); } }","title":"App Mesh Sidecar Injection"},{"location":"addons/app-mesh/#tracing-integration","text":"App Mesh integrates with a number of tracing providers for distributed tracing support. At the moment it supports AWS X-Ray, Jaeger and Datadog providers. X-Ray integration at present requires either a managed node group or a self-managed auto-scaling group backed by EC2. Fargate is not supported. Enabling integration: const appMeshAddOn = new ssp . AppMeshAddOn ({ enableTracing : true , tracingProvider : \"x-ray\" }), When configured, App Mesh will automatically inject an XRay sidecar to handle tracing which enables troubleshooting latency issues.","title":"Tracing Integration"},{"location":"addons/app-mesh/#app-mesh-and-xray-integration-example","text":"team-burnham sample workload repository is configured with an example workload that demonstrates meshified workloads with SSP. After workload is bootstrapped with ArgoCD or applied directly to the cluster in team-burnham namespace it will create a DJ application similar to the one used for the EKS Workshop . It was adapted for GitOps integration with SSP and relies on automatic sidecar injection as well as tracing integration with App Mesh. After the workload is deployed you can generate some traffic to populated traces: $ export DJ_POD_NAME = $( kubectl get pods -n team-burnham -l app = dj -o jsonpath = '{.items[].metadata.name}' ) $ kubectl -n team-burnham exec -it ${ DJ_POD_NAME } -c dj bash $ while true ; do curl http://jazz.team-burnham.svc.cluster.local:9080/ echo curl http://metal.team-burnham.svc.cluster.local:9080/ echo done The above script will start producing traces with XRay. Once traces are produced (for a minute or more) you can navigate to the AWS XRay console and click on Service Map. You will see a screenshot similar to this:","title":"App Mesh and XRay Integration Example"},{"location":"addons/app-mesh/#functionality","text":"Creates an App Mesh IAM service account. Adds both AWSCloudMapFullAccess and AWSAppMeshFullAccess roles to the service account. Adds AWSXRayDaemonWriteAccess to the instance role if XRay integration is enabled. Creates the appmesh-system namespace. Deploys the appmesh-controller Helm chart into the cluster.","title":"Functionality"},{"location":"addons/argo-cd/","text":"Argo CD AddOn \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD addon provisions Argo CD into an EKS cluster. For instructions on getting started with Argo CD once your cluster is deployed with this addon included, see our Getting Started guide. Full Argo CD project documentation can be found here . Usage \u00b6 import { ArgoCDAddOn , ClusterAddOn , EksBlueprint } from '@shapirov/cdk-eks-blueprint' ; const argoCDAddOn = new ArgoCDAddOn (); const addOns : Array < ClusterAddOn > = [ argoCDAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Functionality \u00b6 Creates the argo-cd namespace. Deploys the argo-cd Helm chart into the cluster.","title":"ArgoCD"},{"location":"addons/argo-cd/#argo-cd-addon","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD addon provisions Argo CD into an EKS cluster. For instructions on getting started with Argo CD once your cluster is deployed with this addon included, see our Getting Started guide. Full Argo CD project documentation can be found here .","title":"Argo CD AddOn"},{"location":"addons/argo-cd/#usage","text":"import { ArgoCDAddOn , ClusterAddOn , EksBlueprint } from '@shapirov/cdk-eks-blueprint' ; const argoCDAddOn = new ArgoCDAddOn (); const addOns : Array < ClusterAddOn > = [ argoCDAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/argo-cd/#functionality","text":"Creates the argo-cd namespace. Deploys the argo-cd Helm chart into the cluster.","title":"Functionality"},{"location":"addons/aws-load-balancer-controller/","text":"AWS Load Balancer Controller Add-on \u00b6 The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. In the past, you used the Kubernetes in-tree load balancer for instance targets, but used the AWS Load balancer Controller for IP targets. With the AWS Load Balancer Controller version 2.2.0 or later, you can create Network Load Balancers using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. For more information about AWS Load Balancer Controller please see the official documentation . This controller is a required for proper configuration of other ingress controllers such as NGINX. Usage \u00b6 import { AwsLoadBalancerControllerAddon } from '@shapirov/cdk-eks-blueprint' ; readonly awsLoadBalancerController = new AwsLoadBalancerControllerAddon ({ version : '2.2.0' }); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ awsLoadBalancerController ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); To validate that controller is running ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s You can now provision NLB and ALB load balancers. For example to provision an NLB you can use the following service manifest: apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb name : udp-test1 spec : type : LoadBalancer ports : - port : 5005 protocol : UDP targetPort : 5005 selector : name : your-app Functionality \u00b6 Adds proper IAM permissions and creates a Kubernetes service account with IRSA integration. Allows configuration options such as enabling WAF and Shield. Allows to replace the helm chart version if a specific version of the controller is needed.","title":"Load Balancer Controller"},{"location":"addons/aws-load-balancer-controller/#aws-load-balancer-controller-add-on","text":"The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. In the past, you used the Kubernetes in-tree load balancer for instance targets, but used the AWS Load balancer Controller for IP targets. With the AWS Load Balancer Controller version 2.2.0 or later, you can create Network Load Balancers using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. For more information about AWS Load Balancer Controller please see the official documentation . This controller is a required for proper configuration of other ingress controllers such as NGINX.","title":"AWS Load Balancer Controller Add-on"},{"location":"addons/aws-load-balancer-controller/#usage","text":"import { AwsLoadBalancerControllerAddon } from '@shapirov/cdk-eks-blueprint' ; readonly awsLoadBalancerController = new AwsLoadBalancerControllerAddon ({ version : '2.2.0' }); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ awsLoadBalancerController ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); To validate that controller is running ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s You can now provision NLB and ALB load balancers. For example to provision an NLB you can use the following service manifest: apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb name : udp-test1 spec : type : LoadBalancer ports : - port : 5005 protocol : UDP targetPort : 5005 selector : name : your-app","title":"Usage"},{"location":"addons/aws-load-balancer-controller/#functionality","text":"Adds proper IAM permissions and creates a Kubernetes service account with IRSA integration. Allows configuration options such as enabling WAF and Shield. Allows to replace the helm chart version if a specific version of the controller is needed.","title":"Functionality"},{"location":"addons/calico/","text":"Calico Add-on \u00b6 The Calico addon adds support for Kubernetes Network Policies to an EKS cluster. Project Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. To secure workloads in Kubernetes, Calico utilizes Network Policies as you will see below. Usage \u00b6 import { addons } from '@shapirov/cdk-eks-blueprint' ; const myCalicoCNI = new addon . CalicoAddon (); const addOns : Array < ClusterAddOn > = [ myCalicoCNI ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Securing your environment with Kubernetes Network Policies \u00b6 By default, native VPC-CNI plugin for Kubernetes on EKS does not support Kubernetes Network Policies. Installing Calico addon (or alternate CNI provider) for network policy support will enable customers to define and apply standard Kubernetes Network Policies . However, Calico also allows Custom Resource Definitions (CRD) which gives you the ability to add features not in the standard Kubernetes policies, such as: - Explicit Deny rules - Layer 7 rule support (i.e. Http Request types) - Endpoint support other than standard pods: OpenShift, VMs, interfaces, etc. In order to use CRDs (in particular defined within the projectcalico.org/v3 Calico API), you need to install the Calico CLI ( calicoctl ). You can find more information about Calico Network Policy and using calicoctl here . In this section, you will look at how the standard Kubernetes Network Policies are applied. This will be done using kubectl . Pod to Pod communications with no policies \u00b6 If you deploy App of Apps installed using ArgoCD , you can verify that there are no network policies in place: kubectl get networkpolicy -A This means that any resources within the cluster should be able to make ingress and egress connections with other resources within and outside the cluster. You can verify, for example, that you are able to ping from team-riker pod to team-burnham pod. First you retrieve the pod name from the team-burnham namespace its podIP: BURNHAM_POD = $( kubectl get pod -n team-burnham -o jsonpath = '{.items[0].metadata.name}' ) BURNHAM_POD_IP = $( kubectl get pod -n team-burnham $BURNHAM_POD -o jsonpath = '{.status.podIP}' ) Now you can start a shell from the pod in the team-riker namespace and ping the pod from team-burnham namespace: RIKER_POD = $( kubectl -n team-riker get pod -o jsonpath = '{.items[0].metadata.name}' ) kubectl exec -ti -n team-riker $RIKER_POD -- sh Note: since this opens a shell inside the pod, it will not have the environment variables saved above. You should retrieve the actual podIP from the environment variable BURNHAM_POD_IP . With those actual values, curl the IP and port 80 of the pod from team-burnham : # curl -s <Team Burnham Pod IP>:80>/dev/null && echo Success. || echo Fail. You should see Success. Applying Kubernetes Network Policy to block traffic \u00b6 Let's apply the following Network Policy: kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : default-deny spec : podSelector : matchLabels : {} Save it as deny-all.yaml . Run the following commands to apply the policy to both team-riker and team-burnham namespaces: kubectl -n team-riker apply -f deny-all.yaml kubectl -n team-burnham apply -f deny-all.yaml This will essentially prevent access to all resources within both namespaces. Try curl commands from above to verify that it fails. Applying additional policy to re-open pod to pod communications \u00b6 You can apply Kubernetes NetworkPolicy on top of that to \u201cpoke holes\u201d for egress and ingress needs. For example, if you wanted to be able to curl from the team-riker pod to the team-burnham pod, the following Kubernetes NetworkPolicy should be applied. kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : namespace : team-burnham name : allow-riker-to-burnham spec : podSelector : matchLabels : app : guestbook-ui policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 egress : - to : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 Save as allow-burnham-riker.yaml and apply the new NetworkPolicy: kubectl apply -f allow-burnham-riker.yaml Once the policy is applied, once again try the curl command from above. You should now see Success. once again.","title":"Calico Add-on"},{"location":"addons/calico/#calico-add-on","text":"The Calico addon adds support for Kubernetes Network Policies to an EKS cluster. Project Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. To secure workloads in Kubernetes, Calico utilizes Network Policies as you will see below.","title":"Calico Add-on"},{"location":"addons/calico/#usage","text":"import { addons } from '@shapirov/cdk-eks-blueprint' ; const myCalicoCNI = new addon . CalicoAddon (); const addOns : Array < ClusterAddOn > = [ myCalicoCNI ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/calico/#securing-your-environment-with-kubernetes-network-policies","text":"By default, native VPC-CNI plugin for Kubernetes on EKS does not support Kubernetes Network Policies. Installing Calico addon (or alternate CNI provider) for network policy support will enable customers to define and apply standard Kubernetes Network Policies . However, Calico also allows Custom Resource Definitions (CRD) which gives you the ability to add features not in the standard Kubernetes policies, such as: - Explicit Deny rules - Layer 7 rule support (i.e. Http Request types) - Endpoint support other than standard pods: OpenShift, VMs, interfaces, etc. In order to use CRDs (in particular defined within the projectcalico.org/v3 Calico API), you need to install the Calico CLI ( calicoctl ). You can find more information about Calico Network Policy and using calicoctl here . In this section, you will look at how the standard Kubernetes Network Policies are applied. This will be done using kubectl .","title":"Securing your environment with Kubernetes Network Policies"},{"location":"addons/calico/#pod-to-pod-communications-with-no-policies","text":"If you deploy App of Apps installed using ArgoCD , you can verify that there are no network policies in place: kubectl get networkpolicy -A This means that any resources within the cluster should be able to make ingress and egress connections with other resources within and outside the cluster. You can verify, for example, that you are able to ping from team-riker pod to team-burnham pod. First you retrieve the pod name from the team-burnham namespace its podIP: BURNHAM_POD = $( kubectl get pod -n team-burnham -o jsonpath = '{.items[0].metadata.name}' ) BURNHAM_POD_IP = $( kubectl get pod -n team-burnham $BURNHAM_POD -o jsonpath = '{.status.podIP}' ) Now you can start a shell from the pod in the team-riker namespace and ping the pod from team-burnham namespace: RIKER_POD = $( kubectl -n team-riker get pod -o jsonpath = '{.items[0].metadata.name}' ) kubectl exec -ti -n team-riker $RIKER_POD -- sh Note: since this opens a shell inside the pod, it will not have the environment variables saved above. You should retrieve the actual podIP from the environment variable BURNHAM_POD_IP . With those actual values, curl the IP and port 80 of the pod from team-burnham : # curl -s <Team Burnham Pod IP>:80>/dev/null && echo Success. || echo Fail. You should see Success.","title":"Pod to Pod communications with no policies"},{"location":"addons/calico/#applying-kubernetes-network-policy-to-block-traffic","text":"Let's apply the following Network Policy: kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : default-deny spec : podSelector : matchLabels : {} Save it as deny-all.yaml . Run the following commands to apply the policy to both team-riker and team-burnham namespaces: kubectl -n team-riker apply -f deny-all.yaml kubectl -n team-burnham apply -f deny-all.yaml This will essentially prevent access to all resources within both namespaces. Try curl commands from above to verify that it fails.","title":"Applying Kubernetes Network Policy to block traffic"},{"location":"addons/calico/#applying-additional-policy-to-re-open-pod-to-pod-communications","text":"You can apply Kubernetes NetworkPolicy on top of that to \u201cpoke holes\u201d for egress and ingress needs. For example, if you wanted to be able to curl from the team-riker pod to the team-burnham pod, the following Kubernetes NetworkPolicy should be applied. kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : namespace : team-burnham name : allow-riker-to-burnham spec : podSelector : matchLabels : app : guestbook-ui policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 egress : - to : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 Save as allow-burnham-riker.yaml and apply the new NetworkPolicy: kubectl apply -f allow-burnham-riker.yaml Once the policy is applied, once again try the curl command from above. You should now see Success. once again.","title":"Applying additional policy to re-open pod to pod communications"},{"location":"addons/cluster-autoscaler/","text":"Cluster Autoscaler Add-on \u00b6 The ClusterAutoscaler addon adds support for Cluster Autoscaler . Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: - pods fail due to insufficient resources, or - pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time. Usage \u00b6 import { ClusterAutoScalerAddOn } from '@shapirov/cdk-eks-blueprint' ; readonly myClusterAutoscaler = new ClusterAutoscalerAddOn ( \"v1.19.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Functionality \u00b6 Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances. Testing the scaling functionality \u00b6 These following steps will help test the functionality of the Cluster Autoscaler: Deploy a sample app as a deployment. Create a Horizontal Pod Autoscaler (HPA) resource. Generate load to trigger scaling. Deploy a sample app \u00b6 Take a note of the number of nodes available: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-189-107.us-west-2.compute.internal Ready <none> 80m v1.19.6-eks-49a6c0 The first step is to create a sample application via deployment and request 20m of CPU: kubectl create deployment php-apache --image = us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests = cpu = 20m kubectl expose php-apache --port 80 You can see that there's 1 pod currently running: kubectl get pod -l app = php-apache NAME READY STATUS RESTARTS AGE php-apache-55c4584468-vsbl7 1/1 Running 0 63s Create HPA resource \u00b6 Now we can create Horizontal Pod Autoscaler resource with 50% CPU target utilization, and the minimum number of pods at 1 and max at 20: kubectl autoscale deployment php-apache \\ --cpu-percent = 50 \\ --min = 1 \\ --max = 20 You can verify by looking at the hpa resource: kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 10%/50% 1 20 2 52s Generate load \u00b6 With the resources created, you can generate load on the apache server with a busybox container: kubectl --generator = run-pod/v1 run -i --tty load-generator --image = busybox /bin/sh You can generate the actual load on the shell by running a while loop: while true ; do wget -q -O - http://php-apache ; done Verify that Cluster Autoscaler works \u00b6 While the load is being generated, access another terminal to verify that HPA is working. The following command should return a list of many nods created (as many as 10): kubectl get pods -l app = php-apache -o wide --watch With more pods being created, you would expect more nodes to be created; you can access the Cluster Autoscaler logs to confirm: kubectl -n kube-system logs -f deployment/cluster-autoscaler Lastly, you can list all the nodes and see that there are now multiple nodes: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-187-70.us-west-2.compute.internal Ready <none> 73s v1.19.6-eks-49a6c0 ip-10-0-189-107.us-west-2.compute.internal Ready <none> 84m v1.19.6-eks-49a6c0 ip-10-0-224-226.us-west-2.compute.internal Ready <none> 46s v1.19.6-eks-49a6c0 ip-10-0-233-105.us-west-2.compute.internal Ready <none> 90s v1.19.6-eks-49a6c0","title":"Cluster Autoscaler"},{"location":"addons/cluster-autoscaler/#cluster-autoscaler-add-on","text":"The ClusterAutoscaler addon adds support for Cluster Autoscaler . Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: - pods fail due to insufficient resources, or - pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time.","title":"Cluster Autoscaler Add-on"},{"location":"addons/cluster-autoscaler/#usage","text":"import { ClusterAutoScalerAddOn } from '@shapirov/cdk-eks-blueprint' ; readonly myClusterAutoscaler = new ClusterAutoscalerAddOn ( \"v1.19.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/cluster-autoscaler/#functionality","text":"Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances.","title":"Functionality"},{"location":"addons/cluster-autoscaler/#testing-the-scaling-functionality","text":"These following steps will help test the functionality of the Cluster Autoscaler: Deploy a sample app as a deployment. Create a Horizontal Pod Autoscaler (HPA) resource. Generate load to trigger scaling.","title":"Testing the scaling functionality"},{"location":"addons/cluster-autoscaler/#deploy-a-sample-app","text":"Take a note of the number of nodes available: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-189-107.us-west-2.compute.internal Ready <none> 80m v1.19.6-eks-49a6c0 The first step is to create a sample application via deployment and request 20m of CPU: kubectl create deployment php-apache --image = us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests = cpu = 20m kubectl expose php-apache --port 80 You can see that there's 1 pod currently running: kubectl get pod -l app = php-apache NAME READY STATUS RESTARTS AGE php-apache-55c4584468-vsbl7 1/1 Running 0 63s","title":"Deploy a sample app"},{"location":"addons/cluster-autoscaler/#create-hpa-resource","text":"Now we can create Horizontal Pod Autoscaler resource with 50% CPU target utilization, and the minimum number of pods at 1 and max at 20: kubectl autoscale deployment php-apache \\ --cpu-percent = 50 \\ --min = 1 \\ --max = 20 You can verify by looking at the hpa resource: kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 10%/50% 1 20 2 52s","title":"Create HPA resource"},{"location":"addons/cluster-autoscaler/#generate-load","text":"With the resources created, you can generate load on the apache server with a busybox container: kubectl --generator = run-pod/v1 run -i --tty load-generator --image = busybox /bin/sh You can generate the actual load on the shell by running a while loop: while true ; do wget -q -O - http://php-apache ; done","title":"Generate load"},{"location":"addons/cluster-autoscaler/#verify-that-cluster-autoscaler-works","text":"While the load is being generated, access another terminal to verify that HPA is working. The following command should return a list of many nods created (as many as 10): kubectl get pods -l app = php-apache -o wide --watch With more pods being created, you would expect more nodes to be created; you can access the Cluster Autoscaler logs to confirm: kubectl -n kube-system logs -f deployment/cluster-autoscaler Lastly, you can list all the nodes and see that there are now multiple nodes: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-187-70.us-west-2.compute.internal Ready <none> 73s v1.19.6-eks-49a6c0 ip-10-0-189-107.us-west-2.compute.internal Ready <none> 84m v1.19.6-eks-49a6c0 ip-10-0-224-226.us-west-2.compute.internal Ready <none> 46s v1.19.6-eks-49a6c0 ip-10-0-233-105.us-west-2.compute.internal Ready <none> 90s v1.19.6-eks-49a6c0","title":"Verify that Cluster Autoscaler works"},{"location":"addons/container-insights/","text":"Container Insights Addon \u00b6 The ContainerInsights addon adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing. Usage \u00b6 Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import { AddOns } from '@shapirov/cdk-eks-blueprint' ; const myClusterAutoscaler = new AddOns . ContainerInsightsAddOn (); const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Next run cdk deploy to update your CDK stack. Prerequisites \u00b6 Once the Container Insights add-on has been installed to your cluster check to see that the CloudWatch Agent and the FluentD daemons are running. Run kubectl get all -n amazon-cloudwatch and you should see the following output. NAME READY STATUS RESTARTS AGE pod/cloudwatch-agent-k8wxl 1/1 Running 0 105s pod/fluentd-cloudwatch-78zv4 1/1 Running 0 105s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cloudwatch-agent 1 1 1 1 1 <none> 107s daemonset.apps/fluentd-cloudwatch 1 1 1 1 1 <none> 106s To enable or disable control plane logs with the console, please run the following command in your terminal - aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command - aws eks describe-update \\ --region <region-code>\\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } View metrics for cluster and workloads \u00b6 Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance. View cluster level logs \u00b6 After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane. View workload level logs \u00b6 In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax. View containers via that container map in container insights. \u00b6 In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"Container Insights"},{"location":"addons/container-insights/#container-insights-addon","text":"The ContainerInsights addon adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing.","title":"Container Insights Addon"},{"location":"addons/container-insights/#usage","text":"Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import { AddOns } from '@shapirov/cdk-eks-blueprint' ; const myClusterAutoscaler = new AddOns . ContainerInsightsAddOn (); const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Next run cdk deploy to update your CDK stack.","title":"Usage"},{"location":"addons/container-insights/#prerequisites","text":"Once the Container Insights add-on has been installed to your cluster check to see that the CloudWatch Agent and the FluentD daemons are running. Run kubectl get all -n amazon-cloudwatch and you should see the following output. NAME READY STATUS RESTARTS AGE pod/cloudwatch-agent-k8wxl 1/1 Running 0 105s pod/fluentd-cloudwatch-78zv4 1/1 Running 0 105s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cloudwatch-agent 1 1 1 1 1 <none> 107s daemonset.apps/fluentd-cloudwatch 1 1 1 1 1 <none> 106s To enable or disable control plane logs with the console, please run the following command in your terminal - aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command - aws eks describe-update \\ --region <region-code>\\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } }","title":"Prerequisites"},{"location":"addons/container-insights/#view-metrics-for-cluster-and-workloads","text":"Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance.","title":"View metrics for cluster and workloads"},{"location":"addons/container-insights/#view-cluster-level-logs","text":"After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane.","title":"View cluster level logs"},{"location":"addons/container-insights/#view-workload-level-logs","text":"In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax.","title":"View workload level logs"},{"location":"addons/container-insights/#view-containers-via-that-container-map-in-container-insights","text":"In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"View containers via that container map in container insights."},{"location":"addons/xray/","text":"AWS X-Ray AddOn \u00b6 AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. The X-Ray addon provisions X-Ray daemon into an EKS cluster. This daemon exposes an internal endpoint xray-service.xray-system.svc.cluster.local:2000 that could be leveraged to aggregate and post traces to the AWS X-Ray service. For instructions on getting started with X-Ray on EKS refer to the EKS Workshop X-Ray Section . Usage \u00b6 import { XrayAddOn , ClusterAddOn , EksBlueprint } from '@shapirov/cdk-eks-blueprint' ; const xrayAddOn = new XrayAddOn (); const addOns : Array < ClusterAddOn > = [ xrayAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Once deployed, it allows applications to be instrumented with X-Ray by leveraging the X-Ray SDK. Examples of such integration can be found on GitHub . Functionality \u00b6 Creates the xray-system namespace. Deploys the xray-daemon manifests into the cluster. Configures Kubernetes service account with IRSA ( AWSXRayDaemonWriteAccess ) for communication between the cluster and the AWS X-Ray service","title":"AWS X-Ray AddOn"},{"location":"addons/xray/#aws-x-ray-addon","text":"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. The X-Ray addon provisions X-Ray daemon into an EKS cluster. This daemon exposes an internal endpoint xray-service.xray-system.svc.cluster.local:2000 that could be leveraged to aggregate and post traces to the AWS X-Ray service. For instructions on getting started with X-Ray on EKS refer to the EKS Workshop X-Ray Section .","title":"AWS X-Ray AddOn"},{"location":"addons/xray/#usage","text":"import { XrayAddOn , ClusterAddOn , EksBlueprint } from '@shapirov/cdk-eks-blueprint' ; const xrayAddOn = new XrayAddOn (); const addOns : Array < ClusterAddOn > = [ xrayAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Once deployed, it allows applications to be instrumented with X-Ray by leveraging the X-Ray SDK. Examples of such integration can be found on GitHub .","title":"Usage"},{"location":"addons/xray/#functionality","text":"Creates the xray-system namespace. Deploys the xray-daemon manifests into the cluster. Configures Kubernetes service account with IRSA ( AWSXRayDaemonWriteAccess ) for communication between the cluster and the AWS X-Ray service","title":"Functionality"},{"location":"cluster-providers/ec2-cluster-provider/","text":"EC2 Cluster Provider \u00b6 Stack Configuration \u00b6 Supports context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.instance-type : (defaulted to \"m5.large\") Type of instance for the EKS cluster, must be a valid instance type like t3.medium eks.default.vpc : Specifies whether to use an existing VPC (if specified) or create a new one if not specified. eks.default.min-size : Min cluster size, must be positive integer greater than 0 (default 1). eks.default.max-size : Max cluster size, must be greater than minSize (default 3). eks.default.desired-size : Desired cluster size, must be greater or equal to minSize (default min-size ). eks.default.vpc supports an option to look up default VPC in your account/region if the value is set to default . Note, there should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass EC2ProviderClusterProps to each cluster provider. Upgrading Worker Nodes \u00b6 Upgrading Kubernetes version at the cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, including worker nodes use the following configuration: const props: EC2ProviderClusterProps = { version: KubernetesVersion.V1_20, instanceTypes: [new InstanceType('t3.large')], amiType: NodegroupAmiType.AL2_X86_64, amiReleaseVersion: \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } const myClusterProvider = new EC2ClusterProvider(props); new EksBlueprint(app, { id: \"test-cluster-provider\", clusterProvider: myClusterProvider }); Note: consult the official EKS documentation for the information of the AMI release version that matches your Kubernetes version. Creating Clusters with Spot Capacity Type \u00b6 You can specify capacity type with the cluster configuration options: const props: EC2ProviderClusterProps = { nodeGroupCapacityType: CapacityType.SPOT, version: KubernetesVersion.V1_20, instanceTypes: [new InstanceType('t3.large'), new InstanceType('m5.large')], amiType: NodegroupAmiType.AL2_X86_64, amiReleaseVersion: \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note two attributes in this configuration that are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability.","title":"EC2 Cluster Provider"},{"location":"cluster-providers/ec2-cluster-provider/#ec2-cluster-provider","text":"","title":"EC2 Cluster Provider"},{"location":"cluster-providers/ec2-cluster-provider/#stack-configuration","text":"Supports context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.instance-type : (defaulted to \"m5.large\") Type of instance for the EKS cluster, must be a valid instance type like t3.medium eks.default.vpc : Specifies whether to use an existing VPC (if specified) or create a new one if not specified. eks.default.min-size : Min cluster size, must be positive integer greater than 0 (default 1). eks.default.max-size : Max cluster size, must be greater than minSize (default 3). eks.default.desired-size : Desired cluster size, must be greater or equal to minSize (default min-size ). eks.default.vpc supports an option to look up default VPC in your account/region if the value is set to default . Note, there should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass EC2ProviderClusterProps to each cluster provider.","title":"Stack Configuration"},{"location":"cluster-providers/ec2-cluster-provider/#upgrading-worker-nodes","text":"Upgrading Kubernetes version at the cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, including worker nodes use the following configuration: const props: EC2ProviderClusterProps = { version: KubernetesVersion.V1_20, instanceTypes: [new InstanceType('t3.large')], amiType: NodegroupAmiType.AL2_X86_64, amiReleaseVersion: \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } const myClusterProvider = new EC2ClusterProvider(props); new EksBlueprint(app, { id: \"test-cluster-provider\", clusterProvider: myClusterProvider }); Note: consult the official EKS documentation for the information of the AMI release version that matches your Kubernetes version.","title":"Upgrading Worker Nodes"},{"location":"cluster-providers/ec2-cluster-provider/#creating-clusters-with-spot-capacity-type","text":"You can specify capacity type with the cluster configuration options: const props: EC2ProviderClusterProps = { nodeGroupCapacityType: CapacityType.SPOT, version: KubernetesVersion.V1_20, instanceTypes: [new InstanceType('t3.large'), new InstanceType('m5.large')], amiType: NodegroupAmiType.AL2_X86_64, amiReleaseVersion: \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note two attributes in this configuration that are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability.","title":"Creating Clusters with Spot Capacity Type"},{"location":"cluster-providers/fargate-cluster-provider/","text":"","title":"Fargate cluster provider"},{"location":"cluster-providers/outpost-cluster-provider/","text":"","title":"Outpost cluster provider"},{"location":"internal/readme-internal/","text":"Description \u00b6 This is an internal readme for development processes that should be followed for this repository. Local Development \u00b6 This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build. Publishing \u00b6 At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): make build (compile) npm publish (this will require credentials to npm) Submitting Changes \u00b6 For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Description"},{"location":"internal/readme-internal/#description","text":"This is an internal readme for development processes that should be followed for this repository.","title":"Description"},{"location":"internal/readme-internal/#local-development","text":"This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build.","title":"Local Development"},{"location":"internal/readme-internal/#publishing","text":"At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): make build (compile) npm publish (this will require credentials to npm)","title":"Publishing"},{"location":"internal/readme-internal/#submitting-changes","text":"For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Submitting Changes"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the <code>Amazon EKS Blueprints Quick Start</code> documentation site.</p> <p>This repository contains the source code for the <code>eks-blueprints</code> NPM module. It can be used by AWS customers, partners, and internal AWS teams to configure and manage complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. </p>"},{"location":"#what-is-eks-blueprints","title":"What is EKS Blueprints?","text":"<p>EKS Blueprints helps you compose complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. With EKS Blueprints, you describe the configuration for the desired state of your EKS environment, such as the control plane, worker nodes, and Kubernetes add-ons, as an IaC blueprint. Once a blueprint is configured, you can use it to stamp out consistent environments across multiple AWS accounts and Regions using continuous deployment automation.</p> <p>You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Fluent Bit, Keda, ArgoCD, and more. EKS Blueprints also helps you implement relevant security controls needed to operate workloads from multiple teams in the same cluster.</p>"},{"location":"#what-can-i-do-with-this-quickstart","title":"What can I do with this QuickStart?","text":"<p>Customers can use this Quick Start to easily architect and deploy a multi-tenant Blueprints built on EKS. Specifically, customers can leverage the <code>eks-blueprints</code> module to:</p> <ul> <li> Deploy Well-Architected EKS clusters across any number of accounts and regions.</li> <li> Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository.</li> <li> Define teams, namespaces, and their associated access permissions for your clusters.</li> <li> Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure.</li> <li> Leverage GitOps-based workflows for onboarding and managing workloads for your teams. </li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>To view a library of examples for how you can leverage the <code>eks-blueprints</code>, please see our Blueprints Patterns Repository.</p>"},{"location":"#workshop","title":"Workshop","text":"<p>We maintain a hands-on self-paced workshop, the EKS Blueprints for CDK workshop helps you with foundational setup of your EKS cluster, and it gradually adds complexity via existing and new modules.</p> <p>To post feedback, submit feature ideas, or report bugs regarding the workshop, use the Issues section of this GitHub repo, and tag it with <code>workshop</code>.</p>"},{"location":"cluster-management/","title":"Multi-Cluster Management","text":"<p>Multi-cluster management refers to the strategies associated with managing and updating cluster configuration across many Amazon EKS clusters. Infrastructure as code (IaC) tools like the AWS CDK provides the ability to bring automation and consistency when deploying your clusters. You need the ability to apply the same configurations to as many of your clusters as necessary and by defining all of your resources via Infrastructure as Code (IaC), it removes the problem of having to generate or apply custom YAML files for each one of your clusters allowing your teams to move faster. Defining your clusters resources using the AWS CDK allows your teams to focus on the underlying workloads as the infrastructure components will be taken care of via the AWS CDK. </p> <p>The main benefits organizations can see using the AWS CDK to manage their Amazon EKS clusters can be summarized as follows: - Consistency across all clusters and environments - Streamlining access control across your organization - Ease of management for multiple clusters - Access to GitOps methodologies and best practices - Automated lifecycle management for cluster deployment</p> <p>The Amazon EKS Blueprints Quick Start references the <code>eks-blueprints-patterns</code> repository repository that includes examples of different deployment patterns and options which includes patterns for multi-cluster that can be deployed across multiple regions. If you take a look at the main.ts file in the patterns repository, you will notice that the stacks that define our Amazon EKS clusters and associated pipelines that are deployed to different regions as shown in the snippet below:</p> <pre><code>#!/usr/bin/env node\nimport * as cdk from 'aws-cdk-lib';\nconst app = new cdk.App();\n//-------------------------------------------\n// Single Cluster with multiple teams.\n//-------------------------------------------\nimport MultiTeamConstruct from '../lib/multi-team-construct'\nnew MultiTeamConstruct(app, 'multi-team');\n//-------------------------------------------\n// Multiple clusters, multiple regions.\n//-------------------------------------------\nimport MultiRegionConstruct from '../lib/multi-region-construct'\nnew MultiRegionConstruct(app, 'multi-region');\n//-------------------------------------------\n// Single Fargate cluster.\n//-------------------------------------------\nimport FargateConstruct from '../lib/fargate-construct'\nnew FargateConstruct(app, 'fargate');\n//-------------------------------------------\n// Multiple clusters with deployment pipeline.\n//-------------------------------------------\nimport PipelineStack from '../lib/pipeline-stack'\nconst account = process.env.CDK_DEFAULT_ACCOUNT\nconst region = process.env.CDK_DEFAULT_REGION\nconst env = { account, region }\nnew PipelineStack(app, 'pipeline', { env });\n</code></pre> <p>Using the AWS CDK, you can define the specific region to deploy your clusters using environment variables as a construct in Typescript as shown in the example above. If you were to deploy all the stacks in your main.ts file you would be able to view your running clusters by region by running the following command</p> <pre><code>aws eks list-cluster --region &lt;insert region&gt;\n</code></pre> <p>If for example you chose the region us-west-2, you would get a similar output: <pre><code>{\n    \"clusters\": [\n        \"all clusters in this region\"\n    ]\n}\n</code></pre></p>"},{"location":"cluster-management/#multi-region-management","title":"Multi-Region Management","text":"<p>In a production environment, it is common to have clusters that reside in different locations. This could be in different regions, on-prem, or follow a hybrid cloud model. Some of the common design patterns that come in to play when it comes to multi-cluster management across these different operational models include things like high availability, data replication, networking, traffic routing, and the underlying management of those clusters. In the eks-blueprints-patterns/lib/multi-region-construct directory, you will find the index.ts file which shows a concrete example of how to deploy multiple clusters to different regions as shown below</p> <p><pre><code>import * as cdk from 'aws-cdk-lib';\n// Blueprints Lib\nimport * as blueprints from '@aws-quickstart/eks-blueprints'\n// Team implementations\nimport * as team from '../teams'\nexport default class MultiRegionConstruct extends cdk.Construct {\n    constructor(scope: cdk.Construct, id: string) {\n        super(scope, id);\n        // Setup platform team\n        const accountID = process.env.CDK_DEFAULT_ACCOUNT!\n        const platformTeam = new team.TeamPlatform(accountID)\n        const teams: Array&lt;blueprints.Team&gt; = [platformTeam];\n        const version = \"auto\";\n        // AddOns for the cluster.\n        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.NginxAddOn,\n            new blueprints.addons.ArgoCDAddOn,\n            new blueprints.addons.MetricsServerAddOn,\n            new blueprints.addons.ClusterAutoScalerAddOn,\n            new blueprints.addons.ContainerInsightsAddOn,\n            new blueprints.addons.VpcCniAddOn(),\n            new blueprints.addons.CoreDnsAddOn(),\n            new blueprints.addons.KubeProxyAddOn()\n        ];\n        const east = 'blueprint-us-east-2'\n        new blueprints.EksBlueprint(scope, { id: `${id}-${east}`, addOns, teams, version}, {\n            env: { region: east }\n        });\n        const central = 'blueprint-us-central-2'\n        new blueprints.EksBlueprint(scope, { id: `${id}-${central}`, addOns, teams, version }, {\n            env: { region: central }\n        });\n        const west = 'blueprint-us-west-2'\n        new blueprints.EksBlueprint(scope, { id: `${id}-${west}`, addOns, teams, version }, {\n            env: { region: west }\n        });\n    }\n}\n</code></pre> This construct imports all of the core components of the <code>EKS Blueprints</code> framework, Teams construct, and Addons as modules which then deploys our clusters to different regions. The main point to take away from this is that we are adding automation and consistency to our clusters as we deploy multiple clusters to multiple regions since all of our components have already been defined in the <code>EKS Blueprints</code> library along with Teams and Addons. </p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>This document provides a high level overview of the Core Concepts that are embedded in the <code>eks-blueprints</code> framework. For the purposes of this document, we will assume the reader is familiar with Git, Docker, Kubernetes and AWS. </p> Concept Description Blueprint A <code>blueprint</code> combines <code>clusters</code>, <code>add-ons</code>, and <code>teams</code> into a cohesive object that can deployed as a whole Cluster A Well-Architected EKS Cluster. Resource Provider Resource providers are abstractions that supply external AWS resources to the cluster (e.g. hosted zones, VPCs, etc.) Add-on Allow you to configure, deploy, and update the operational software, or add-ons, that provide key functionality to support your Kubernetes applications. Team A logical grouping of IAM identities that have access to a Kubernetes namespace(s). Pipeline Continuous Delivery pipelines for deploying <code>clusters</code> and <code>add-ons</code>. Application An application that runs within an EKS Cluster."},{"location":"core-concepts/#blueprint","title":"Blueprint","text":"<p>The <code>eks-blueprints</code> framework allows you to configure and deploy what we call <code>blueprint</code> clusters. A <code>blueprint</code> consists of an EKS cluster, a set of <code>add-ons</code> that will be deployed into the cluster, and a set of <code>teams</code> who will have access to a cluster. Once a <code>blueprint</code> is configured, it can be easily deployed across any number of AWS accounts and regions. <code>Blueprints</code> also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding. </p> <p>To view sample <code>blueprint</code> implementations, please see our patterns repository. </p>"},{"location":"core-concepts/#cluster","title":"Cluster","text":"<p>A <code>cluster</code> is simply an EKS cluster. The <code>eks-blueprints</code> framework provides for customizing the compute options you leverage with your <code>clusters</code>. The framework currently supports <code>EC2</code>, <code>Fargate</code> and <code>BottleRocket</code> instances. To specify the type of compute you want to use for your <code>cluster</code>, you supply a <code>ClusterProvider</code> object to your <code>blueprint</code>. The framework defaults to leveraging the <code>MngClusterProvider</code>.</p> <p>Each <code>ClusterProvider</code> provides additional configuration options as well. For example, the <code>MngClusterProvider</code> allows you to configure instance types, min and max instance counts, and amiType, among other options. </p> <p>See our <code>Cluster Providers</code> documentation page for detailed information. </p>"},{"location":"core-concepts/#resource-provider","title":"Resource Provider","text":"<p>A <code>resource</code> is a CDK construct that implements <code>IResource</code> interface from <code>aws-cdk-lib</code> which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 <code>IHostedZone</code>, an ACM certificate <code>ICertificate</code>, a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams.</p> <p><code>ResourceProviders</code> enable customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint. </p> <p>See our <code>Resource Providers</code> documentation page for detailed information.</p>"},{"location":"core-concepts/#add-on","title":"Add-on","text":"<p><code>Add-ons</code> allow you to configure the tools and services that you would like to run in order to support your EKS workloads. When you configure <code>add-ons</code> for a <code>blueprint</code>, the <code>add-ons</code> will be provisioned at deploy time. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. </p> <p>For example, the <code>MetricsServerAddOn</code> only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server (as a Helm chart). By contrast, the <code>AWSLoadBalancerControllerAddon</code> deploys Kubernetes resources, in addition to creating resources via AWS APIs that are needed to support the AWS Load Balancer Controller. The most common case to address via an add-on is configuration of IAM roles and permissions and the Kubernetes service account, leveraging IRSA to access AWS resources.</p> <p>See our <code>Add-ons</code> documentation page for detailed information. </p>"},{"location":"core-concepts/#team","title":"Team","text":"<p><code>Teams</code> allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. The framework currently supports two types of <code>teams</code>: <code>ApplicationTeam</code> and <code>PlatformTeam</code>. <code>ApplicationTeam</code> members are granted access to specific namespaces. <code>PlatformTeam</code> members are granted administrative access to your clusters. </p> <p>See our <code>Teams</code> documentation page for detailed information. </p>"},{"location":"core-concepts/#pipeline","title":"Pipeline","text":"<p><code>Pipelines</code> allow you to configure <code>Continuous Delivery</code> (CD) pipelines for your cluster <code>blueprints</code> that are directly integrated with your Git provider.</p> <p>See our <code>Pipelines</code> documentation page for detailed information. </p>"},{"location":"core-concepts/#application","title":"Application","text":"<p><code>Applications</code> represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. </p> <p>See our Workload Bootstrapping documentation for detailed information.</p>"},{"location":"extensibility/","title":"Extensibility","text":"<p>This guide provides an overview of extensibility options focusing on add-on extensions as the primary mechanism for the partners and customers.</p>"},{"location":"extensibility/#overview","title":"Overview","text":"<p>Blueprints Framework is designed to be extensible. In the context of this guide, extensibility refers to the ability of customers and partners to both add new capabilities to the framework or platforms based on Blueprints as well as customize existing behavior, including the ability to modify or override existing behavior.</p> <p>The following abstractions can be leveraged to add new features to the framework:</p> <ul> <li>Add-on. Customers and partners can implement new add-ons which could be leveraged exactly the same way as the core add-ons (supplied by the framework).</li> <li>Resource Provider. This construct allows customers to create resources that can be reused across multiple add-ons and/or teams. For example, IAM roles, VPC, hosted zone.</li> <li>Cluster Provider. This construct allows creation of custom code that provisions an EKS cluster with node groups. It can be leveraged to extend behavior such as control plane customization, custom settings for node groups.</li> <li>Team. This abstraction allows to create team templates for application and platform teams and set custom setting for network isolation, policies (network, security), software wiring (auto injection of proxies, team specific service mesh configuration) and other extensions pertinent to the teams.</li> </ul>"},{"location":"extensibility/#add-on-extensions","title":"Add-on Extensions","text":"<p>In a general case, implementation of an add-on is a class which implements the <code>ClusterAddOn</code> interface.</p> <pre><code>export declare interface ClusterAddOn { \n    id? : string;\n    deploy(clusterInfo: types.ClusterInfo): Promise&lt;Construct&gt; | void;\n}\n</code></pre> <p>Note: The add-on implementation can optionally supply the <code>id</code> attribute if the target add-on can be added to a blueprint more than once.</p> <p>Implementation of the add-on is expected to be an exported class that implements the interface and supplies the implementation of the <code>deploy</code> method. In order for the add-on to receive the deployment contextual information about the provisioned cluster, region, resource providers and/or other add-ons, the <code>deploy</code> method takes the <code>ClusterInfo</code> parameter (see types), which represents a structure defined in the SPI (service provider interface) contracts. The API for the cluster info structure is stable and provides access to the provisioned EKS cluster, scheduled add-ons (that have not been installed yet but are part of the blueprint) or provisioned add-ons and other contexts.</p>"},{"location":"extensibility/#post-deployment-hooks","title":"Post Deployment Hooks","text":"<p>In certain cases, add-on provisioning may require logic to be executed after provisioning of the add-ons (and teams) is complete. For such cases, add-on can optionally implement <code>ClusterPostDeploy</code> interface.</p> <pre><code>/**\n * Optional interface to allow cluster bootstrapping after provisioning of add-ons and teams is complete.\n * Can be leveraged to bootstrap workloads, perform cluster checks. \n * ClusterAddOn implementation may implement this interface in order to get post deployment hook point.\n */\nexport declare interface ClusterPostDeploy {\n    postDeploy(clusterInfo: types.ClusterInfo, teams: Team[]): void;\n}\n</code></pre> <p>This capability is leveraged for example in ArgoCD add-on to bootstrap workloads after all add-ons finished provisioning. Note, in this case unlike the standard <code>deploy</code> method implementation, the add-on also gets access to the provisioned teams.</p>"},{"location":"extensibility/#helm-add-ons","title":"Helm Add-ons","text":"<p>Helm add-ons are the most common case that generally combines provisioning of a helm chart as well as supporting infrastructure such as wiring of proper IAM policies for the Kubernetes service account, provisioning or configuring other AWS resources (VPC, subnets, node groups).</p> <p>In order to provide consistency across all Helm add-ons supplied by the Blueprints framework all Helm add-ons are implemented as derivatives of the <code>HelmAddOn</code> base class and support properties based on <code>HelmAddOnUserProps</code>. See the example extension section below for more details.</p> <p>Use cases that are enabled by leveraging the base <code>HelmAddOn</code> class:</p> <ol> <li>Consistency across all helm based add-on will reduce effort to understand how to apply and configure standard add-on options.</li> <li>Ability to override helm chart repository can enable leveraging private helm chart repository by the customer and facilitate add-on usage for private EKS clusters.</li> <li>Extensibility mechanisms available in the Blueprints framework can allow to intercept helm deployments and leverage GitOps driven add-on configuration.</li> </ol>"},{"location":"extensibility/#non-helm-add-ons","title":"Non-helm Add-ons","text":"<p>Add-ons that don't leverage helm but require to install arbitrary Kubernetes manifests will not be able to leverage the benefits provided by the <code>HelmAddOn</code> however, they are still relatively easy to implement. Deployment of arbitrary kubernetes manifests can leverage the following construct:</p> <pre><code>import { KubernetesManifest } from \"aws-cdk-lib/aws-eks\";\nimport * as blueprints from \"@aws-quickstart/eks-blueprints\";\n\nexport class MyNonHelmAddOn implements blueprints.ClusterAddOn {\n    deploy(clusterInfo: blueprints.ClusterInfo): void {\n        const cluster = clusterInfo.cluster;\n        // Apply manifest\n        const doc = blueprints.utils.readYamlDocument(__dirname + '/my-product.yaml');\n        // ... apply any substitutions for dynamic values \n        const manifest = doc.split(\"---\").map(e =&gt; blueprints.utils.loadYaml(e));\n        new KubernetesManifest(cluster.stack, \"myproduct-manifest\", {\n            cluster,\n            manifest,\n            overwrite: true\n        });\n    }\n}\n</code></pre> <p>Note: When leveraging this approach consider how customers can apply the add-on for fully private clusters. It may be reasonable to bundle the manifest with the add-on in the npm package.</p>"},{"location":"extensibility/#add-on-dependencies","title":"Add-on Dependencies","text":"<p>Add-ons can depend on other add-ons and that dependency may be soft or hard. Hard dependency implies that add-on provisioning must fail if the dependency is not available. For example, if an add-on requires access to AWS Secrets Manager for a secret containing a license key, credentials or other sensitive information, it can declare dependency on the CSI Secret Store Driver.</p> <p>Dependency management for direct hard dependency are implemented using a decorator <code>@dependable</code>.</p> <p>Example:</p> <pre><code>import { Construct } from \"constructs\";\nimport * as blueprints from \"@aws-quickstart/eks-blueprints\";\n\nexport class MyProductAddOn extends blueprints.HelmAddOn {\n\n    readonly options: MyProductAddOnProps; // extends HelmAddOnUserProps\n\n    ...\n\n    @@blueprints.utils.dependable('AwsLoadBalancerControllerAddOn') // depends on AwsLoadBalancerController\n    deploy(clusterInfo: ClusterInfo): Promise&lt;Construct&gt; {\n        ...\n    }\n</code></pre>"},{"location":"extensibility/#passing-secrets-to-add-ons","title":"Passing Secrets to Add-ons","text":"<p>Secrets from the AWS Secrets Manager or AWS Systems Manager Parameter Store can be made available as files mounted in Amazon EKS pods. It can be achieved with the help of AWS Secrets and Configuration Provider (ASCP) for the Kubernetes Secrets Store CSI Driver. The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+. More information on general concepts for leveraging ASCP can be found here.</p> <p>Blueprints Framework provides support for both Secrets Store CSI Driver as well as ASCP with the Secrets Store Add-on.</p> <p>Add-ons requiring support for secrets can declare dependency on the secret store add-on:</p> <pre><code>export class MyAddOn extends blueprints.addons.HelmAddOn {\n...\n    // Declares dependency on secret store add-on if secrets are needed. \n    // Customers will have to explicitly add this add-on to the blueprint.\n    @blueprints.utils.dependable(blueprints.SecretsStoreAddOn.name) \n    deploy(clusterInfo: blueprints.ClusterInfo): Promise&lt;Construct&gt; {\n        ...\n    }\n</code></pre> <p>In order to propagate the secret from the Secrets Manager to the Kubernetes cluster, the add-on should create a <code>SecretProviderClass</code> Kubernetes object.  leveraging the <code>blueprints.addons.SecretProviderClass</code>. The framework will take care of wiring the Kubernetes service account with the correct IAM permissions to pull the secret:</p> <pre><code>const sa = clusterInfo.cluster.addServiceAccount(...);\n\nconst csiSecret: blueprints.addons.CsiSecretProps = {\n    secretProvider: new blueprints.LookupSecretsManagerSecretByName(licenseKeySecret), // the secret must be defined upfront and available in the region with the name specified in the licenseKeySecret\n    kubernetesSecret: {\n        secretName: 'my-addon-license-secret',\n        data: [\n            {\n                key: 'licenseKey'\n            }\n        ]\n    }\n};\n\nconst secretProviderClass = new blueprints.addons.SecretProviderClass(clusterInfo, sa, \"my-addon-license-secret-class\", csiSecret);\n</code></pre> <p>Note: you can also leverage <code>LookupSecretsManagerSecretByArn</code>, <code>LookupSsmSecretByAttrs</code> or a custom implementation of the secret provider interface <code>blueprints.addons.SecretProvider</code>.</p> <p>After the secret provider class is created, it should be mounted on any pod in the namespace to make the secret accessible. Mounting the secret volume also creates a regular Kubernetes <code>Secret</code> object based on the supplied description (<code>my-addon-license-secret</code>). This capability is controlled by the configuration of the Blueprints Secret Store add-on and is enabled by default.</p> <p>Many Helm charts provide options to mount additional volumes and mounts to the provisioned product. For example, a Helm chart (ArgoCD, FluentBit) allows specifying <code>volumes</code> and <code>volumeMounts</code> as the helm chart values. Mounting the secret in such cases is simple and does not require an additional pod for secrets.</p> <p>Here is an example of a secret volume and volume mount passed as values to a Helm chart:</p> <pre><code>const chart = this.addHelmChart(clusterInfo, {\n    ... // standard values\n    ,\n    volumes: [\n        {\n            name: \"secrets-store-inline\",\n            csi: {\n                driver: \"secrets-store.csi.k8s.io\",\n                readOnly: true,\n                volumeAttributes: {\n                    secretProviderClass: \"my-addon-license-secret-class\"\n        }\n            }\n        }\n    ],\n    volumeMounts: [\n        {\n            name: \"secrets-store-inline\",\n            mountPath: \"/mnt/secret-store\"\n        }\n    ]\n});\n</code></pre> <p>After the secret volume is mounted (on any pod), you will see that a Kubernetes secret (for example <code>my-addon-license-secret</code>) is also created in the target namespace. See the supplied code example for more details.</p>"},{"location":"extensibility/#private-extensions","title":"Private Extensions","text":"<p>Extensions specific to a customer instance of Blueprints can be implemented inline with the blueprint in the same codebase. Such extensions are scoped to the customer base and cannot be reused. Example of a private extension:</p> <pre><code>class MyAddOn extends HelmAddOn {\n\n    constructor() {\n        super({\n            chart: 'mychart',\n            ...\n        });\n    }\n\n    deploy(clusterInfo: blueprints.ClusterInfo): Promise&lt;Construct&gt; {\n        return Promise.resolve(this.addHelmChart(clusterInfo, {}));\n    }\n}\n\nblueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(new MyAddOn())\n    .build(app, 'my-extension-test-blueprint');\n</code></pre>"},{"location":"extensibility/#public-extensions","title":"Public Extensions","text":"<p>The life-cycle of a public extension should be decoupled from the life-cycle of the EKS Blueprints main repository. When decoupled, extensions can be released at any arbitrary cadence specific to the extension, enabling better agility when it comes to new features or bug fixes.</p> <p>In order to enable this model the following workflow outline steps required to create and release a public extension:</p> <ol> <li>Public extensions are created in a separate repository. Public GitHub repository is preferred as it aligns with the open-source spirit of the framework and enables external reviews/feedback.</li> <li>Extensions are released and consumed as distinct public NPM packages.</li> <li>Public Extensions are expected to have sufficient documentation to allow customers to consume them independently. Documentation can reside in GitHub or external resources referenced in the documentation bundled with the extension.</li> <li>Public extensions are expected to be tested and validated against released Blueprints versions, e.g. with a CICD pipeline. Pipeline can be created with the pipelines support from the Blueprints framework or leveraging customer/partner specific tools.</li> </ol>"},{"location":"extensibility/#partner-extensions","title":"Partner Extensions","text":"<p>Partner extensions (APN Partner) are expected to comply with the public extension workflow and additional items required to ensure proper validation and documentation support for a partner extension.</p> <ol> <li>Documentation PR should be created to the main Blueprints Quickstart repository to update the AddOns section. Example of add-on documentation can be found here along with the list of other add-ons.</li> <li>An example that shows a ready to use pattern leveraging the add-on should be submitted to the Blueprints Patterns Repository. This step will enable AWS PSAs to validate the add-on as well as provide a ready to use pattern to the customers, that could be copied/cloned in their Blueprints implementation.</li> </ol>"},{"location":"extensibility/#example-extension","title":"Example Extension","text":"<p>Example extension contains a sample implementation of a FluentBit log forwarder add-on and covers the following aspects of an extension workflow:</p> <ol> <li>Pre-requisite configuration related to nodejs, npm, typescript.</li> <li>Project template with support to build, test and run the extension.</li> <li>Example blueprint (can be found in ./bin/main.ts) that references the add-on.</li> <li>Example of configuring a Kubernetes service account with IRSA (IAM roles for service accounts) and required IAM policies.</li> <li>Example of the helm chart provisioning.</li> <li>Example of passing secret values to the add-on (such as credentials and/or licenseKeys) by leveraging CSI Secret Store Driver.</li> <li>Outlines support to build, package and publish the add-on in an NPM repository.</li> </ol>"},{"location":"extensibility/#architecture-validation","title":"Architecture Validation","text":"<p>Architecture validation in the blueprints feature is to specify whether a particular addon supports architecture types such as <code>ARM</code>, <code>X86</code>. New addons should add the decorator <code>@supportALL</code> or <code>@supportX86</code> or <code>@supportARM</code> as shown below before the <code>class</code> to make sure the dynamic map is updated with addon name and supported architectures.</p> <pre><code>@supportsALL\nexport class AdotCollectorAddOn extends CoreAddOn {\n\n    constructor(props?: AdotCollectorAddOnProps) {\n        super({ ...defaultProps, ...props });\n    }\n    @dependable(CertManagerAddOn.name)\n</code></pre> <p>While creating solutions with blueprints, it is recommended to use the below method override to <code>addOns</code> method to validate a particular addon is supported for a particular architecure.</p> <pre><code>    public addOns(...addOns: spi.ClusterAddOn[]): this {\n        addOns.forEach(a =&gt; validateSupportedArchitecture(a.constructor.name, ArchType.ARM)); \n        return super.addOns(...addOns);\n    }\n</code></pre> <p>In case of a particular addon not supporting an architecture type, following error is reportind during compile time:</p> <pre><code>Addon AckAddon is not supported on architecture ARM.\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This getting started guide will walk you through setting up a new CDK project which leverages the <code>eks-blueprints</code> NPM module to deploy a simple Blueprints. </p>"},{"location":"getting-started/#project-setup","title":"Project Setup","text":"<p>Before proceeding, make sure AWS CLI is installed on your machine.</p> <p>To use the <code>eks-blueprints</code> module, you must have Node.js and npm installed. We will also use <code>make</code> to simplify build and other common actions. You can do it using the following instructions:</p> Mac <pre><code>brew install make\nbrew install node\n</code></pre> Ubuntu <pre><code>sudo apt install make\nsudo apt install nodejs\n</code></pre> <p>Create a directory that represents you project (e.g. <code>my-blueprints</code>) and then create a new <code>typescript</code> CDK project in that directory.</p> <pre><code>npm install -g n # may require sudo\nn stable # may require sudo \nnpm install -g aws-cdk@2.173.4 # may require sudo (Ubuntu) depending on configuration\ncdk --version # must produce 2.173.4\nmkdir my-blueprints\ncd my-blueprints\ncdk init app --language typescript\n</code></pre>"},{"location":"getting-started/#configure-your-project","title":"Configure Your Project","text":"<p>Install the <code>eks-blueprints</code> NPM package (keep reading if you get an error or warning message):</p> <pre><code>npm i @aws-quickstart/eks-blueprints\n</code></pre> <p>CDK version of the EKS Blueprints is pinned as <code>peerDependencies</code> to the version that we tested against to minimize the risk of incompatibilities and/or broken functionality. When running the install command, NPM will detect any mismatch in the version and issue an error. For example:</p> <pre><code>npm ERR! code ERESOLVE\nnpm ERR! ERESOLVE unable to resolve dependency tree\nnpm ERR! \nnpm ERR! While resolving: my-blueprint@0.1.0\nnpm ERR! Found: aws-cdk-lib@2.133.0\nnpm ERR! node_modules/aws-cdk-lib\nnpm ERR!   aws-cdk-lib@\"2.133.0\" from the root project\nnpm ERR! \nnpm ERR! Could not resolve dependency:\nnpm ERR! peer bundled aws-cdk-lib@\"2.133.0\" from @aws-quickstart/eks-blueprints@1.14.0\nnpm ERR! node_modules/@aws-quickstart/eks-blueprint\n</code></pre> <p>This message means that the version of CDK that the customer is using is different from the version of CDK used in EKS Blueprints. Locate the line <code>peer bundled</code> and check the expected version of the CDK. Make sure that in your <code>package.json</code> the version is set to the expected. In this example, <code>package.json</code> contained <code>\"aws-cdk-lib\": \"2.133.0\"</code>, while the expected version was <code>2.173.4</code>.</p> <p>Note: after the initial installation, upgrading the version of CDK to an incompatible higher/lower version will produce a warning, but will succeed. For community support (submitting GitHub issues) please make sure you have a matching version configured.</p> <p>Example warning:</p> <pre><code>npm WARN \nnpm WARN Could not resolve dependency:\nnpm WARN peer bundled aws-cdk-lib@\"2.133.0\" from @aws-quickstart/eks-blueprints@1.14.0\n</code></pre>"},{"location":"getting-started/#deploy-eks-clusters","title":"Deploy EKS Clusters","text":"<p>Replace the contents of <code>bin/&lt;your-main-file&gt;.ts</code> (where <code>your-main-file</code> by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install a number of addons.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\nconst account = 'XXXXXXXXXXXXX';\nconst region = 'us-east-2';\nconst version = 'auto';\n\nblueprints.HelmAddOn.validateHelmVersions = true; // optional if you would like to check for newer versions\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new blueprints.addons.ArgoCDAddOn(),\n    new blueprints.addons.MetricsServerAddOn(),\n    new blueprints.addons.ClusterAutoScalerAddOn(),\n    new blueprints.addons.AwsLoadBalancerControllerAddOn(),\n    new blueprints.addons.VpcCniAddOn(), // support network policies ootb\n    new blueprints.addons.CoreDnsAddOn(),\n    new blueprints.addons.KubeProxyAddOn()\n];\n\nconst stack = blueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .version(version)\n    .addOns(...addOns)\n    .useDefaultSecretEncryption(true) // set to false to turn secret encryption off (non-production/demo cases)\n    .build(app, 'eks-blueprint');\n</code></pre> <p>Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is a process of creating IAM roles and lambda functions that can execute some of the common CDK constructs.</p> <p>For application of the EKS Blueprints Framework with AWS Organizations, Multi-account framework and Control Tower consider a process to automatically or manually CDK-bootstrapping new (workload/environment) accounts when they are added to the organization. More info on account bootstrapping here.</p> <p>Bootstrap your environment with the following command. </p> <pre><code>cdk bootstrap aws://&lt;your-account-number&gt;/&lt;region-to-bootstrap&gt;\n</code></pre> <p>Note: if the account/region combination used in the code example above is different from the initial combination used with <code>cdk bootstrap</code>, you will need to perform <code>cdk bootstrap</code> again to avoid error.</p> <p>Please reference CDK usage doc for detail.</p> <p>Setup the <code>AWS_REGION</code> environment variable and deploy the stack using the following command. This command will take roughly 20 minutes to complete.</p> <pre><code>export AWS_REGION=us-east-2 # Replace this with region of your choice.\ncdk deploy\n</code></pre> <p>Note: Your terminal needs access to an AWS environment along with the <code>AWS_REGION</code> environment variable setup. If you fail to do so, you will observe following errors logs in your <code>cdk deploy</code> command. The stack will still run fine but will fall back picking default versions for the addon from <code>versionMap</code> in the respective <code>CoreAddon</code> instead of making an EKS API call to retrieve the latest version for the core addon.</p> <pre><code> Error  Region is missing\nerror stack:\n  \u2022 index.js    default\n        /node_modules/@smithy/config-resolver/dist-cjs/index.js:117\n  \u2022 index.js\n        /node_modules/@smithy/node-config-provider/dist-cjs/index.js:72\n  \u2022 index.js\n        /node_modules/@smithy/property-provider/dist-cjs/index.js:79\n  \u2022 index.js    async coalesceProvider\n        /node_modules/@smithy/property-provider/dist-cjs/index.js:106\n  \u2022 index.js\n        async /node_modules/@smithy/property-provider/dist-cjs/index.js:117\n  \u2022 index.js    async region\n        /node_modules/@smithy/config-resolver/dist-cjs/index.js:142\n  \u2022 httpAuthSchemeProvider.js   async Object.defaultEKSHttpAuthSchemeParametersProvider [as httpAuthSchemeParametersProvider]\n        /node_modules/@aws-sdk/client-eks/dist-cjs/auth/httpAuthSchemeProvider.js:9\n  \u2022 index.js\n        async /node_modules/@smithy/core/dist-cjs/index.js:61\n  \u2022 index.js\n        async /node_modules/@aws-sdk/middleware-logger/dist-cjs/index.js:33\n  \u2022 index.ts    async VpcCniAddOn.provideVersion\n        /node_modules/@aws-quickstart/eks-blueprints/lib/addons/core-addon/index.ts:187\n2024-04-29 19:33:48.638 WARN    /node_modules/@aws-quickstart/eks-blueprints/lib/addons/core-addon/index.ts:209 main    Failed to retrieve add-on versions from EKS for add-on vpc-cni. Falling back to default version.\n</code></pre> <p>Congratulations! You have deployed your first EKS cluster with <code>eks-blueprints</code>. The above code will provision the following:</p> <ul> <li> A new Well-Architected VPC with both Public and Private subnets.</li> <li> A new Well-Architected EKS cluster in the region and account you specify.</li> <li> Nginx into your cluster to serve as a reverse proxy for your workloads. </li> <li> ArgoCD into your cluster to support GitOps deployments. </li> <li> Metrics Server into your cluster to support metrics collection.</li> <li> AWS and Kubernetes resources needed to support Cluster Autoscaler.</li> <li> AWS and Kubernetes resources needed to forward logs and metrics to Container Insights.</li> <li> AWS and Kubernetes resources needed to support AWS Load Balancer Controller.</li> <li> Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS.</li> <li> CoreDNS Amazon EKS add-on (CoreDns) into your cluster. CoreDns is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS</li> <li>  kube-proxy Amazon EKS add-on (KubeProxy) into your cluster to maintains network rules on each Amazon EC2 node</li> <li> AWS and Kubernetes resources needed to support AWS X-Ray.</li> </ul>"},{"location":"getting-started/#cluster-access","title":"Cluster Access","text":"<p>Once the deploy completes, you will see output in your terminal window similar to the following:</p> <pre><code>Outputs:\neast-test-1.easttest1ClusterName8D8E5E5E = east-test-1\neast-test-1.easttest1ConfigCommand25ABB520 = aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn &lt;ROLE_ARN&gt;\neast-test-1.easttest1GetTokenCommand337FE3DD = aws eks get-token --cluster-name east-test-1 --region us-east-1 --role-arn &lt;ROLE_ARN&gt;\n\nStack ARN:\narn:aws:cloudformation:us-east-1:115717706081:stack/east-test-1/e1b9e6a0-d5f6-11eb-8498-0a374cd00e27\n</code></pre> <p>To update your Kubernetes config for you new cluster, copy and run the <code>east-test-1.easttest1ConfigCommand25ABB520</code> command (the second command) in your terminal. </p> <pre><code>aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn &lt;ROLE_ARN&gt;\n</code></pre> <p>Validate that you now have <code>kubectl</code> access to your cluster via the following:</p> <pre><code>kubectl get namespace\n</code></pre> <p>You should see output that lists all namespaces in your cluster. </p>"},{"location":"getting-started/#deploy-workloads-with-argocd","title":"Deploy workloads with ArgoCD","text":"<p>Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads across multiple namespaces. The sample app of apps repository that we use in this getting started guide can be found here.</p> <p>You can leverage Automatic Bootstrapping for automatic onboarding of workloads. This feature may be leveraged even when workload repositories are not ready yet, as it creates a placeholder for future workloads and decouples workload onboarding for the infrastructure provisioning pipeline. The next steps, described in this guide apply for cases when customer prefer to bootstrap their workloads manually through ArgoCD UI console.</p>"},{"location":"getting-started/#install-argocd-cli","title":"Install ArgoCD CLI","text":"<p>These steps are needed for manual workload onboarding. For automatic bootstrapping please refer to the Automatic Bootstrapping.</p> <p>Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following:</p> <pre><code>argocd version --short --client\n</code></pre> <p>You should see output similar to the following:</p> <pre><code>argocd: v2.0.1+33eaf11.dirty\n</code></pre>"},{"location":"getting-started/#exposing-argocd","title":"Exposing ArgoCD","text":"<p>To access ArgoCD running in your Kubernetes cluster, we can leverage Kubernetes Port Forwarding. To do so, first capture the ArgoCD service name in an environment variable.</p> <pre><code>export ARGO_SERVER=$(kubectl get svc -n argocd -l app.kubernetes.io/name=argocd-server -o name) \n</code></pre> <p>Next, in a new terminal tab, expose the service locally.</p> <pre><code>kubectl port-forward $ARGO_SERVER -n argocd 8080:443\n</code></pre> <p>Open your browser to http://localhost:8080 and you should see the ArgoCD login screen.</p> <p></p>"},{"location":"getting-started/#logging-into-argocd","title":"Logging Into ArgoCD","text":"<p>ArgoCD will create an <code>admin</code> user and password on a fresh install. To get the ArgoCD admin password, run the following.</p> <pre><code>export ARGO_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\n</code></pre> <p>While still port-forwarding, login via the following.</p> <pre><code>argocd login localhost:8080 --username admin --password $ARGO_PASSWORD\n</code></pre> <p>You can also login to the ArgoCD UI with generated password and the username <code>admin</code>. </p> <pre><code>echo $ARGO_PASSWORD\n</code></pre>"},{"location":"getting-started/#deploy-workloads-to-your-cluster","title":"Deploy workloads to your cluster","text":"<p>Create a project in Argo by running the following command</p> <pre><code>argocd proj create sample \\\n    -d https://kubernetes.default.svc,argocd \\\n    -s https://github.com/aws-samples/eks-blueprints-workloads.git\n</code></pre> <p>Create the application within Argo by running the following command</p> <pre><code>argocd app create dev-apps \\\n    --dest-namespace argocd  \\\n    --dest-server https://kubernetes.default.svc  \\\n    --repo https://github.com/aws-samples/eks-blueprints-workloads.git \\\n    --path \"envs/dev\"\n</code></pre> <p>Sync the apps by running the following command</p> <pre><code>argocd app sync dev-apps \n</code></pre>"},{"location":"getting-started/#validate-deployments","title":"Validate deployments.","text":"<p>To validate your deployments, leverage <code>kubectl port-forwarding</code> to access the <code>guestbook-ui</code> service for <code>team-riker</code>.</p> <pre><code>kubectl port-forward svc/guestbook-ui -n team-riker 4040:80\n</code></pre> <p>Open up <code>localhost:4040</code> in your browser and you should see the application.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>For information on onboarding teams to your clusters, see <code>Team</code> documentation. </p> <p>For information on deploying Continuous Delivery pipelines for your infrastructure, see <code>Pipelines</code> documentation.</p> <p>For information on supported add-ons, see <code>Add-on</code> documentation</p> <p>For information on Onboarding and managing workloads in your clusters, see <code>Workload</code> documentation. </p>"},{"location":"opa-gatekeeper/","title":"What is OPA Gatekeeper?","text":"<p>The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link.</p> <p>OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper.</p> <p>)</p> <p>In the context of a development platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below:</p> <ul> <li>Which users can access which resources.</li> <li>Which subnets egress traffic is allowed to.</li> <li>Which clusters a workload must be deployed to.</li> <li>Which registries binaries can be downloaded from.</li> <li>Which OS capabilities a container can execute with.</li> <li>Which times of day the system can be accessed at.</li> </ul> <p>RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster.</p>"},{"location":"opa-gatekeeper/#key-terminology","title":"Key Terminology","text":"<ul> <li>OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable</li> <li>Constraint -  A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.</li> <li>Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems.</li> <li>Constraint Template - Templates that allows users to declare new constraints </li> <li>Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context</li> </ul>"},{"location":"opa-gatekeeper/#usage","title":"Usage","text":"<pre><code>import * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\nconst account = &lt;AWS_ACCOUNT_ID&gt;;\nconst region = &lt;AWS_REGION&gt;;\nconst version = \"auto\";\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .account(account)\n  .region(region)\n  .version(version)\n  .addOns( new blueprints.addons.OpaGatekeeperAddOn() )\n  .teams().build(app, 'my-stack-name');\n</code></pre> <p>To validate that OPA Gatekeeper is running within your cluster run the following command:</p> <pre><code>kubectl get pod -n gatekeeper-system\n</code></pre> <p>You should see the following output:</p> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\ngatekeeper-audit-7c5998d4c-b5n7j                 1/1     Running   0          1d\ngatekeeper-controller-manager-5894545cc9-b86zm   1/1     Running   0          1d\ngatekeeper-controller-manager-5894545cc9-bntdt   1/1     Running   0          1d\ngatekeeper-controller-manager-5894545cc9-tb7fz   1/1     Running   0          1d\n</code></pre> <p>You will notice the <code>gatekeeper-audit-7c5998d4c-b5n7j</code> pod that is created when we deploy the <code>OpaGatekeeperAddOn</code>. The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The <code>gatekeeper-controller-manager</code> is simply there to manage the <code>OpaGatekeeperAddOn</code>.</p>"},{"location":"opa-gatekeeper/#example-with-opa-gatekeeper","title":"Example with OPA Gatekeeper","text":"<p>For the purposes of operating within a platform defined by <code>EKS Blueprints</code>, we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here. In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here.</p> <p>Run the following command to create the ConstraintTemplate:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml\n</code></pre> <p>To verify that the ConstraintTemplate was created run the following command:</p> <pre><code>kubectl get constrainttemplate\n</code></pre> <p>You should see the following output:</p> <pre><code>NAME                AGE\nk8srequiredlabels   45s\n</code></pre> <p>You will notice that if you create a new namespace without any labels that the request will go through and that is because we now need to create the individual <code>Constraint CRD</code> as defined by the <code>Constraint Template</code> that we created above. Let's create the individal <code>Constraint CRD</code> using the command below: </p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml\n</code></pre> <p>If we then try and create a namespace by running <code>kubectl create ns test</code> (notice that we are not adding any labels) you will get the following error message:</p> <pre><code>Error from server ([all-must-have-owner] All namespaces must have an `owner` label that points to your company username): admission webhook \"validation.gatekeeper.sh\" denied the request: [all-must-have-owner] All namespaces must have an `owner` label that points to your company username\n</code></pre> <p>For more information on OPA Gatekeeper please refer to the links below:</p> <ul> <li>https://github.com/open-policy-agent</li> <li>https://open-policy-agent.github.io/gatekeeper/website/docs/</li> <li>https://github.com/open-policy-agent/gatekeeper-library</li> </ul>"},{"location":"pipelines/","title":"Pipelines","text":"<p>While it is convenient to leverage the CDK command line tool to deploy your first cluster, we recommend setting up automated pipelines that will be responsible for deploying and updating your EKS infrastructure.</p> <p>To accomplish this, the EKS Blueprints - Reference Solution leverages the <code>Pipelines</code> CDK module. This module makes it trivial to create Continuous Delivery (CD) pipelines via CodePipeline that are responsible for deploying and updating your infrastructure.</p> <p>Additionally, the EKS Blueprints - Reference Solution leverages the GitHub integration that the <code>Pipelines</code> CDK module provides in order to integrate our pipelines with GitHub. The end result is that any new configuration pushed to a GitHub repository containing our CDK will be automatically deployed.</p>"},{"location":"pipelines/#defining-your-blueprint-to-use-with-pipeline","title":"Defining your blueprint to use with pipeline","text":"<p>Pipeline support requires enabling a setting for modern stack synthesis. This setting should be enabled for blueprints that leverage CDKv1 explicitly, for CDKv2 it is enabled by default.</p> <p>Creation of a pipeline starts with defining the blueprint that will be deployed across the pipeline stages.</p> <p>The framework allows defining a blueprint builder without instantiating the stack.</p> <pre><code>import * as blueprints from '@aws-quickstart/eks-blueprints'\nimport * as team from 'path/to/teams'\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    .account(account) // the supplied default will fail, but build and synth will pass\n    .region('us-west-1')\n    .version(\"auto\")\n    .addOns(\n        new blueprints.AwsLoadBalancerControllerAddOn,\n        new blueprints.ExternalDnsAddOn,\n        new blueprints.NginxAddOn,\n        new blueprints.MetricsServerAddOn,\n        new blueprints.ClusterAutoScalerAddOn\n    )\n    .teams(new team.TeamRikerSetup);\n</code></pre> <p>The difference between the above code and a normal way of instantiating the stack is lack of <code>.build()</code> at the end of the blueprint definition. This code will produce a blueprint builder that can be instantiated inside the pipeline stages.</p>"},{"location":"pipelines/#creating-a-pipeline","title":"Creating a pipeline","text":"<p>We can create a new <code>CodePipeline</code> resource via the following.</p>"},{"location":"pipelines/#using-github-as-codepipeline-repository-source","title":"Using GitHub as CodePipeline repository source.","text":"<pre><code>import * as blueprints from '@aws-quickstart/eks-blueprints'\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    ...; // configure your blueprint builder\n\n blueprints.CodePipelineStack.builder()\n    .name(\"eks-blueprints-pipeline\")\n    .repository({\n        owner: \"aws-samples\",\n        repoUrl: 'cdk-eks-blueprints-patterns',\n        credentialsSecretName: 'github-token',\n        targetRevision: 'main' // optional, default is \"main\",\n    })\n</code></pre> <p>If you IaC code is located under a specific folder, for example <code>PROJECT_ROOT/infra/blueprints</code> you can specify that directory with the repository (applies to GitHub. CodeCommit and CodeStar repos). </p> <pre><code> blueprints.CodePipelineStack.builder()\n    .name(\"eks-blueprints-pipeline\")\n    .repository({\n        owner: \"aws-samples\",\n        repoUrl: 'cdk-eks-blueprints-patterns',\n        credentialsSecretName: 'github-token',\n        path: \"./infra/blueprints\" // optional, default is './'\n    })```\n\nNote: the above code depends on the AWS secret `github-token` defined in the target account/region. The secret may be defined in one main region, and replicated to all target regions.\n\n### Using AWS CodeCommit as CodePipeline repository source.\n\n```typescript\nimport * as blueprints from '@aws-quickstart/eks-blueprints'\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    ...; // configure your blueprint builder\n\n blueprints.CodePipelineStack.builder()\n    .name(\"eks-blueprints-pipeline\")\n    .repository({\n        codeCommitRepoName: 'cdk-eks-blueprints-patterns', // should be in the same region with pipeline-stack\n        targetRevision: 'master' // optional, default is \"master\"\n    })\n</code></pre>"},{"location":"pipelines/#using-aws-codestar-connection-as-codepipeline-repository-source","title":"Using AWS CodeStar connection as CodePipeline repository source.","text":"<pre><code>import * as blueprints from '@aws-quickstart/eks-blueprints'\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    ...; // configure your blueprint builder\n\n blueprints.CodePipelineStack.builder()\n    .name(\"eks-blueprints-pipeline\")\n    .codeBuildPolicies(blueprints.DEFAULT_BUILD_POLICIES)\n    .owner('aws-samples')\n    .repository({\n        repoUrl: 'blueprints-repo-codestar', // repository name \n        codeStarConnectionArn: 'fill-in-codestar-arn', // CodeStar connection ARN\n        targetRevision: 'main', // optional, default is \"main\"\n    })\n</code></pre>"},{"location":"pipelines/#creating-stages","title":"Creating stages","text":"<p>Once our pipeline is created, we need to define <code>stages</code> for the pipeline. To do so, we can leverage <code>blueprints.StackStage</code> convenience class and builder support for it. Let's continue leveraging the pipeline builder defined in the previous step.</p> <pre><code>const blueprint = blueprints.EksBlueprint.builder()\n    ...; // configure your blueprint builder\n\nblueprints.CodePipelineStack.builder()\n    .name(\"blueprints-eks-pipeline\")\n    .owner(\"aws-samples\")\n    .repository({\n        //  your repo info\n    })\n    .stage({\n        id: 'us-west-1-managed-blueprints-test',\n        stackBuilder: blueprint.clone('us-west-1') // clone the blueprint to customize for the stage. You can add more add-ons, teams here.\n    })\n    .stage({\n        id: 'us-east-2-managed-blueprints-prod',\n        stackBuilder: blueprint.clone('us-east-2'), // clone the blueprint to customize for the stage. You can add more add-ons, team, here.\n        stageProps: {\n            manualApprovals: true\n        }\n    })\n</code></pre> <p>Consider adding <code>ArgoCDAddOn</code> with your specific workload bootstrap repository to automatically bootstrap workloads in the provisioned clusters. See Bootstrapping for more details.</p>"},{"location":"pipelines/#adding-waves","title":"Adding Waves","text":"<p>In many case, when enterprises configure their SDLC environments, such as dev/test/staging/prod, each environment may contain more than a single cluster. It is convenient to provision and maintain (update/upgrade) such clusters in parallel within the limits of the environment. Such environments may be represented as waves of the pipeline. The wave concept is not limited to just a logical environment. It may represent any grouping of clusters that should be executed in parallel. An important advantage of running stages in parallel is the time gain associated with it. Each stage may potentially take tens of minutes (e.g. initial provisioning, upgrade, etc.) and as the number of clusters increase, the overall pipeline run may become very lengthy and won't provide enough agility for the enterprise. Running parallel stages within a wave provides roughly the time performance equivalent to a single stage.</p> <p>Pipeline functionality provides wave support to express waves with blueprints. You can mix individual stages and waves together. An individual stage can be viewed as a wave with a single stage.</p> <pre><code>blueprints.CodePipelineStack.builder()\n    .name(\"eks-blueprints-pipeline\")\n    .owner(\"aws-samples\")\n    .repository({\n        //...\n    })\n    .stage({\n        id: 'us-west-1-managed-blueprints',\n        stackBuilder: blueprint.clone('us-west-1')\n    })\n    .wave( {  // adding two clusters for dev env\n        id: \"dev\",\n        stages: [\n            { id: \"dev-west-1\", stackBuilder: blueprint.clone('us-west-1').account(DEV_ACCOUNT)}, // requires trust relationship with the code pipeline role\n            { id: \"dev-east-2\", stackBuilder: blueprint.clone('us-east-2').account(DEV_ACCOUNT)}, // See https://docs.aws.amazon.com/cdk/api/v1/docs/pipelines-readme.html#cdk-environment-bootstrapping\n\n        ]\n    })\n    .wave( {\n        id: \"prod\",\n        stages: [\n            { id: \"prod-west-1\", stackBuilder: blueprint.clone('us-west-1')},\n            { id: \"prod-east-2\", stackBuilder: blueprint.clone('us-east-2')},\n        ]\n    })\n</code></pre>"},{"location":"pipelines/#handling-build-time-access","title":"Handling Build Time Access","text":"<p>CodePipeline leverages CodeBuild to build artifacts. During build time, add-ons may require various look-ups. For example, the add-on or stack may look up VPC, subnets, certificates, hosted zones as well as secrets. </p> <p>By default, such look-ups at build time are restricted by the CodeBuild role created by the pipeline dynamically. That means that builds that require look-ups will fail with access denied exception. </p> <p>Customers can configure the IAM policies used by the CodeBuild as part of the pipeline execution. The framework provides a convenience default build policies <code>blueprints.DEFAULT_BUILD_POLICIES</code> that enable look-ups (including secret look-ups).</p> <p>Example with default policies:</p> <pre><code>const pipeline = blueprints.CodePipelineStack.builder()\n            .name(\"blueprints-pipeline-with-build-roles\")\n            .owner('aws-samples')\n            .codeBuildPolicies(blueprints.DEFAULT_BUILD_POLICIES)\n</code></pre> <p>Example with customer supplied policies:</p> <pre><code>import { PolicyStatement } from 'aws-cdk-lib/aws-iam';\n\nconst pipeline = blueprints.CodePipelineStack.builder()\n            .name(\"blueprints-pipeline-inaction\")\n            .owner('your-owner')\n            .codeBuildPolicies([ \n                new PolicyStatement({\n                    resources: [mySecretArn],\n                    actions: [    \n                        \"secretsmanager:GetSecretValue\",\n                        \"secretsmanager:DescribeSecret\",\n                    ]\n                })\n            ])\n            .build(...)\n</code></pre>"},{"location":"pipelines/#monorepo-support","title":"Monorepo support","text":"<p>In some cases, customers choose to have their blueprints defined in subdirectories on a monorepo project. In this scenario, the root of the project is not the root of the CDK pipeline. A single monorepo can also support multiple subprojects each containing its own pipeline. </p> <p>To support this case, customers can specify the relative path of the subdirectory containing the blueprints. </p> <p>Example pipeline with inline explanation:</p> <pre><code>const blueprint = ...; // define your blueprint\n\nblueprints.CodePipelineStack.builder()\n    .application(\"npx ts-node bin/main.ts\") // optionally set application to point to the launch file. In this case bin/main.ts is located in the path set in the repo configuration  - examples/monorepo/bin/main.ts. Useful when multiple apps are defined. \n    .name(\"blueprints-eks-pipeline\")\n    .owner(\"aws-quickstart\")\n    .codeBuildPolicies(blueprints.DEFAULT_BUILD_POLICIES)\n    .repository({\n        repoUrl: 'cdk-eks-blueprints',\n        credentialsSecretName: 'github-token',\n        targetRevision: 'bug/broken-monorepo-pipelines',\n        path: \"examples/monorepo\", // set the path relative to the root of the monrepo\n        trigger: blueprints.GitHubTrigger.POLL // optionally set trigger, by default it will create a webhook.\n    })\n    .stage({\n        id: 'us-west-2-sandbox',\n        stackBuilder: blueprint.clone('us-west-2')\n    })\n    .build(app, \"pipeline\", { env: { region, account }});\n</code></pre>"},{"location":"pipelines/#build-the-pipeline-stack","title":"Build the pipeline stack","text":"<p>Now that we have defined the blueprint builder, the pipeline with repository and stages we just need to invoke the build() step to create the stack.</p> <pre><code>const blueprint = blueprints.EksBlueprint.builder()\n    ...; // configure your blueprint builder\n\nblueprints.CodePipelineStack.builder()\n    .name(\"eks-blueprints-pipeline\")\n    .owner(\"aws-samples\") // owner of your repo\n    .repository({\n        //  your repo info\n    })\n    .stage({\n        id: 'dev',\n        stackBuilder: blueprint.clone('us-west-1') // clone the blueprint to customize for the stage. You can add more add-ons, teams here.\n    })\n    .stage({\n        id: 'test',\n        stackBuilder: blueprint.clone('us-east-2'), // clone the blueprint to customize for the stage. You can add more add-ons, team, here.\n    })\n    .stage({\n        id: 'prod',\n        stackBuilder: blueprint.clone('us-west-2'), // clone the blueprint to customize for the stage. You can add more add-ons, team, here.\n    })\n    .build(scope, \"blueprints-pipeline-stack\", props); // will produce the self-mutating pipeline in the target region and start provisioning the defined blueprints.\n</code></pre>"},{"location":"pipelines/#deploying-pipelines","title":"Deploying Pipelines","text":"<p>In order to deploy pipelines, each environment (account and region) where pipeline will either be running and each environment to which it will be deploying should be bootstrapped based on CodePipeline documentation.</p> <p>Examples of bootstrapping (from the original documentation):</p> <p>To bootstrap an environment for provisioning the pipeline:</p> <p><pre><code>$ env CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n    [--profile admin-profile-1] \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    aws://111111111111/us-east-1\n</code></pre> To bootstrap a different environment for deploying CDK applications into using a pipeline in account 111111111111:</p> <pre><code>$ env CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n    [--profile admin-profile-2] \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust 11111111111 \\\n    aws://222222222222/us-east-2\n</code></pre> <p>If you only want to trust an account to do lookups (e.g, when your CDK application has a Vpc.fromLookup() call), use the option --trust-for-lookup:</p> <pre><code>$ env CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n    [--profile admin-profile-2] \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust-for-lookup 11111111111 \\\n    aws://222222222222/us-east-2\n</code></pre>"},{"location":"pipelines/#troubleshooting","title":"Troubleshooting","text":"<p>Problem Blueprints Build can fail with AccessDenied exception during build phase. Typical error messages:</p> <pre><code>Error: AccessDeniedException: User: arn:aws:sts::&lt;account&gt;:assumed-role/blueprints-pipeline-stack-blueprintsekspipelinePipelineBuildSynt-1NPFJRH6H7TB1/AWSCodeBuild-e95830ee-07f6-46f5-aaee-90e269c7eb5f is not authorized to perform:\n</code></pre> <pre><code>current credentials could not be used to assume 'arn:aws:iam::&lt;account&gt;:role/cdk-hnb659fds-lookup-role-,account&gt;-eu-west-3', but are for the right account. Proceeding anyway.\n</code></pre> <pre><code>Error: you are not authorized to perform this operation.\n</code></pre> <p>This can happen for a few reasons, but most typical is  related to the stack requiring elevated permissions at build time. Such permissions may be required to perform lookups, such as look up fo VPC, Hosted Zone, Certificate (if imported) and those are handled during stack synthesis.</p> <p>Resolution</p> <p>You can take advantage of supplying the require IAM policies to the pipeline to avoid this error. See this for more details.</p> <p>To address this issue \"manually\" (without the change to the pipeline), you can locate the role leveraged for Code Build and provide ***required permissions. Depending on the scope of the build role, the easiest resolution is to add <code>AdministratorAccess</code> permission to the build role which typically looks similar to this <code>blueprints-pipeline-stack-blueprintsekspipelinePipelineBuildSynt-1NPFJRH6H7TB1</code> provided your pipeline stack was named <code>blueprints-pipeline-stack</code>. If adding administrative access to the role solves the issue, you can then consider tightening the role scope to just the required permissions, such as access to specific resources needed for the build.</p> <p>Problem</p> <p>Failure to create webhook for GitHub sources. When user does not admin rights to a repository or in a more narrow case does not have required permissions to create a webhook, the pipeline fails with an error message similar to this:</p> <pre><code>Webhook could not be registered with GitHub. Error cause: Not found [StatusCode: 404, Body: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/repos/webhooks#create-a-repository-webhook\"}]\n</code></pre> <p>Resolution</p> <ol> <li>Option one: configure proper permissions to create webhook against the target repo.</li> <li>Option two: modify the trigger to use POLL instead of webhook. </li> </ol> <p>For option two here is the example configuration:</p> <pre><code>blueprints.CodePipelineStack.builder()\n    .name(\"blueprints-eks-pipeline\")\n    .owner(...)\n    .repository({\n        ...\n        trigger: blueprints.GitHubTrigger.POLL // optionally set trigger to POLL, by default it will create a webhook.\n    })\n    ...\n</code></pre>"},{"location":"addons/","title":"Add-ons","text":"<p>The <code>eks-blueprints</code> framework leverages a modular approach to managing Add-ons that run within the context of a Kubernetes cluster. Customers are free to select the add-ons that run in each of their blueprint clusters.</p> <p>Within the context of the <code>eks-blueprints</code> framework, an add-on is abstracted as <code>ClusterAddOn</code> interface, and the implementation of the add-on interface can do whatever is necessary to support the desired add-on functionality. This can include applying manifests to a Kubernetes cluster or calling AWS APIs to provision new resources.</p> <p>Here's an improved version of the public documentation abstract with enhanced readability:</p>"},{"location":"addons/#add-on-dependencies-and-ordering-in-eks-blueprints","title":"Add-on Dependencies and Ordering in EKS Blueprints","text":"<p>Add-ons in EKS Blueprints rely on CDK/CloudFormation constructs for provisioning. By default, these constructs don't guarantee a specific order unless explicitly defined using the CDK dependency mechanism.</p> <p>Default Behavior - Add-ons without explicit dependencies are provisioned concurrently in an arbitrary order. - The order in which you add add-ons to the blueprint may not matter if there are no explicit dependencies between them.</p> <p>Lack of explicit dependencies can lead to: - Race conditions - Non-deterministic behavior - Difficult-to-troubleshoot problems</p> <p>For example, if an add-on requires the AWS LoadBalancer Controller to be in place, but there's no explicit dependency, the dependent add-on might start installing before the ALB controller is fully provisioned.</p>"},{"location":"addons/#built-in-dependencies","title":"Built-in Dependencies","text":"<p>Many add-ons in EKS Blueprints have pre-defined dependencies. For example, <code>Istio*</code> add-ons depend on <code>IstioBase</code>, <code>AmpAddOn</code> depends on <code>AdotCollectorAddOn</code>, etc.</p> <p>These dependencies are implemented using the <code>@dependable</code> decorator applied to the <code>deploy</code> method of the dependent add-on:</p> <pre><code>export class AmpAddOn implements ClusterAddOn {\n    @dependable(AdotCollectorAddOn.name)\n    deploy(clusterInfo: ClusterInfo): Promise&lt;Construct&gt; {\n        // Implementation\n    }\n}\n</code></pre>"},{"location":"addons/#custom-ordering","title":"Custom Ordering","text":"<p>For cases where the framework doesn't capture all necessary dependencies, you have two options:</p> <ol> <li>Subclass an add-on and override the <code>deploy</code> method to declare additional dependencies.</li> <li>Use the EKS Blueprints framework's mechanism to create dependencies at the project level.</li> </ol> <p>Creating Dependencies at the Project Level</p> <p>To ensure one add-on is installed before another:</p> <ol> <li>Ensure the prerequisite add-on is added to the blueprint ahead of the dependent add-ons.</li> <li>Mark the prerequisite add-on as \"strictly ordered\" using:</li> </ol> <pre><code>Reflect.defineMetadata(\"ordered\", true, blueprints.addons.PrerequisiteAddOn);\n</code></pre> <p>This ensures that all add-ons declared after the marked add-on will only be provisioned after it's successfully deployed.</p>"},{"location":"addons/#example","title":"Example","text":"<pre><code>// Enable detailed logging\nblueprints.utils.logger.settings.minLevel = 1;\n\n// Mark AwsLoadBalancerControllerAddOn as requiring strict ordering\nReflect.defineMetadata(\"ordered\", true, blueprints.addons.AwsLoadBalancerControllerAddOn);\n\nblueprints.EksBlueprint.builder()\n    .addOns(new VpcCniAddon) // add all add-ons that do NOT need to depend on ALB before the ALB add-on\n    .addOns(new AwsLoadBalancerControllerAddOn())\n    .addOns(new MyAddOn()) // Automatically depends on AwsLoadBalancerControllerAddOn\n    .build(...);\n</code></pre> <p>Note: You can mark multiple add-ons as <code>ordered</code> if needed.</p>"},{"location":"addons/#supported-add-ons","title":"Supported Add-ons","text":"<p>The framework currently supports the following add-ons.</p> Addon Description x86_64/amd64 arm64 <code>ACKAddOn</code> Adds ACK (AWS Controllers for Kubernetes . \u2705 <code>AdotAddOn</code> Adds AWS Distro for OpenTelemetry (ADOT) Operator. \u2705 \u2705 <code>AmpAdotAddOn</code> Deploys ADOT Collector for Prometheus to remote write metrics from AMP. \u2705 \u2705 <code>AppMeshAddOn</code> Adds an AppMesh controller and CRDs. \u2705 <code>ApacheAirflowAddOn</code> This add-on is an implementation of Apache Airflow on EKS using the official helm chart. \u2705 <code>ArgoCDAddOn</code> Provisions Argo CD into your cluster. \u2705 \u2705 <code>AWS Batch for EKS</code> Enables EKS cluster to be used with AWS Batch on EKS \u2705 \u2705 <code>AWS CloudWatch Insgihts</code> Provisions CloudWatch Insights to be used with the EKS cluster. \u2705 \u2705 <code>AWS for Fluent Bit</code> Provisions Fluent Bit into your cluster for log aggregation and consumption. \u2705 \u2705 <code>AWS Load Balancer Controller</code> Provisions the AWS Load Balancer Controller into your cluster. \u2705 \u2705 <code>AWS Node Termination Handler</code> Provisions Node Termination Handler into your cluster. \u2705 <code>AWS Private CA Issuer</code> Installs AWS Private CA Issuer into your cluster. \u2705 <code>Backstage</code> Installs Backstage. \u2705 <code>CertManagerAddOn</code> Adds Certificate Manager to your EKS cluster. \u2705 \u2705 <code>CalicoOperatorAddOn</code> Adds the Calico CNI/Network policy cluster. \u2705 \u2705 <code>CloudWatchAdotAddOn</code> Adds Cloudwatch exporter based on ADOT operator integrating monitoring with CloudWatch. \u2705 \u2705 <code>CloudWatchLogsAddOn</code> Adds AWS for Fluent Bit to the cluster that exports logs to CloudWatch. \u2705 \u2705 <code>ClusterAutoscalerAddOn</code> Adds the standard cluster autoscaler. \u2705 \u2705 <code>ContainerInsightsAddOn</code> Adds support for container insights. \u2705 \u2705 <code>CoreDnsAddOn</code> Adds CoreDNS Amazon EKS add-on. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. \u2705 \u2705 <code>DatadogAddOn</code> Adds Datadog Amazon EKS add-on. Datadog is the monitoring and security platform for cloud applications. \u2705 \u2705 <code>Dynatrace</code> Adds the Dynatrace OneAgent Operator. \u2705 <code>EbsCsiDriverAddOn</code> Adds EBS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EBS volumes for persistent storage. \u2705 \u2705 <code>EfsCsiDriverAddOn</code> Adds EFS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EFS volumes for persistent storage. \u2705 \u2705 <code>EmrOnEksAddOn</code> Enable EKS cluster to be used with EMR on EKS \u2705 \u2705 <code>ExternalDnsAddOn</code> Adds External DNS support for AWS to the cluster, integrating with Amazon Route 53. \u2705 \u2705 <code>ExternalSecretsAddOn</code> Adds External Secrets Operator to the cluster. \u2705 \u2705 <code>FluxcdAddOn</code> Setting up Fluxcd to manage one or more Kubernetes clusters. \u2705 \u2705 <code>GpuOperatorAddon</code> Deploys NVIDIA GPU Operator on your EKS Cluster to manage configuration of drivers and software dependencies for GPU instances \u2705 \u2705 <code>GrafanaOperatorAddon</code> Deploys GrafanaOperatorAddon  on your EKS Cluster to manage Amazon Managed Grafana and other external Grafana instances. \u2705 \u2705 <code>IngressNginxAddOn</code> Adds Kubernetes NGINX ingress controller \u2705 \u2705 <code>IstioBaseAddOn</code> Adds support for Istio base chart to the EKS cluster. \u2705 \u2705 <code>InstanaAddOn</code> Adds the IBM\u00ae Instana\u00ae Agent Operator to the EKS cluster. \u2705 \u2705 <code>IstioControlPlaneAddOn</code> Installs Istio Control Plane addon to the EKS cluster. \u2705 \u2705 <code>IstioCniAddOn</code> Installs Istio Cni Plugin addon to the EKS cluster. \u2705 \u2705 <code>IstioIngressGatewayAddOn</code> Installs Istio Ingress Gateway Plugin to the EKS cluster. \u2705 \u2705 <code>JupyterHubAddOn</code> Adds JupyterHub support for AWS to the cluster. \u2705 \u2705 <code>Kasten-K10AddOn</code> Kasten K10 add-on installs Kasten K10 into your Amazon EKS cluster. \u2705 <code>KedaAddOn</code> Installs Keda into EKS cluster. \u2705 \u2705 <code>Keptn</code> Keptn Control Plane and Execution Plane AddOn. \u2705 <code>KnativeAddOn</code> Deploys the KNative Operator to ease setting up the rest of KNatives CRDs \u2705 \u2705 <code>KomodorAddOn</code> Adds the Komodor Agent to the EKS Cluster. \u2705 \u2705 <code>KonveyorAddOn</code> Adds Konveyor to the EKS Cluster. \u2705 \u2705 <code>KubecostAddOn</code> Adds Kubecost cost analyzer to the EKS cluster. \u2705 <code>KubeflowAddOn</code> Adds kubeflow Kubeflow pipeline addon the EKS cluster. \u2705 <code>KubeRayAddOn</code> Installs the KubeRay Operator. \u2705 \u2705 <code>KubeviousAddOn</code> Adds Kubevious open source Kubernetes dashboard to an EKS cluster. \u2705 <code>KarpenterAddOn</code> Adds Karpenter support for Amazon EKS. \u2705 \u2705 <code>KubeProxyAddOn</code> Adds kube-proxy Amazon EKS add-on. Kube-proxy maintains network rules on each Amazon EC2 node. \u2705 \u2705 <code>KubeStateMetricsAddOn</code> Adds kube-state-metrics into the EKS cluster. \u2705 \u2705 <code>KubesharkAddOn</code> Deep visibility and monitoring of all API traffic \u2705 \u2705 <code>MetricsServerAddOn</code> Adds metrics server (pre-req for HPA and other monitoring tools). \u2705 \u2705 <code>NewRelicAddOn</code> Adds New Relic and Pixie observability for Amazon EKS. \u2705 <code>NginxAddOn</code> Adds NGINX ingress controller \u2705 \u2705 <code>NeuronDevicePluginAddOn</code> Adds Neuron Device Plugin Addon \u2705 <code>NeuronMonitorAddOn</code> Adds Neuron Monitor Addon \u2705 <code>OpaGatekeeperAddOn</code> Adds OPA Gatekeeper \u2705 \u2705 <code>ParalusAddOn</code> Adds Paralus \u2705 \u2705 <code>PixieAddOn</code> Adds Pixie to the EKS Cluster. Pixie provides auto-telemetry for requests, metrics, application profiles, and more. \u2705 <code>PrometheusNodeExporterAddOn</code> Adds prometheus-node-exporter to the EKS Cluster. Prometheus Node Exporter enables you to measure various machine resources such as memory, disk and CPU utilization. \u2705 \u2705 <code>Rafay</code> Adds Rafay\u2019s Kubernetes Operations Platform (KOP) to the EKS Cluster.  Rafay allows you to deploy, operate, and manage the lifecycle of Kubernetes clusters \u2705 <code>SecretsStoreAddOn</code> Adds AWS Secrets Manager and Config Provider for Secret Store CSI Driver to the EKS Cluster. \u2705 \u2705 <code>Snyk</code> Adds the Snyk Monitor to the EKS Cluster. \u2705 <code>SSMAgentAddOn</code> Adds Amazon SSM Agent to worker nodes. \u2705 <code>UpboundUniversalCrossplaneAddOn</code> Allows Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Crossplane distribution. \u2705 <code>VpcCniAddOn</code> Adds the Amazon VPC CNI Amazon EKS addon to support native VPC networking for Amazon EKS. \u2705 \u2705 <code>VeleroAddOn</code> Adds Velero to the EKS Cluster. \u2705 \u2705 <code>XrayAddOn</code> Adds XRay Daemon to the EKS Cluster. NA NA <code>XrayAdotAddOn</code> Deploys ADOT Collector for Xray to receive traces from your workloads. \u2705 \u2705 ~~<code>GmaestroAddOn</code>~~ Deprecated due to EOL. Adds gMaestro cost optimization solution for EKS cluster. <code>EksPodIdentityAgentAddOn</code> Setting up the EKS Pod Identity Agent \u2705 \u2705"},{"location":"addons/#standard-helm-add-on-configuration-options","title":"Standard Helm Add-On Configuration Options","text":"<p>Many add-ons leverage helm to provision and maintain deployments. All provided add-ons that leverage helm allow specifying the following add-on attributes:</p> <pre><code>    /**\n     * Name of the helm chart (add-on)\n     */\n    name?: string,\n\n    /**\n     * Namespace where helm release will be installed\n     */\n    namespace?: string,\n\n    /**\n     * Chart name\n     */\n    chart?: string,\n\n    /**\n     * Helm chart version.\n     */\n    version?: string,\n\n    /**\n     * Helm release\n     */\n    release?: string,\n\n    /**\n     * Helm repository\n     */\n    repository?: string,\n\n    /**\n     * When global helm version validation is enabled with HelmAddOn.validateHelmVersions = true\n     * allows to skip validation for a particular helm add-on. \n     */\n    skipVersionValidation?: boolean,\n\n    /**\n     * Optional values for the helm chart.\n     */\n    values?: Values\n</code></pre> <p>Ability to set repository url may be leveraged for private repositories.</p> <p>Version field can be modified from the default chart version, e.g. if the add-on should be upgraded to the desired version, however, since the helm chart version supplied by the customer may not have been tested as part of the Blueprints release process, Blueprints community may not be able to reproduce/fix issues related to the helm chart version upgrade.</p>"},{"location":"addons/#helm-version-validation","title":"Helm Version Validation","text":"<p>All add-ons that derive from <code>HelmAddOn</code> support optional version validation against the latest published version in the target helm repository. </p> <p>Helm version validation can result either in a warning on console during <code>list</code>, <code>synth</code> and <code>deploy</code> operations or an exception if the target helm repository contains higher version than the one leveraged in the add-on. </p> <p>Example output:</p> <pre><code>INFO  Chart argo-cd-4.9.12 is at the latest version. \nINFO  Chart external-dns-6.6.0 is at the latest version. \nWARN Upgrade is needed for chart gatekeeper-3.8.1: latest version is 3.9.0-beta.2. \nINFO  Chart appmesh-controller-1.5.0 is at the latest version. \nINFO  Chart tigera-operator-v3.23.2 is at the latest version. \nWARN Upgrade is needed for chart adot-exporter-for-eks-on-ec2-0.1.0: latest version is 0.6.0. \nINFO  Chart aws-load-balancer-controller-1.4.2 is at the latest version. \nINFO  Chart nginx-ingress-0.14.0 is at the latest version. \nINFO  Chart velero-2.30.1 is at the latest version. \nINFO  Chart falco-1.19.4 is at the latest version. \nWARN Upgrade is needed for chart karpenter-0.13.1: latest version is 0.13.2. \nINFO  Chart kubevious-1.0.10 is at the latest version. \nINFO  Chart aws-efs-csi-driver-2.2.7 is at the latest version. \nINFO  Chart keda-2.7.2 is at the latest version. \nINFO  Chart secrets-store-csi-driver-1.2.1 is at the latest version. \n</code></pre> <ul> <li>Enable/Disable Helm version validation globally</li> </ul> <pre><code>import { HelmAddOn } from '@aws-quickstart/eks-blueprints';\n\nHelmAddOn.validateHelmVersions = true; // by default will print out warnings\nHelmAddOn.failOnVersionValidation = true; // enable synth to throw exceptions on validation check failures\n</code></pre> <ul> <li>Enable/Disable Helm version validation per add-on</li> </ul> <pre><code>new blueprints.addons.MetricsServerAddOn({\n    skipVersionValidation: true\n})\n</code></pre>"},{"location":"addons/ack-addon/","title":"AWS Controller for Kubernetes Add-on","text":"<p>This add-on installs aws-controller-8s.</p> <p>AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster.</p> <p>ACK is an open source project built with \u2764\ufe0f by AWS. The project is composed of many source code repositories containing a common runtime, a code generator, common testing tools and Kubernetes custom controllers for individual AWS service APIs.</p>"},{"location":"addons/ack-addon/#usage","title":"Usage","text":"<p>Pattern # 1 : This installs AWS Controller for Kubernetes for IAM ACK Controller. This uses all default parameters for installation of the IAM Controller.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AckAddOn({\n  serviceName: AckServiceName.IAM,\n}),\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern # 2 : This installs AWS Controller for Kubernetes for EC2 ACK controller using service name internally referencing service mapping values for helm options. After Installing this EC2 ACK Controller, the instructions in Provision ACK Resource can be used to provision EC2 namespaces <code>SecurityGroup</code> resources required for creating Amazon RDS database as an example.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AckAddOn({\n  id: \"ec2-ack\", // Having this field is important if you are using multiple iterations of this Addon.\n  createNamespace: false, //This is essential if you are using multiple iterations of this Addon to run in same namespace.\n  serviceName: AckServiceName.EC2 // This value can be references from supported service section below,\n}),\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern # 3 : This installs AWS Controller for Kubernetes for S3 ACK controller with user specified values. After Installing this S3 ACK Controller, the instructions in Provision ACK Resource can be used to provision Amazon S3 resources using the S3 ACK controller as an example.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as iam from \"aws-cdk-lib/aws-iam\";\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AckAddOn({\n  id: \"s3-ack\",\n  serviceName: AckServiceName.S3,\n  name: \"s3-chart\",\n  chart: \"s3-chart\",\n  version: \"v0.1.1\",\n  release: \"s3-chart\",\n  repository: \"oci://public.ecr.aws/aws-controllers-k8s/s3-chart\",\n  managedPolicyName: \"AmazonS3FullAccess\",\n  inlinePolicyStatements: [\n    iam.PolicyStatement.fromJson({\n      \"Sid\": \"S3AllPermission\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:*\",\n        \"s3-object-lambda:*\"\n      ],\n      \"Resource\": \"*\"\n    }),\n    iam.PolicyStatement.fromJson({\n      \"Sid\": \"S3ReplicationPassRole\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"iam:PassedToService\": \"s3.amazonaws.com\"\n        }\n      },\n      \"Action\": \"iam:PassRole\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    })\n  ],\n  createNamespace: false,\n  saName: \"s3-chart\"\n})\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/ack-addon/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>id</code>: Unique identifier of the Addon especially if you are using ACK Addon multiple times</li> <li><code>serviceName</code>: Name of the service and this is mandatory</li> <li><code>name</code>: Name of the ACK Chart</li> <li><code>chart</code>: Chart Name of the ACK Chart</li> <li><code>version</code>: Version of the ACK Chart</li> <li><code>release</code>: Release Name of the ACK Chart</li> <li><code>repository</code>: Repository URI of the specific ACK Chart</li> <li><code>managedPolicyName</code>: Policy Name required to be added to the IAM role for that ACK</li> <li><code>inlinePolicyStatements</code>: Inline Policy Statements required to be added to the IAM role for that ACK</li> <li><code>createNamespace</code>: (boolean) This should be false if you are using for the second time</li> <li><code>saName</code> : Name to create the service account.</li> <li><code>values</code>: Arbitrary values to pass to the chart</li> <li>Standard helm configuration options.</li> </ul>"},{"location":"addons/ack-addon/#validation","title":"Validation","text":"<p>To validate that ack-controller-k8s is installed properly in the cluster, check if the namespace is created and pods are running in the <code>ack-system</code> namespace.</p> <p>Verify if the namespace is created correctly <pre><code>  kubectl get all -n ack-system\n</code></pre> There should be list the following resources in the namespace <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\npod/iam-chart-64c8fd7f6-wpb5k    1/1     Running   0          34m\npod/rds-chart-5f6f5b8fc7-hp55l   1/1     Running   0          5m26s\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/iam-chart   1/1     1            1           35m\ndeployment.apps/rds-chart   1/1     1            1           5m36s\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/iam-chart-64c8fd7f6    1         1         1       35m\nreplicaset.apps/rds-chart-5f6f5b8fc7   1         1         1       5m36s\n</code></pre></p>"},{"location":"addons/ack-addon/#aws-controller-8s-references","title":"aws-controller-8s references","text":"<p>Please refer to following aws-controller-8s references for more information : - ACK Workshop - ECR Gallery for ACK  - ACK GitHub</p>"},{"location":"addons/ack-addon/#supported-aws-services-by-ack-addon","title":"Supported AWS Services by ACK Addon","text":"<p>You can use this ACK Addon today to provision resources for below mentioned 22 AWS services:</p> <ol> <li>ACM</li> <li>ACMPCA</li> <li>APIGATEWAYV2</li> <li>APPLICATIONAUTOSCALING</li> <li>CLOUDTRAIL</li> <li>CLOUDWATCH</li> <li>CLOUDWATCHLOGS</li> <li>DYNAMODB</li> <li>EC2</li> <li>ECR</li> <li>EMRCONTAINERS</li> <li>EKS</li> <li>ELASTICACHE</li> <li>ELASTICSEARCHSERVICE</li> <li>EVENTBRIDGE</li> <li>IAM</li> <li>KAFKA</li> <li>KINESIS</li> <li>KMS</li> <li>LAMBDA</li> <li>MEMORYDB</li> <li>MQ</li> <li>OPENSEARCHSERVICE</li> <li>PIPES</li> <li>PROMETHEUSSERVICE</li> <li>RDS</li> <li>ROUTE53</li> <li>ROUTE53RESOLVER</li> <li>S3</li> <li>SAGEMAKER</li> <li>SECRETSMANAGER</li> <li>SFN</li> <li>SNS</li> <li>SQS</li> </ol> <p>We highly recommend you to contribute to this ACK Addon whenever there is a newer service or new version of supported service by this Addon is published to ECR Gallery for ACK.</p>"},{"location":"addons/adot-addon/","title":"AWS Distro for OpenTelemetry (ADOT) Add-on","text":"<p>Amazon EKS supports using Amazon EKS API to install and manage the AWS Distro for OpenTelemetry (ADOT) Operator. This enables a simplified experience for instrumenting your applications running on Amazon EKS to send metric and trace data to multiple monitoring service options like Amazon CloudWatch, Prometheus, and X-Ray. </p> <p>This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage ADOT Collectors. For creating add-on in specific namespace, <code>namespace</code> property needs to be passed.</p> <p>For more information on the add-on, please review the user guide.</p>"},{"location":"addons/adot-addon/#prerequisites","title":"Prerequisites","text":"<ul> <li>The ADOT Operator uses admission webhooks to mutate and validate the Collector Custom Resource (CR) requests. In Kubernetes, the webhook requires a TLS certificate that the API server is configured to trust. There are multiple ways for you to generate the required TLS certificate. However, the default method is to install the latest version of the cert-manager. You can install Certificate Manager using this user guide or you can use <code>certificate-manager</code> EKS blueprints addon which should be added before ADOT addon.</li> </ul>"},{"location":"addons/adot-addon/#usage","title":"Usage","text":""},{"location":"addons/adot-addon/#example-with-default-namespace","title":"Example with default namespace","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AdotCollectorAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/adot-addon/#example-with-non-default-namespace","title":"Example with non-default namespace","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AdotCollectorAddOn({\n                namespace:'adot', //User supplied, non-default namespace\n                version: 'v0.80.0-eksbuild.2'\n              }),\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/adot-addon/#validation","title":"Validation","text":"<p>To validate that ADOT add-on is installed properly, ensure that the ADOT kubernetes resources are running in the cluster</p> <pre><code>kubectl get all -n opentelemetry-operator-system\n</code></pre>"},{"location":"addons/adot-addon/#output","title":"Output","text":"<pre><code>NAME                                                             READY   STATUS    RESTARTS   AGE\npod/opentelemetry-operator-controller-manager-845cbd7bf7-b5s9l   2/2     Running   0          140m\n\nNAME                                                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/opentelemetry-operator-controller-manager-metrics-service   ClusterIP   172.20.210.200   &lt;none&gt;        8443/TCP   140m\nservice/opentelemetry-operator-webhook-service                      ClusterIP   172.20.56.72     &lt;none&gt;        443/TCP    140m\n\nNAME                                                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/opentelemetry-operator-controller-manager   1/1     1            1           140m\n\nNAME                                                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/opentelemetry-operator-controller-manager-845cbd7bf7   1         1         1       140m\n</code></pre>"},{"location":"addons/adot-addon/#testing","title":"Testing","text":"<p>Additionally, the <code>aws cli</code> can be used to determine which version of the add-on is installed in the cluster. <pre><code># Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on\naws eks describe-addon \\\n    --cluster-name my-cluster \\\n    --addon-name adot \\\n    --query \"addon.addonVersion\" \\\n    --output text\n\n# Output\nv0.51.0-eksbuild.1\n</code></pre></p>"},{"location":"addons/adot-addon/#functionality","title":"Functionality","text":"<p>Applies the ADOT add-on to an Amazon EKS cluster. </p>"},{"location":"addons/amp-addon/","title":"Amazon Managed Service for Prometheus (AMP) ADOT Add-on","text":"<p>Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for Amazon Managed Service for Prometheus (AMP) which receives OTLP metrics from the application and Prometheus metrics scraped from pods on the cluster and remote writes the metrics to AMP remote write endpoint. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy.</p> <p>This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup AMP for remote write of metrics.</p> <p>For more information on the add-on, please review the user guide. please review the Amazon Managed Service for Prometheus supported regions documentation page for more information.</p>"},{"location":"addons/amp-addon/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>adot</code> EKS Blueprints add-on.</li> <li><code>kube-state-metrics</code> EKS Blueprints add-on.</li> <li><code>prometheus-node-explorter</code> EKS Blueprints add-on.</li> </ul>"},{"location":"addons/amp-addon/#usage","title":"Usage","text":"<p>This add-on can used with four different patterns :</p> <p>Pattern #1: Simple and Easy - Using all default property values. This pattern creates a new AMP workspace with default property values such as <code>workspaceName</code>, <code>namespace</code> with no tags on the AMP workspace and deploys an ADOT collector in the <code>default</code> namespace with <code>deployment</code> as the mode to remote write metrics to AMP workspace. </p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AmpAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>With the same pattern, to deploy ADOT collector in non-default namespace:</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AmpAddOn({\n                ampPrometheusEndpoint: ampWorkspace.attrPrometheusEndpoint,\n                namespace: 'adot'\n              }),\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern #2: Overriding property values for Name and Tags for a custom AMP Workspace name and tags. This pattern creates a new AMP workspace with property values passed on such as <code>workspaceName</code>, <code>tags</code> and deploys an ADOT collector on the namespace specified in <code>namespace</code> with name in <code>name</code> and <code>deployment</code> as the mode to remote write metrics to AMP workspace.</p> <p><pre><code>import * as cdk from 'aws-cdk-lib';\nimport { CfnWorkspace } from 'aws-cdk-lib/aws-aps';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\nconst ampWorkspaceName = \"blueprints-amp-workspace\";\nconst ampWorkspace: CfnWorkspace = blueprints.getNamedResource(ampWorkspaceName);\n\nconst addOn = new blueprints.addons.AmpAddOn({\n    ampPrometheusEndpoint: ampWorkspace.attrPrometheusEndpoint,\n    deploymentMode: DeploymentMode.DEPLOYMENT,\n    namespace: 'default',\n    name: 'adot-collector-amp'\n})\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .resourceProvider(ampWorkspaceName, new blueprints.CreateAmpProvider(ampWorkspaceName, ampWorkspaceName, [\n    {\n        key: 'Name',\n        value: 'Sample-AMP-Workspace',\n    },\n    {\n        key: 'Environment',\n        value: 'Development',\n    },\n    {\n        key: 'Department',\n        value: 'Operations',\n    }\n  ]))\n  .build(app, 'my-stack-name');\n</code></pre> Pattern #3: Passing on AMP Remote Endpoint of an existing AMP workspace to be used to remote write metrics. This pattern does not create an AMP workspace. Deploys an ADOT collector on the namespace specified in <code>namespace</code> with name in <code>name</code> and <code>deployment</code> as the mode to remote write metrics to AMP workspace of the URL passed as input. This pattern ignores any other property values passed if <code>ampPrometheusEndpoint</code> is present.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst ampPrometheusEndpoint = \"https://aps-workspaces.us-west-2.amazonaws.com/workspaces/ws-e859f589-7eed-43c1-a82b-58f44119f17d\";\n\nconst addOn = new blueprints.addons.AmpAddOn({\n    ampPrometheusEndpoint: ampPrometheusEndpoint,\n    deploymentMode: DeploymentMode.DEPLOYMENT,\n    namespace: 'default',\n    name: 'adot-collector-amp'\n})\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern #4: Overriding property values for different deployment Modes. This pattern creates a new AMP workspace with property values passed on such as <code>workspaceName</code>, <code>tags</code> and deploys an ADOT collector on the namespace specified in <code>namespace</code> with name in <code>name</code> and <code>daemonset</code> as the mode to remote write metrics to AMP workspace. Deployment modes can be overridden to any of these values - <code>deployment</code>, <code>daemonset</code>, <code>statefulset</code>, <code>sidecar</code>.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport { CfnWorkspace } from 'aws-cdk-lib/aws-aps';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\nconst ampWorkspaceName = \"blueprints-amp-workspace\";\nconst ampWorkspace: CfnWorkspace = blueprints.getNamedResource(ampWorkspaceName);\n\nconst addOn = new blueprints.addons.AmpAddOn({\n    ampPrometheusEndpoint: ampWorkspace.attrPrometheusEndpoint,\n    deploymentMode: DeploymentMode.DAEMONSET,\n    namespace: 'default',\n    name: 'adot-collector-amp'\n    // deploymentMode: DeploymentMode.DEPLOYMENT\n    // deploymentMode: DeploymentMode.STATEFULSET\n    // deploymentMode: DeploymentMode.SIDECAR\n})\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .resourceProvider(ampWorkspaceName, new blueprints.CreateAmpProvider(ampWorkspaceName, ampWorkspaceName,{\n    {\n        key: 'Name',\n        value: 'Sample-AMP-Workspace',\n    },\n    {\n        key: 'Environment',\n        value: 'Development',\n    },\n    {\n        key: 'Department',\n        value: 'Operations',\n    }\n  }))\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern #5: Configuring rules into AMP. This pattern creates recording rules and/or alerting rules, based on the YAML files provided. A workspace ARN is also required as input.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport { CfnWorkspace } from 'aws-cdk-lib/aws-aps';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\nconst ampWorkspaceName = \"blueprints-amp-workspace\";\nconst ampWorkspace = blueprints.getNamedResource(ampWorkspaceName);\nconst attrPrometheusEndpoint = ampWorkspace.attrPrometheusEndpoint;\nconst ampWorkspaceArn = ampWorkspace.attrArn;\n\nconst addOn = new blueprints.addons.AmpAddOn({\n    ampPrometheusEndpoint: attrPrometheusEndpoint,\n    ampRules: {\n        ampWorkspaceArn: ampWorkspaceArn,\n        ruleFilePaths: [\n            __dirname + '/../common/resources/amp-config/alerting-rules.yml',\n            __dirname + '/../common/resources/amp-config/recording-rules.yml'\n        ]\n    }\n})\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .resourceProvider(ampWorkspaceName, new blueprints.CreateAmpProvider(ampWorkspaceName, ampWorkspaceName))\n  .build(app, 'my-stack-name');\n</code></pre> <p>The recording rules and alerting rules files should be placed in the same repository that is using this add-on. They should also follow the same format as rules files in standalone Prometheus, like explained in the AMP User Guide:</p> <pre><code>groups:\n  - name: test\n    rules:\n    - record: metric:recording_rule\n      expr: avg(rate(container_cpu_usage_seconds_total[5m]))\n  - name: alert-test\n    rules:\n    - alert: metric:alerting_rule\n      expr: avg(rate(container_cpu_usage_seconds_total[5m])) &gt; 0\n      for: 2m\n</code></pre> <p>An example of rules configuration can be found in the data section of the EKS monitoring rules file in the aws-observability repository.</p> <p>Pattern #6: Configuring the AWS Distro for OpenTelemetry Collector. This pattern enables you to configure the Collector by providing a manifest describing an OpenTelemetryCollector resource:</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\n  name: sidecar-for-my-app\nspec:\n  mode: sidecar\n  config: |\n    ...\n    {{javaScrapeSampleLimit}}\n    ...\n    {{javaPrometheusMetricsEndpoint}}\n</code></pre> <p>This pattern is useful when you need to customize the configuration of the OpenTelemetryCollector, e.g. to add specific scraping targets and parameters in the Prometheus receiver configuration, for a job capturing metrics of a Java workload.</p> <p>The optional <code>openTelemetryCollector.manifestParameterMap</code> parameter allows you to define a map where the keys can be used (in double curly braces) within the OpenTelemetryCollector manifest as e.g. {{javaScrapeSampleLimit}}, to be replaced by the corresponding values.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport { CfnWorkspace } from 'aws-cdk-lib/aws-aps';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\nconst ampWorkspaceName = \"blueprints-amp-workspace\";\nconst attrPrometheusEndpoint = blueprints.getNamedResource(ampWorkspaceName).attrPrometheusEndpoint;\n\nconst addOn = new blueprints.addons.AmpAddOn({\n    ampPrometheusEndpoint: attrPrometheusEndpoint,\n    openTelemetryCollector: {\n        manifestPath: __dirname + '/../common/resources/otel-collector-config.yml',\n        manifestParameterMap: {\n            javaScrapeSampleLimit: 1000,\n            javaPrometheusMetricsEndpoint: \"/metrics\"\n        }\n    }\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .resourceProvider(ampWorkspaceName, new blueprints.CreateAmpProvider(ampWorkspaceName, ampWorkspaceName))\n  .build(app, 'my-stack-name');\n</code></pre> <p>If you need to configure rules, please do not use the rule_files field like in standalone Prometheus, but rather use the ampRules parameter.</p>"},{"location":"addons/amp-addon/#validation","title":"Validation","text":"<p>To validate that AMP add-on is installed properly, ensure that the required kubernetes resources are running in the cluster:</p> <pre><code>kubectl get all -n default\n</code></pre> <pre><code>NAME                                              READY   STATUS        RESTARTS   AGE\npod/otel-collector-amp-collector-7877b86dd4-z9ds5   1/1     Running       0          31m\n\nNAME                                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/kubernetes                              ClusterIP   172.20.0.1       &lt;none&gt;        443/TCP    4h35m\nservice/otel-collector-amp-collector-monitoring ClusterIP   172.20.216.242   &lt;none&gt;        8888/TCP   31m\n\nNAME                                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/otel-collector-amp-collector   1/1     1            1           31m\n\nNAME                                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/otel-collector-amp-collector-7877b86dd4   1         1         1       31m\n</code></pre>"},{"location":"addons/amp-addon/#testing","title":"Testing","text":"<p>To test whether Amazon Managed Service for Prometheus received the metrics, Please use the following commands: For instructions on installing awscurl, see awscurl.</p> <pre><code>AMP_WORKSPACE_NAME=\"blueprints-amp-workspace\" \n# The above should be replaced with your AMP workspace name if you are passing remote write URL specified in Pattern #3.\nWORKSPACE_ID=$(aws amp list-workspaces \\\n  --alias $AMP_WORKSPACE_NAME --region=${AWS_REGION} --query 'workspaces[0].[workspaceId]' --output text)\nAMP_ENDPOINT_QUERY=https://aps-workspaces.$AWS_REGION.amazonaws.com/workspaces/$WORKSPACE_ID/api/v1/query\\?query=\nawscurl --service=\"aps\" --region=${AWS_REGION} ${AMP_ENDPOINT_QUERY}up\n</code></pre>"},{"location":"addons/amp-addon/#functionality","title":"Functionality","text":"<p>Applies the Amazon Managed Service for Prometheus (AMP) add-on to an Amazon EKS cluster. </p>"},{"location":"addons/apache-airflow/","title":"Apache Airflow Add-on","text":"<p>Apache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. Airflow contains extensible framework that allows for building workflows connecting with many technologies. For more information on Airflow, please consult the official documentation.</p> <p>This add-on is an implementation of Apache Airflow on EKS using the official helm chart.</p>"},{"location":"addons/apache-airflow/#prerequisites","title":"Prerequisites","text":"<ol> <li>If you are using an S3 bucket (for logging storage) or EFS File System (for DAGs persistent storage), you must register the resources using Resource Provider. The framework provides S3 and EFS Resource Provider.</li> <li>If you are using an EFS File System, you must include an EFS CSI Driver in the add-on array, otherwise will run into the following error: <code>Missing a dependency: EfsCsiDriverAddOn. Please add it to your list of addons.</code>.</li> <li>If you are using an Ingress with AWS Load Balancer, you must include an AWS Load Balacner Controller in the add-on array, otherwise you will run into the following error: <code>Missing a dependency: AwsLoadBalancerControllerAddOn. Please add it to your list of addons</code>.</li> </ol>"},{"location":"addons/apache-airflow/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst apacheAirflowS3Bucket = new blueprints.CreateS3BucketProvider({\n    id: 'apache-airflow-s3-bucket-id',\n    s3BucketProps: { removalPolicy: cdk.RemovalPolicy.DESTROY }\n});\nconst apacheAirflowEfs = new blueprints.CreateEfsFileSystemProvider({\n    name: 'blueprints-apache-airflow-efs',    \n});\n\nconst addOn = [new blueprints.EfsCsiDriverAddOn(),\n    new blueprints.ApacheAirflowAddOn({\n        enableLogging: true,\n        s3Bucket: 'airflow-logging-s3-bucket',\n        enableEfs: true,\n        efsFileSystem: 'apache-airflow-efs-provider',\n    })\n];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .resourceProvider('apache-airflow-s3-bucket-provider', apacheAirflowS3Bucket)\n  .resourceProvider('apache-airflow-efs-provider', apacheAirflowEfs)\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/apache-airflow/#validation","title":"Validation","text":"<p>To validate that the Airflow add-on is installed properly, ensure that the required kubernetes resources are running in the cluster:</p> <pre><code>kubectl get all -n airflow\n</code></pre> <pre><code>NAME                                                             READY   STATUS    RESTARTS     AGE\npod/blueprints-addon-apache-airflow-postgresql-0                 1/1     Running   0            1m4s\npod/blueprints-addon-apache-airflow-scheduler-697958497d-tbblk   2/2     Running   0            1m4s\npod/blueprints-addon-apache-airflow-statsd-5b97b9fcb4-9r8qc      1/1     Running   0            1m4s\npod/blueprints-addon-apache-airflow-triggerer-86b94646c6-xrjnn   2/2     Running   0            1m4s\npod/blueprints-addon-apache-airflow-webserver-6b8db695fc-9d87q   1/1     Running   0            1m4s\n\nNAME                                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nservice/blueprints-addon-apache-airflow-postgresql      ClusterIP   172.20.155.11    &lt;none&gt;        5432/TCP            1m4s\nservice/blueprints-addon-apache-airflow-postgresql-hl   ClusterIP   None             &lt;none&gt;        5432/TCP            1m4s\nservice/blueprints-addon-apache-airflow-statsd          ClusterIP   172.20.247.149   &lt;none&gt;        9125/UDP,9102/TCP   1m4s\nservice/blueprints-addon-apache-airflow-webserver       ClusterIP   172.20.211.66    &lt;none&gt;        8080/TCP            1m4s\n\nNAME                                                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/blueprints-addon-apache-airflow-scheduler   1/1     1            1           1m4s\ndeployment.apps/blueprints-addon-apache-airflow-statsd      1/1     1            1           1m4s\ndeployment.apps/blueprints-addon-apache-airflow-triggerer   1/1     1            1           1m4s\ndeployment.apps/blueprints-addon-apache-airflow-webserver   1/1     1            1           1m4s\n\nNAME                                                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/blueprints-addon-apache-airflow-scheduler-669dff9c6d   0         0         0       1m4s\nreplicaset.apps/blueprints-addon-apache-airflow-scheduler-697958497d   1         1         1       1m4s\nreplicaset.apps/blueprints-addon-apache-airflow-statsd-5b97b9fcb4      1         1         1       1m4s\nreplicaset.apps/blueprints-addon-apache-airflow-triggerer-7b7c69486d   0         0         0       1m4s\nreplicaset.apps/blueprints-addon-apache-airflow-triggerer-86b94646c6   1         1         1       1m4s\nreplicaset.apps/blueprints-addon-apache-airflow-webserver-5db49dcb94   0         0         0       1m4s\nreplicaset.apps/blueprints-addon-apache-airflow-webserver-6b8db695fc   1         1         1       1m4s\n\nNAME                                                          READY   AGE\nstatefulset.apps/blueprints-addon-apache-airflow-postgresql   1/1     1m4s\n</code></pre>"},{"location":"addons/apache-airflow/#testing","title":"Testing","text":"<p>To test the Airflow functionality, expose the webserver by port forwarding:</p> <pre><code>kubectl port-forward service/blueprints-addon-apache-airflow-webserver -n airflow 8080:8080\n</code></pre> <p>You should be able to access the webserver via your browser: <code>http://localhost:8080</code>. Default username and password are both <code>admin</code>.</p>"},{"location":"addons/apache-airflow/#functionality","title":"Functionality","text":"<p>Applies the Apache Airflow add-on to an Amazon EKS cluster. Optionally, you can leverage integrations with the following AWS services out of the box:</p> <ol> <li>S3 Bucket for storing server, worker, and scheduler logs remotely.</li> <li>EFS File System for storing DAGs</li> <li>AWS Load Balancer for ingress to the webserver</li> <li>A Certificate for AWS Certificate Manager for SSL</li> </ol>"},{"location":"addons/app-mesh/","title":"AWS App Mesh Add-on","text":"<p>AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. The App Mesh add-on provisions the necessary AWS resources and Helm charts into an EKS cluster that are needed to support App Mesh for EKS workloads. </p> <p>Full documentation on using App Mesh with EKS can be found here.</p>"},{"location":"addons/app-mesh/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AppMeshAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/app-mesh/#functionality","title":"Functionality","text":"<ol> <li>Creates an App Mesh IAM service account.</li> <li>Adds both <code>AWSCloudMapFullAccess</code> and <code>AWSAppMeshFullAccess</code> roles to the service account.</li> <li>Adds <code>AWSXRayDaemonWriteAccess</code> to the instance role if XRay integration is enabled.</li> <li>Creates the <code>appmesh-system</code> namespace.</li> <li>Deploys the <code>appmesh-controller</code> Helm chart into the cluster.</li> <li>Supports standard helm configuration options.</li> </ol>"},{"location":"addons/app-mesh/#app-mesh-sidecar-injection","title":"App Mesh Sidecar Injection","text":"<p>You can configure certain namespaces for automatic injection of App Mesh sidecar (Envoy) proxy. This will enable handling cross-cutting aspects such as service to service communication, resiliency patterns (circuit breaker/retries) as well handle ingress and egress for the workloads running in the namespace.</p> <p>Here is an example of a team with a namespace configured for automatic sidecar injection:</p> <pre><code>export class TeamBurnham extends ApplicationTeam {\n    constructor(scope: Construct) {\n        super({\n            name: \"burnham\",\n            users: getUserArns(scope, \"team-burnham.users\"),\n            namespaceAnnotations: {\n                \"appmesh.k8s.aws/sidecarInjectorWebhook\": \"enabled\"\n            }\n        });\n    }\n}\n</code></pre>"},{"location":"addons/app-mesh/#tracing-integration","title":"Tracing Integration","text":"<p>App Mesh integrates with a number of tracing providers for distributed tracing support. At the moment it supports AWS X-Ray, Jaeger, and Datadog providers.  The X-Ray integration at present requires either a managed node group or a self-managed auto-scaling group backed by EC2. Fargate is not supported. </p> <p>Enabling integration:</p> <pre><code>const appMeshAddOn = new blueprints.AppMeshAddOn({\n  enableTracing: true, \n  tracingProvider: \"x-ray\"\n}),\n</code></pre> <p>When configured, App Mesh will automatically inject an XRay sidecar to handle tracing which enables troubleshooting latency issues.</p>"},{"location":"addons/app-mesh/#app-mesh-and-xray-integration-example","title":"App Mesh and XRay Integration Example","text":"<p><code>team-burnham</code> sample workload repository is configured with an example workload that demonstrates a \"meshified\" workload.</p> <p>After the workload is deployed with ArgoCD or applied directly to the cluster, a DJ application will be created in the <code>team-burnham</code> namespace, similar to the one used for the EKS Workshop. It was adapted for GitOps integration with Blueprints and relies on automatic sidecar injection as well as tracing integration with App Mesh.</p> <p>After the workload is deployed you can generate some traffic to populated traces:</p> <pre><code>$ export DJ_POD_NAME=$(kubectl get pods -n team-burnham -l app=dj -o jsonpath='{.items[].metadata.name}')\n$ kubectl -n team-burnham exec -it ${DJ_POD_NAME} -c dj bash\n$ while true; do\n  curl http://jazz.team-burnham.svc.cluster.local:9080/\n  echo\n  curl http://metal.team-burnham.svc.cluster.local:9080/\n  echo\ndone\n</code></pre> <p>The above script will start producing load which will generate traces with XRay. Once traces are produced (for a minute or more) you can navigate to the AWS XRay console and click on Service Map. </p> <p>You will see a screenshot similar to this:</p> <p></p>"},{"location":"addons/argo-cd/","title":"Argo CD Add-on","text":"<p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and can optionally bootstrap your workloads from public and private Git repositories. </p> <p>The Argo CD add-on allows platform administrators to combine cluster provisioning and workload bootstrapping in a single step and enables use cases such as replicating an existing running production cluster in a different region in a matter of minutes. This is important for business continuity and disaster recovery cases as well as for cross-regional availability and geographical expansion.</p> <p>Please see the documentation below for details on automatic boostrapping with ArgoCD add-on. If you prefer manual bootstrapping (once your cluster is deployed with this add-on included), you can find instructions on getting started with Argo CD in our Getting Started guide.</p> <p>Full Argo CD project documentation can be found here.</p>"},{"location":"addons/argo-cd/#usage","title":"Usage","text":"<p>To provision and maintain ArgoCD components without any bootstrapping, the add-on provides a no-argument constructor to get started. </p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.ArgoCDAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>The above will create an <code>argocd</code> namespace and install all Argo CD components. In order to bootstrap workloads you will need to change the default ArgoCD admin password and add repositories as specified in the Getting Started documentation.</p>"},{"location":"addons/argo-cd/#functionality","title":"Functionality","text":"<ol> <li>Creates the namespace specified in the construction parameter (<code>argocd</code> by default).</li> <li>Deploys the <code>argo-cd</code> Helm chart into the cluster.</li> <li>Allows to specify <code>ApplicationRepository</code> selecting the required authentication method as SSH Key, username/password or username/token. Credentials are expected to be set in AWS Secrets Manager and replicated to the desired region. If bootstrap repository is specified, creates the initial bootstrap application which may be leveraged to bootstrap workloads and/or other add-ons through GitOps.</li> <li>Allows setting the initial admin password through AWS Secrets Manager, replicating to the desired region. </li> <li>Supports standard helm configuration options.</li> </ol>"},{"location":"addons/argo-cd/#setting-an-admin-password","title":"Setting an Admin Password","text":"<p>By default, the Argo CD add-on will create a new admin password for you. To specify your own, you can leverage the AWS Secrets Manager.</p> <pre><code>const argoCDAddOn = new ArgoCDAddOn({\n    adminPasswordSecretName: `your-secret-name`\n});\nconst addOns: Array&lt;ClusterAddOn&gt; = [ argoCDAddOn ];\n</code></pre> <p>The attribute <code>adminPasswordSecretName</code> is the logical name of the secret in AWS Secret Manager. Note, that when deploying to multiple regions, the secret is expected to be replicated to each region. </p> <p>Inside ArgoCD, the admin password is stored as a <code>bcrypt</code> hash. This step will be performed by the framework and stored in the ArgoCD admin <code>secret</code>. </p> <p>You can change the admin password through the Secrets Manager, but it will require rerunning the provisioning pipeline to apply the change. </p>"},{"location":"addons/argo-cd/#bootstrapping","title":"Bootstrapping","text":"<p>The Blueprints framework provides an approach to bootstrap workloads and/or additional add-ons from a customer GitOps repository. In a general case, the bootstrap GitOps repository may contains an App of Apps that points to all workloads and add-ons.  </p> <p>In order to enable bootstrapping, the add-on allows passing an <code>ApplicationRepository</code> at construction time. The following repository types are supported at present:</p> <ol> <li>Public HTTP/HTTPS repositories (e.g., GitHub)</li> <li>Private HTTPS accessible git repositories requiring username/password authentication.</li> <li>Private git repositories with SSH access requiring an SSH key for authentication.</li> <li>Private HTTPS accessible GitHub repositories accessible with GitHub token. </li> </ol> <p>An example is provided below, along with an approach that could use a separate app of apps to bootstrap workloads in different stages, which is important for a software delivery platform as it allows segregating workloads specific to each stage of the SDLC and defines clear promotion processes through GitOps.</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport { ArgoCDAddOn, ClusterAddOn, EksBlueprint, ApplicationRepository }  from '@aws-quickstart/eks-blueprints';\n\nconst secretStoreAddOn = new SecretsStoreAddOn();\nconst repoUrl = 'git@github.com:aws-samples/eks-blueprints-workloads.git'\n\nconst bootstrapRepo: ApplicationRepository = {\n    repoUrl,\n    credentialsSecretName: 'github-ssh-test',\n    credentialsType: 'SSH'\n}\n\nconst devBootstrapArgo = new ArgoCDAddOn({\n    bootstrapRepo: {\n        ...bootstrapRepo,\n        path: 'envs/dev'\n    }\n});\nconst testBootstrapArgo = new ArgoCDAddOn({\n    bootstrapRepo: {\n        ...bootstrapRepo,\n        path: 'envs/test',\n    },\n\n});\nconst prodBootstrapArgo = new ArgoCDAddOn({\n    bootstrapRepo: {\n        ...bootstrapRepo,\n        path: 'envs/prod',\n    },\n    adminPasswordSecretName: 'ArgoCDAdmin',\n});\n\nconst blueprint = EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(secretStoreAddOn)\n    .account(account);\n\nblueprint.clone('us-east-1')\n    .addOns(devBootstrapArgo)\n    .build(app, 'argo-us-east-1');\n\nblueprint.clone('us-east-2')\n    .addOns(testBootstrapArgo)\n    .build(app, 'argo-us-east-2');\n\nblueprint.clone('us-west-2')\n    .addOns(prodBootstrapArgo)\n    .build(app, 'argo-us-west-1');\n</code></pre> <p>The application promotion process in the above example is handled entirely through GitOps. Each stage specific App of Apps contains references to respective application GitOps repository for each stage (e.g referencing the release vs work branches or path-based within individual app GitOps repository).</p>"},{"location":"addons/argo-cd/#gitops-for-eks-blueprints-addons","title":"GitOps for EKS Blueprints AddOns","text":"<p>By default all AddOns defined in a blueprint are deployed to the cluster via CDK. You can opt-in to deploy them following the GitOps model via ArgoCD. You will need a repository contains all the AddOns you would like to deploy via ArgoCD, such as, eks-blueprints-add-ons. You then configure ArgoCD bootstrapping with this repository as shown above.</p> <p>There are two types of GitOps deployments via ArgoCD depending on whether you would like to adopt the App of Apps strategy:</p> <ul> <li>CDK deploys the <code>Application</code> resource for each AddOn enabled, and ArgoCD deploys the actual AddOn via GitOps based on the <code>Application</code> resource. Example:</li> </ul> <pre><code>    import * as blueprints from '@aws-quickstart/eks-blueprints';\n\n    // enable gitops bootstrapping with argocd\n    const prodBootstrapArgo = new blueprints.addons.ArgoCDAddOn({\n        bootstrapRepo: {\n            repoUrl: 'https://github.com/aws-samples/eks-blueprints-add-ons',\n            path: 'add-ons',\n            targetRevision: \"main\",\n        },\n    });\n    // addons\n    const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n        prodBootstrapArgo,\n        new blueprints.addons.AppMeshAddOn(),\n        new blueprints.addons.MetricsServerAddOn(),\n        new blueprints.addons.AwsLoadBalancerControllerAddOn(),\n    ];\n\n    blueprints.EksBlueprint.builder()\n        .version(\"auto\")\n        .addOns(...addOns)\n        .enableGitOps(blueprints.GitOpsMode.APPLICATION)\n        .build(scope, stackID);\n</code></pre> <ul> <li>CDK deploys the only <code>Application</code> resource for the App of Apps, aka <code>bootstrap-apps</code>, and ArgoCD deploys all the AddOns based on the <code>bootstrap-apps</code>. This requires the naming pattern between the AddOns and App of Apps matches. Example:</li> </ul> <pre><code>    import * as blueprints from '@aws-quickstart/eks-blueprints';\n\n    // enable gitops bootstrapping with argocd app of apps\n    const prodBootstrapArgo = new blueprints.addons.ArgoCDAddOn({\n        bootstrapRepo: {\n            repoUrl: 'https://github.com/aws-samples/eks-blueprints-add-ons',\n            path: 'chart',\n            targetRevision: \"main\",\n        },\n    });\n    // addons\n    const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n        prodBootstrapArgo,\n        new blueprints.addons.AppMeshAddOn(),\n        new blueprints.addons.MetricsServerAddOn(),\n        new blueprints.addons.AwsLoadBalancerControllerAddOn(),\n    ];\n\n    blueprints.EksBlueprint.builder()\n        .version(\"auto\")\n        .addOns(...addOns)\n        .enableGitOps(blueprints.GitOpsMode.APP_OF_APPS)\n        .build(scope, stackID);\n</code></pre>"},{"location":"addons/argo-cd/#secrets-support","title":"Secrets Support","text":"<p>The framework provides support to supply repository and administrator secrets in AWS Secrets Manager. This support is evolving and will be improved over time as ArgoCD itself matures. </p>"},{"location":"addons/argo-cd/#private-repositories","title":"Private Repositories","text":"<p>SSH Key Authentication</p> <ol> <li>Set <code>credentialsType</code> to <code>SSH</code> when defining bootstrap repository in the ArgoCD add-on configuration.</li> </ol> <pre><code>.addOns(new blueprints.addons.ArgoCDAddOn({\n    bootstrapRepo: {\n        repoUrl: 'git@github.com:aws-samples/eks-blueprints-workloads.git',\n        path: 'envs/dev',\n        credentialsSecretName: 'github-ssh-json',\n        credentialsType: 'SSH'\n    }\n}))\n</code></pre> <p>Note: In this case the configuration assumes that there is a secret <code>github-ssh-json</code> define in the target account and all the regions where the blueprint will be deployed. </p> <ol> <li>Define the secret in AWS Secret Manager as \"Plain Text\" that contains a JSON structure (as of ArgoCD 2.x) with the fields <code>sshPrivateKey</code> and <code>url</code> defined. Note, that JSON does not allow line break characters, so all new line characters must be escaped with <code>\\n</code>.</li> </ol> <p>Example Structure: <pre><code>{\n    \"sshPrivateKey\": \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\",\n\n    \"url\": \"git@github\"\n}\n</code></pre> Note:  You can notice explicit <code>\\n</code> characters in the <code>sshPrivateKey</code>.</p> <p>url attribute is required and must specify full or partial URL for credentials template. For example <code>git@github</code> will set the credentials for all GitHub repositories when SSH authentication is used.  For more information see Repository Credentials and SSH Repositories from official ArgoCD documentation.</p> <p>To escape your SSH private key for storing it as a secret you can use the following command on Mac/Linux:</p> <pre><code>awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}'  &lt;path-to-your-cert&gt;\n</code></pre> <p>A convenience script to create the JSON structure for SSH private key can be found here. You will need to set the <code>PEM_FILE</code>(full path to the ssh private key file) and <code>URL_TEMPLATE</code> (part of the URL for credentials template) variables inside the script.</p> <ol> <li>(important) Replicate the secret to all the desired regions. </li> <li>Please see instructions for GitHub oou n details on setting up SSH access.</li> </ol> <p>Username Password and Token Authentication </p> <ol> <li>Set <code>credentialsType</code> to <code>USERNAME</code> or <code>TOKEN</code> when defining <code>ApplicationRepository</code> in the ArgoCD add-on configuration.</li> <li>Define the secret in the AWS Secret Manager as \"Key Value\" and set fields <code>url</code>, <code>username</code> and <code>password</code> to the desired values (clear text). For <code>TOKEN</code> username could be set to any username and password field set to the GitHub token. Replicate to the desired regions.</li> <li>Make sure that for this type of authentication your repository URL is set as <code>https</code>, e.g. https://github.com/aws-samples/eks-blueprints-workloads.git.</li> </ol> <p>Example Structure for <code>USERNAME</code> and <code>TOKEN</code> authentication type: <pre><code>{\n    \"username\": \"YOUR_GIT_USERNAME\", \n    \"password\": \"YOUR PASSWORD OR TOKEN\",\n    \"url\": \"https://github.com/aws-samples\"\n}\n</code></pre></p> <p>Note: <code>url</code> value can be a path to the org, rather than an actual repository.  </p> <p>Admin Secret</p> <ol> <li>Create a secret in the AWS Secrets Manager as \"Plain Text\" and set the value to the desired ArgoCD admin password. </li> <li>Replicate the secret to all the desired regions.</li> <li>Set the secret name in <code>adminPasswordSecretName</code> in ArgoCD add-on configuration.</li> <li>You can change the secret value through AWS Secrets Manager, however, it will require to rerun <code>cdk deploy</code> with the minimal changeset to apply the change. </li> </ol> <p>Alternatively to get started, the admin password hash can be set bypassing the AWS Secret by setting the following structure in the values properties of the add-on parameters:</p> <pre><code>import * as bcrypt from \"bcrypt\";\n\n.addOns(new blueprints.addons.ArgoCDAddOn({\n         ... // other settings\n         values: {\n           \"configs\": {\n                \"secret\": {\n                    \"argocdServerAdminPassword\": bcrypt.hash(&lt;your password plain text&gt;, 10) // or just supply &lt;your bcrypt hash&gt; directly\n                }\n            }\n        }\n     }))\n</code></pre> <p>For more information, please refer to the ArgoCD official documentation.</p>"},{"location":"addons/argo-cd/#known-issues","title":"Known Issues","text":"<ol> <li> <p>Destruction of the cluster with provisioned applications may cause cloud formation to get stuck on deleting ArgoCD namespace. This happens because the server component that handles Application CRD resource is destroyed before it has a chance to clean up applications that were provisioned through GitOps (of which CFN is unaware). To address this issue at the moment, App of Apps application should be destroyed manually before destroying the stack. </p> </li> <li> <p>Changing the administrator password in the AWS Secrets Manager and rerunning the stack causes login error on ArgoCD UI. This happens due to the fact that Argo Helm rewrites the secret containing the Dex server API Key (OIDC component of ArgoCD). The workaround at present is to restart the <code>argocd-server</code> pod, which repopulates the token. Secret management aspect of ArgoCD will be improved in the future to not require this step after password change. </p> </li> </ol>"},{"location":"addons/argo-cd/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Dex Server crashing on startup with <code>server.secretkey is missing</code>. </li> </ol> <p>It may be a byproduct of another failure. As a rule (unless ArgoCD secret is configured separately) the initial start of the ArgoCD server should populate a few fields in the in ArgoCD secret. If ArgoCD server fails to start or is waiting on some condition to become ready, these fields are not populated, causing cascading failures. </p> <p>Make sure that all the secrets are mounted properly onto the ArgoCD server pod. It can be caused by an incorrect shape of the secret for private repositories (see \"Private Repositories\" section above). SSH secret is expected to have two fields (<code>url</code> and <code>sshPrivateKey</code>) and USERNAME/TOKEN is expected to have three fields (<code>username</code>, <code>password</code>, <code>url</code>).</p> <p>Make sure your secret name (as defined in AWS Secrets Manager) does not conflict with ArgoCD reserved secret names, such as <code>argocd-secret</code>.</p>"},{"location":"addons/aws-batch-on-eks/","title":"AWS Batch on EKS","text":"<p>AWS Batch is a managed service that orchestrates batch workloads in your Kubernetes clusters that are managed by Amazon Elastic Kubernetes Service (Amazon EKS). Since AWS Batch is a managed service, there are no Kubernetes components (for example, Operators or Custom Resources) to install or manage in your cluster. AWS Batch only needs your cluster to be configured with Role-Based Access Controls (RBAC) that allow AWS Batch to communicate with the Kubernetes API server. AWS Batch calls Kubernetes APIs to create, monitor, and delete Kubernetes pods and nodes.</p> <p>For more information, consult our official documentations.</p> <p>This Add-on MUST be used with AWS Batch on EKS Team.</p>"},{"location":"addons/aws-batch-on-eks/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AwsBatchAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(addOn)\n    .build(app, 'my-stack-name');\n</code></pre> <p>Once the AddOn is deployed you can execute the following command:</p> <p><pre><code>kubectl describe -n kube-system configmap/aws-auth\n</code></pre> The output of the command would show a list of IAM role and mapping to Kubernetes users, one fo the mapping would be for AWS Batch on EKS role and would be similar to the following:</p> <pre><code>  mapRoles: |\n    - rolearn: arn:aws:iam::&lt;your-account-id&gt;:role/AWSServiceRoleForBatch\n      username: aws-batch\n</code></pre>"},{"location":"addons/aws-cloudwatch-insights/","title":"CloudWatch Insights Add-on","text":"<p>The Cloudwatch Insights add-on adds support for CloudWatch Insights to an EKS cluster. This replaces the current Container Insights add-on and all customers on it, should migrate to CloudWatch Insights.</p> <p>Customers can use CloudWatch Insights to collect, aggregate, and summarize metrics and logs from your containerized  applications and microservices. Container Insights collects data as performance log events using an embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console.</p> <p>IMPORTANT</p> <p>CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events.</p> <p>Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing, see Amazon CloudWatch Pricing.</p> <p>Note: that this add-on cannot co-exist with <code>adot-addon</code>, <code>cloudwatch-adot-addons</code> or <code>cloudwatch-logs</code> on same EKS cluster as they have conflicting and redundant interactions.</p>"},{"location":"addons/aws-cloudwatch-insights/#usage","title":"Usage","text":"<p>Add the following as an add-on to your main.ts file to add CloudWatch Insights to your cluster</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.CloudWatchInsights();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/aws-cloudwatch-insights/#prerequisites","title":"Prerequisites","text":"<p>Once the CloudWatch Insights add-on has been installed in your cluster, validate that the <code>AWS Cloudwatch Observability Controller</code> is installed and FluentBit is installed: </p> <pre><code>kubectl get pods -n amazon-cloudwatch\n</code></pre> <p>You should see output similar to the following respectively:</p> <pre><code>NAMESPACE           NAME                                                 READY   STATUS    RESTARTS        AGE\namazon-cloudwatch   amazon-cloudwatch-observability-controller-manager   1/1     Running   1 (4d22h ago)   5d\namazon-cloudwatch   cloudwatch-agent                                     1/1     Running   1 (4d22h ago)   5d\namazon-cloudwatch   fluent-bit                                           1/1     Running   1 (4d22h ago)   5d\n</code></pre>"},{"location":"addons/aws-cloudwatch-insights/#cloudwatch-dashboard","title":"CloudWatch Dashboard","text":"<p>Once enabled, enhanced container insights page looks like below from AWS console, with the high level summary of your clusters, kube-state and control-plane metrics. The Container Insights dashboard shows cluster status and alarms. It uses predefined thresholds for CPU and memory to quickly identify which resources are having higher consumption, and enabling proactive actions to avoid performance impact.</p> <p></p> <p>Additional functionality of the addon is listed in this blog Introducing CloudWatch Insights with Enhanced Monitoring.</p>"},{"location":"addons/aws-cloudwatch-insights/#view-top-10-lists","title":"View Top 10 Lists","text":"<p>The CloudWatch Insights Dashboard can also let you quickly view the Top 10 lists of Cluster, Nodes, Pods, Workloads, and Containers as shown below. Based on their consumption, you can set up critical charts that let you identify risky components without using alarms and before you are resource constrained.</p> <p></p>"},{"location":"addons/aws-cloudwatch-insights/#cluster-overview","title":"Cluster Overview","text":"<p>The dashboard also lets you quickly view the consumption of your cluster, with the clusters ranked on their \"criticality\" where they're top ranking if they're in alarm, and then all the others are ranked based on resource consumption. They're visible in a list view as shown below.</p> <p></p>"},{"location":"addons/aws-cloudwatch-insights/#popular-workloads","title":"Popular Workloads","text":"<p>CloudWatch insights has pre-built automatic dashboards and alarms for popular workloads, and it allows you to drill into logs generated by those workloads as well.</p> <p></p>"},{"location":"addons/aws-cloudwatch-insights/#metrics-namespaces","title":"Metrics namespaces","text":"<p>CloudWatch insights also exposes the new \"ContainersInsights\" metrics namespace, it contains all the various dimensions  of EKS clusters and the data that's exported from the CloudWatch agents.</p> <p></p>"},{"location":"addons/aws-for-fluent-bit/","title":"AWS for Fluent Bit","text":"<p>Fluent Bit is an open source log processor and forwarder which allows you to collect data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations.</p> <p>AWS provides a Fluent Bit image with plugins for both CloudWatch Logs and Kinesis Data Firehose. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see AWS for Fluent Bit GitHub repository.</p>"},{"location":"addons/aws-for-fluent-bit/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AwsForFluentBitAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/aws-for-fluent-bit/#configuration","title":"Configuration","text":"<p>AWS for FluentBit can be configured to forward logs to multiple AWS destinations including CloudWatch, Kinesis, and Elasticsearch (now AWS OpenSearch Search). </p> <p>Sample configuration can be found below:</p> <pre><code>import * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst awsForFluentBit = new blueprints.addons.AwsForFluentBitAddOn({ \n    values: {\n        cloudWatch: {\n            enabled: true,\n            region: \"&lt;aws_region\",\n            logGroupName: \"&lt;log_groups_name&gt;\"\n        },\n        kinesis: {\n            enabled: true,\n            region: \"&lt;aws_region\",\n            deliveryStream: \"&lt;delivery_stream&gt;\"\n        },\n        elasticSearch: {\n            enabled: true,\n            region: \"&lt;aws_region\",\n            host: \"&lt;elastic_search_host&gt;\"\n        }\n    }\n});\n</code></pre>"},{"location":"addons/aws-for-fluent-bit/#iam-policies","title":"IAM Policies","text":"<p>When leveraging AWS for FluentBit to forward logs to various AWS destinations, you will need to supply an IAM role that grants privileges to the namespace in which FluentBit runs. For example, in order to forward logs to Amazon Elasticsearch Service, you would supply the following IAMPolicyStatement. </p> <pre><code>import * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst domain = es.Domain()\nconst domainWritePolicy = new iam.PolicyStatement({\n    actions: [\n        'es:ESHttpDelete',\n        'es:ESHttpPost',\n        'es:ESHttpPut',\n        'es:ESHttpPatch'\n    ],\n    resources: [domain.arn],\n})\n\nconst awsForFluentBit = new blueprints.addons.AwsForFluentBitAddOn({ \n    iamPolicies: [domainWritePolicy]\n    values: {\n        elasticSearch: {\n            enabled: true,\n            awsRegion: \"&lt;aws_region\",\n            host: \"&lt;elastic_search_host&gt;\"\n        }\n    }\n});\n</code></pre>"},{"location":"addons/aws-load-balancer-controller/","title":"AWS Load Balancer Controller Add-on","text":"<p>The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources:</p> <ul> <li>An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress.</li> <li>An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. In the past, you used the Kubernetes in-tree load balancer for instance targets, but used the AWS Load balancer Controller for IP targets. With the AWS Load Balancer Controller version 2.2.0 or later, you can create Network Load Balancers using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers.</li> </ul> <p>For more information about AWS Load Balancer Controller please see the official documentation. This controller is a required for proper configuration of other ingress controllers such as NGINX.</p>"},{"location":"addons/aws-load-balancer-controller/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AwsLoadBalancerControllerAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .version(\"auto\")\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that controller is running, ensure that controller deployment is in <code>RUNNING</code> state:</p> <pre><code># Assuming controller is installed in kube-system namespace\n$ kubectl get deployments -n kube-system\nNAME                                                       READY   UP-TO-DATE   AVAILABLE   AGE\naws-load-balancer-controller                               2/2     2            2           3m58s\n</code></pre>"},{"location":"addons/aws-load-balancer-controller/#functionality","title":"Functionality","text":"<ol> <li>Adds proper IAM permissions and creates a Kubernetes service account with IRSA integration. </li> <li>Allows configuration options such as enabling WAF and Shield. </li> <li>Allows to replace the helm chart version if a specific version of the controller is needed.</li> <li>Creates an IngressClass associated with the AWS Load Balance Controller when the createIngressClassResource prop is set to true</li> <li>Supports standard helm configuration options.</li> </ol> <p>Note: An ingressClass must be created in the cluster, either using the createIngressClassResource prop or externally, to be able to create Ingresses associated with the AWS ALB.</p>"},{"location":"addons/aws-load-balancer-controller/#creating-a-load-balanced-service","title":"Creating a Load Balanced Service","text":"<p>Once the AWS Load Balancer Controller add-on is installed in your cluster, it is able to provision both Network Load Balancers and Application Load Balancers on your behalf. For example, when the following manifest is applied to your cluster, it will create an NLB.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '60'\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n  name: udp-test1\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 5005\n    protocol: UDP\n    targetPort: 5005\n  selector:\n    name: your-app\n</code></pre>"},{"location":"addons/aws-node-termination-handler/","title":"AWS Node Termination Handler","text":"<p>The AWS Node Termination Handler (NTH) project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as EC2 maintenance events, EC2 Spot interruptions, ASG Scale-In, ASG AZ Rebalance, and EC2 Instance Termination via the API or Console. If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down. For more information see [README.md][https://github.com/aws/aws-node-termination-handler#readme].</p> <p>NTH can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor. To choose the operating mode refer to this table.</p> <p>Best Practice NTH should only be used when you are using self-managed node groups and self-managed node groups with Spot instances. For more information on why you do not need NTH on managed node groups see this issue and EKS Workshop for detailed explanation.</p> <p>Best Practice Use NTH in Queue Processor option to add every AWS Node Termination Handler feature to the self-managed node group.</p> <p>Note With AWS Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to use AWS Node Termination Handler.</p>"},{"location":"addons/aws-node-termination-handler/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AwsNodeTerminationHandlerAddOn();\n\nconst clusterProvider = new blueprints.AsgClusterProvider({\n  version: eks.KubernetesVersion.V1_29,\n  machineImageType:  eks.MachineImageType.BOTTLEROCKET\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .clusterProvider(clusterProvider)\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that controller is running, ensure that controller deployment is in <code>RUNNING</code> state:</p> <pre><code># Assuming handler is installed in kube-system namespace\n$ kubectl get deployments -n kube-system\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\naws-node-termination-handler         1/1     1            1           23m\n</code></pre>"},{"location":"addons/aws-node-termination-handler/#functionality","title":"Functionality","text":""},{"location":"addons/aws-node-termination-handler/#imds-mode-default","title":"IMDS Mode (default)","text":"<ol> <li>Node group ASG tagged with <code>key=aws-node-termination-handler/managed</code></li> <li>Deploy the AWS Node Termination Handler helm chart</li> </ol>"},{"location":"addons/aws-node-termination-handler/#queue-mode","title":"Queue Mode","text":"<ol> <li>Node group ASG tagged with <code>key=aws-node-termination-handler/managed</code></li> <li>AutoScaling Group Termination Lifecycle Hook</li> <li>Amazon Simple Queue Service (SQS) Queue</li> <li>Amazon EventBridge Rule</li> <li>IAM Role for the aws-node-termination-handler Queue Processing Pods</li> <li>Deploy the AWS Node Termination Handler helm chart</li> </ol>"},{"location":"addons/aws-privateca-issuer/","title":"AWS Private CA Issuer Add-on","text":"<p>This addon will install aws-privateca-issuer</p> <p>AWS ACM Private CA is a module of the AWS Certificate Manager that can setup and manage private CAs. The AWS PrivateCA Issuer plugin acts as an addon to cert-manager that signs certificate requests using ACM Private CA.</p> <p>Since its an addon to cert-manager, for Installing AWS ACM Private CA Addon, You must install cert-manager Addon first</p> <p>cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry.</p>"},{"location":"addons/aws-privateca-issuer/#usage","title":"Usage","text":"<p>Please ensure that cert-manager addon is already installed</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\nconst awsPcaParams = {\n  iamPolicies: [\"AWSCertificateManagerPrivateCAFullAccess\"]\n}\nconst addOn = new blueprints.addons.AWSPrivateCAIssuerAddon(awsPcaParams)\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/aws-privateca-issuer/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>serviceAccountName</code>: (string) User provided name for service account. The default value is aws-pca-issuer</li> <li><code>iamPolicies</code> - An array of Managed IAM Policies which Service Account needs for IRSA Eg: irsaRoles:[\"AWSCertificateManagerPrivateCAFullAccess\"]. If not empty the Service Account will be created by the CDK with IAM Roles Mapped (IRSA). In case if its empty,  Service Account will be created with out default IAM Policy - \"AWSCertificateManagerPrivateCAFullAccess\"</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to the aws-pca-issuer Helm Chart Values for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options)</li> </ul>"},{"location":"addons/aws-privateca-issuer/#cert-manager-compatibility-with-eks-and-fargate","title":"cert-manager compatibility with EKS and Fargate","text":"<p>Please refer to the cert-manager compatibility and open issues with EKS and Fargate [cert-manager compatibility with EKS](https://cert-manager.io/docs/installation/compatibility/#aws-eks_</p>"},{"location":"addons/aws-privateca-issuer/#validation","title":"Validation","text":"<p>To validate that aws-pca-issuer is installed properly in the cluster, check if the namespace aws-pca-issuer is created </p> <p>Verify if the namespace is created correctly <pre><code>  kubectl get ns | grep \"aws-pca-issuer\"\n</code></pre> There should be list the pca namespace <pre><code>aws-pca-issuer      Active   31m\n</code></pre> Verify the objects under namespace aws-pca-issuer  <pre><code>  kubectl get all -n aws-pca-issuer \n</code></pre> It should give results as below For Eg: <pre><code>NAME                                                       READY   STATUS    RESTARTS   AGE\npod/aws-pca-issuer-aws-privateca-issuer-7b9df7c7cc-vz8hw   1/1     Running   0          3m2s\n\nNAME                                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/aws-pca-issuer-aws-privateca-issuer   ClusterIP   172.20.17.134   &lt;none&gt;        8080/TCP   3m3s\n\nNAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/aws-pca-issuer-aws-privateca-issuer   1/1     1            1           3m2s\n</code></pre></p>"},{"location":"addons/aws-privateca-issuer/#testing","title":"Testing","text":"<p>1) Create an ACM Private CA For this testing create a private certificate authority in ACM Private CA with RSA 2048 selected as the key algorithm. You can create a CA using the AWS console Once your private CA is active note down the ARN</p> <p>2) Create a K8s namespace for testing purpose <pre><code>  kubectl create ns acm-pca-demo\n</code></pre> 3) Change the current context to namespace acm-pca-demo <pre><code>  kubectl config set-context --current --namespace=acm-pca-demo\n</code></pre> 4) Create CRD AWSPCAIssuer with name demo-awspcs-issuer  &gt;&gt; AWSPCAIssuer.yaml</p> <p>AWSPCAIssuer This is a regular namespaced issuer that can be used as a reference in your Certificate CRs. AWSPCAClusterIssuer This CR is identical to the AWSPCAIssuer. The only difference being that it\u2019s not namespaced and can be referenced from anywhere.</p> <p>In thi example we will use  AWSPCAIssuer Replace the arn Replace ${AWS_REGION} with your target region and ${ARN} with the ARN of CM Private CA recieved from step 1 <pre><code>---\napiVersion: awspca.cert-manager.io/v1beta1\nkind: AWSPCAIssuer\nmetadata:\n  name: demo-awspcs-issuer\n  namespace: acm-pca-demo\nspec:\n  arn: ${ARN}\n  region: ${AWS_REGION}\n---\n</code></pre> Apply the yaml file <pre><code>  kubectl apply -f AWSPCAIssuer.yaml\n</code></pre> Verify AWSPCAIssuer installed correctly <pre><code>  kubectl describe AWSPCAIssuer \n</code></pre> Check the Events section and you must see the message Issuer verified if everything goes correct <pre><code> Normal  Verified  46s (x2 over 46s)  awspcaissuer-controller  Issuer verified\n</code></pre> 4) Create CRD Certificate with name rsa-cert-2048 for dns name rsa-2048.example.com  &gt;&gt; Certificate.yaml For th formats other than 2048 check the examples <pre><code>---\nkind: Certificate\napiVersion: cert-manager.io/v1\nmetadata:\n  name: rsa-cert-2048\nspec:\n  commonName: www.rsa-2048.example.com\n  dnsNames:\n    - www.rsa-2048.example.com\n    - rsa-2048.example.com\n  duration: 2160h0m0s\n  issuerRef:\n    group: awspca.cert-manager.io\n    kind: AWSPCAIssuer\n    name: demo-awspcs-issuer\n  renewBefore: 360h0m0s\n  secretName: rsa-example-cert-2048\n  usages:\n    - server auth\n    - client auth\n  privateKey:\n    algorithm: \"RSA\"\n    size: 2048\n---\n</code></pre> Apply the yaml file <pre><code>  kubectl apply -f Certificate.yaml\n</code></pre> Verify Certificate is installed correctly <pre><code>  kubectl  get Certificates\n</code></pre> It should output Ready as True as shown below <pre><code>  NAME            READY   SECRET                  AGE\n  rsa-cert-2048   True    rsa-example-cert-2048   31s\n</code></pre> The actual certificate file is stored as a secret. To see the details of secret get the secret name <pre><code>  k describe certificate | grep Secret\n</code></pre> Output <pre><code>  Secret Name:   rsa-example-cert-2048\n</code></pre> Describe the secret to get the value <pre><code>  kubectl describe secret rsa-example-cert-2048\n</code></pre></p>"},{"location":"addons/aws-privateca-issuer/#troubleshooting","title":"Troubleshooting","text":"<p>Please use kubectl get events for debugging.  <pre><code>  kubectl get events  \n</code></pre> Sample Output for Successfull Certificate Request <pre><code>  5s          Normal   cert-manager.io   certificaterequest/rsa-cert-2048.io-zqftp   Certificate request has been approved by cert-manager.io\n  2s          Normal   Issued            certificaterequest/rsa-cert-2048.io-zqftp   certificate issued\n  5s          Normal   Issuing           certificate/rsa-cert-2048                Issuing certificate as Secret does not exist\n  5s          Normal   Generated         certificate/rsa-cert-2048               Stored new private key in temporary Secret resource \"rsa-cert-2048-k7zxv\"\n  5s          Normal   Requested         certificate/rsa-cert-2048                Created new CertificateRequest resource \"rsa-cert-2048-zqftp\"\n  2s          Normal   Issuing           certificate/rsa-cert-2048                The certificate has been successfully issued\n  8m22s       Normal   Verified          awspcaissuer/rsa-cert-2048               Issuer verified\n  85s         Normal   Verified          awspcaissuer/rsa-cert-2048               Issuer verified\n</code></pre></p>"},{"location":"addons/backstage/","title":"Backstage Amazon EKS Blueprints AddOn","text":"<p>This add-on installs Backstage.</p>"},{"location":"addons/backstage/#dependencies","title":"Dependencies","text":"<p>The add-on depends on the following components: - a Backstage Docker image stored in a Container Registry - a subdomain for the Backstage application  - AWS Load Balancer Controller, which will instantiate an ALB to fulfill the Ingress installed by the Backstage's Helm chart; the dependency is satisfied by AwsLoadBalancerControllerAddOn, which needs to be included in the list of add-ons - a certificate for the subdomain, made available by the either CreateCertificateProvider or ImportCertificateProvider, to be assigned to the load balancer - a database instance, implementing the IDatabaseInstance interface and registered by a resource provider - a Secret deployed in the cluster, which will be used to expose the database credentials as environement variables: ${POSTGRES_USER} and ${POSTGRES_PASSWORD}</p>"},{"location":"addons/backstage/#setup","title":"Setup","text":"<p>For a fully working setup, please see the Backstage pattern in the EKS Blueprints Pattern repository.</p>"},{"location":"addons/calico-operator/","title":"Calico Operator Add-on","text":"<p>Project Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. To secure workloads in Kubernetes, Calico utilizes Network Policies. The <code>Calico Operator</code> add-on adds support for Calico to an EKS cluster by deploying Tigera Operator.</p> <p>By default, the native VPC-CNI plugin for Kubernetes on EKS does not support Kubernetes Network Policies. Installing Calico (or alternate CNI provider) will enable customers to define and apply standard Kubernetes Network Policies to their EKS cluster. </p> <p>Calico add-on supports standard helm configuration options.</p>"},{"location":"addons/calico-operator/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.CalicoOperatorAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/calico-operator/#upgrading-from-calico-add-on","title":"Upgrading from Calico Add-on","text":"<p>To upgrade from Calico add-on the following two step process has been validated to work:</p> <ol> <li>Remove <code>CalicoAddOn</code> from the blueprint and deploy (via <code>cdk deploy</code> or via pipeline support).</li> <li>Add <code>CalicoOperatorAddOn</code> to the blueprint and deploy again.</li> </ol>"},{"location":"addons/calico-operator/#applying-network-policies","title":"Applying Network Policies","text":"<p>In the Getting Started guide, we bootstrapped an EKS cluster with the workloads contained in the <code>eks-blueprints-workloads</code> repository. Below, we will demonstrate how we can apply network policies to govern traffic between the workloads once Calico is installed.</p> <p>To start, we can verify that there are no network policies in place in your EKS cluster.</p> <pre><code>kubectl get networkpolicy -A\n</code></pre> <p>This means that all resources within the cluster should be able to make ingress and egress connections with other resources within and outside the cluster. You can verify, for example, that you are able to ping a<code>team-burnham</code> pod from a <code>team-riker</code> pod. To do so, first retrieve the podIP from the <code>team-burnham</code> namespace.</p> <pre><code>BURNHAM_POD=$(kubectl get pod -n team-burnham -o jsonpath='{.items[0].metadata.name}') \nBURNHAM_POD_IP=$(kubectl get pod -n team-burnham $BURNHAM_POD -o jsonpath='{.status.podIP}')\n</code></pre> <p>Now you can start a shell from the pod in the <code>team-riker</code> namespace and ping the pod from <code>team-burnham</code> namespace:</p> <pre><code>RIKER_POD=$(kubectl -n team-riker get pod -o jsonpath='{.items[0].metadata.name}')\nkubectl exec -ti -n team-riker $RIKER_POD -- sh\n</code></pre> <p>Note: since this opens a shell inside the pod, it will not have the environment variables saved above. You should retrieve the actual podIP from the environment variable <code>BURNHAM_POD_IP</code>.</p> <p>With those actual values, curl the IP and port 80 of the pod from <code>team-burnham</code>:</p> <pre><code># curl -s &lt;Team Burnham Pod IP&gt;:80&gt;/dev/null &amp;&amp; echo Success. || echo Fail. \n</code></pre> <p>You should see <code>Success.</code></p>"},{"location":"addons/calico-operator/#applying-kubernetes-network-policy-to-block-traffic","title":"Applying Kubernetes Network Policy to block traffic","text":"<p>Let's apply the following Network Policy:</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: default-deny\nspec:\n  podSelector:\n    matchLabels: {}\n</code></pre> <p>Save it as <code>deny-all.yaml</code>. Run the following commands to apply the policy to both <code>team-riker</code> and <code>team-burnham</code> namespaces:</p> <pre><code>kubectl -n team-riker apply -f deny-all.yaml \nkubectl -n team-burnham apply -f deny-all.yaml\n</code></pre> <p>This will prevent access to all resources within both namespaces. Try curl commands from above to verify that it fails.</p>"},{"location":"addons/calico-operator/#applying-additional-policy-to-re-open-pod-to-pod-communications","title":"Applying additional policy to re-open pod to pod communications","text":"<p>You can apply a new Kubernetes Network Policy on top of the previous to \u201cpoke holes\u201d for egress and ingress needs. </p> <p>For example, if you want to be able to curl from the <code>team-riker</code> pod to the <code>team-burnham</code> pod, the following Kubernetes NetworkPolicy should be applied. </p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  namespace: team-burnham\n  name: allow-riker-to-burnham\nspec:\n  podSelector:\n    matchLabels:\n      app: guestbook-ui\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              app: guestbook-ui\n          namespaceSelector:\n            matchLabels:\n              name: team-riker\n      ports:\n        - protocol: TCP\n          port: 80\n  egress:\n    - to:\n        - podSelector:\n            matchLabels:\n              app: guestbook-ui\n          namespaceSelector:\n            matchLabels:\n              name: team-riker\n      ports:\n        - protocol: TCP\n          port: 80\n</code></pre> <p>Save as <code>allow-burnham-riker.yaml</code> and apply the new NetworkPolicy:</p> <pre><code>kubectl apply -f allow-burnham-riker.yaml     \n</code></pre> <p>Once the policy is applied, once again try the curl command from above. You should now see <code>Success.</code> once again.</p>"},{"location":"addons/calico-operator/#securing-your-environment-with-kubernetes-network-policies","title":"Securing your environment with Kubernetes Network Policies","text":"<p>Calico also allows Custom Resource Definitions (CRD) which provides the ability to add features not in the standard Kubernetes  Network Policies, such as:</p> <ul> <li>Explicit Deny rules</li> <li>Layer 7 rule support (i.e. Http Request types)</li> <li>Endpoint support other than standard pods: OpenShift, VMs, interfaces, etc. </li> </ul> <p>In order to use CRDs (in particular defined within the projectcalico.org/v3 Calico API), you must install the Calico CLI (calicoctl). You can find more information about Calico Network Policy and using <code>calicoctl</code> here. </p>"},{"location":"addons/calico/","title":"Calico Add-on","text":"<p>This add-on is deprecated as AWS/Tigera stopped support for the Calico helm chart published on EKS charts. Please use Calico Operator Add-on instead, including instructions for migration from Calico add-on. </p>"},{"location":"addons/cert-manager/","title":"Certificate Manager Add-on","text":"<p>This add-on installs cert-manager.</p> <p>cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates. Add-on can issue certificates from a variety of supported sources, including Let's Encrypt, HashiCorp Vault, and Venafi as well as private PKI. It will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry.</p>"},{"location":"addons/cert-manager/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.CertManagerAddOn()\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/cert-manager/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>installCRDs</code>: (boolean) To automatically install and manage the CRDs as part of your Helm release,</li> <li><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to the cert-manager Helm Chart documentation for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options)</li> </ul>"},{"location":"addons/cert-manager/#cert-manager-compatibility-with-eks-and-fargate","title":"cert-manager compatibility with EKS and Fargate","text":"<p>Please refer to the cert-manager compatibility and open issues with EKS and Fargate [cert-manager compatibility with EKS](https://cert-manager.io/docs/installation/compatibility/#aws-eks_</p>"},{"location":"addons/cert-manager/#validation","title":"Validation","text":"<p>To validate that cert-manager is installed properly in the cluster, check if the namespace is created and cert-manger pods are running.</p> <p>Verify if the namespace is created correctly <pre><code>  kubectl get ns | grep \"cert-manager\"\n</code></pre> There should be list the cert-manager namespace <pre><code>cert-manager      Active   31m\n</code></pre> Verify if the pods are running correctly in cert-manager namespace <pre><code>  kubectl get pods -n cert-manager  \n</code></pre> There should list 3 pods starting with name cert-manager- For Eg: <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\ncert-manager-5bb7949947-vxf76             1/1     Running   0          2m56s\ncert-manager-cainjector-5ff98c66d-g4kpv   1/1     Running   0          2m56s\ncert-manager-webhook-fb48856b5-bpsbl      1/1     Running   0          2m56s\n</code></pre></p>"},{"location":"addons/cert-manager/#testing","title":"Testing","text":"<p>Cert-Manager Kubectl Plugin Cert-Manager has a Kubectl plugin that simplifies some common management tasks. It also lets you check whether Cert-Manager is up and ready to serve requests.</p> <p>Install Cert-Manager Kubectl Plugin <pre><code>  curl -L -o kubectl-cert-manager.tar.gz https://github.com/jetstack/cert-manager/releases/latest/download/kubectl-cert_manager-linux-amd64.tar.gz\n  tar xzf kubectl-cert-manager.tar.gz\n  sudo mv kubectl-cert_manager /usr/local/bin\n</code></pre></p> <p>Run the follwing command to check if the plugin was correctly installed: <pre><code>  kubectl cert-manager check api\n</code></pre> This should print following message \"The cert-manager API is ready\"</p>"},{"location":"addons/cloudwatch-adot-addon/","title":"Amazon CloudWatch ADOT Add-on","text":"<p>Amazon CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. You get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on AWS and on-premises.  This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for CloudWatch which receives metrics and logs from the application and sends the same to CloudWatch console. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy.</p> <p>This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup CloudWatch for remote write metrics.</p> <p>For more information on the add-on, please review the user guide.</p> <p>Note: Due to lack of helm chart support and lack of \u201cserverside apply\u201d in the current version of EKS CW add-on cannot be used together with AMP add-on. Check this Github Issue for more information.</p>"},{"location":"addons/cloudwatch-adot-addon/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>adot</code> EKS Blueprints add-on.</li> </ul>"},{"location":"addons/cloudwatch-adot-addon/#usage","title":"Usage","text":"<p>This add-on can used with two different patterns :</p> <p>Pattern # 1 : Simple and Easy - Using all default property values. This pattern deploys an ADOT collector with <code>deployment</code> as the mode to write traces to CloudWatch console.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.CloudWatchAdotAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern # 2 : Overriding property value for different deployment Modes. This pattern deploys an ADOT collector on the namespace specified in <code>namespace</code>, name specified in <code>name</code> with <code>daemonset</code> as the mode to visualize metrics in CloudWatch console. Deployment mode can be overridden to any of these values - <code>deployment</code>, <code>daemonset</code>, <code>statefulset</code>, <code>sidecar</code>. Mode <code>sidecar</code> is to support Fargate profile. You can pass required metrics including custom metrics and required pod labels of application pods emitting custom metrics to visualize using <code>metricsNameSelectors</code>, <code>podLabelRegex</code> as parameters as shown below.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.CloudWatchAdotAddOn({\n    deploymentMode: cloudWatchDeploymentMode.DEPLOYMENT,\n    namespace: 'default',\n    name: 'adot-collector-cloudwatch',\n    metricsNameSelectors: ['apiserver_request_.*', 'container_memory_.*', 'container_threads', 'otelcol_process_.*'],\n    podLabelRegex: 'frontend|downstream(.*)' \n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/cloudwatch-adot-addon/#validation","title":"Validation","text":"<p>To validate whether CloudWatch add-on is installed properly, ensure that the required kubernetes resources are running in the cluster.</p> <pre><code>kubectl get all -n default\n</code></pre>"},{"location":"addons/cloudwatch-adot-addon/#output","title":"Output","text":"<pre><code>NAME                                                       READY   STATUS    RESTARTS   AGE\npod/otel-collector-cloudwatch-collector-7565f958c6-r485f   1/1     Running   0          2m41s\n\nNAME                                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/kubernetes                                       ClusterIP   172.20.0.1       &lt;none&gt;        443/TCP    18h\nservice/otel-collector-cloudwatch-collector-monitoring   ClusterIP   172.20.254.103   &lt;none&gt;        8888/TCP   2m43s\n\nNAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/otel-collector-cloudwatch-collector   1/1     1            1           2m42s\n\nNAME                                                             DESIRED   CURRENT   READY   AGE\nreplicaset.apps/otel-collector-cloudwatch-collector-7565f958c6   1         1         1       2m42s\n</code></pre>"},{"location":"addons/cloudwatch-adot-addon/#functionality","title":"Functionality","text":"<p>Applies the CloudWatch ADOT add-on to an Amazon EKS cluster. </p>"},{"location":"addons/cloudwatch-logs/","title":"Amazon CloudWatch Logs Addon using Fluent Bit.","text":"<p>Fluent Bit is an open source Log Processor and Forwarder which allows you to collect any data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations.</p> <p>AWS provides a Fluent Bit image with plugins for CloudWatch Logs, Kinesis Data Firehose, Kinesis Data Stream and Amazon OpenSearch Service.</p> <p>This add-on will take care of handling both node and application level logging. It is configured to stream the worker node logs to CloudWatch Logs. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see AWS for Fluent Bit GitHub repository.</p>"},{"location":"addons/cloudwatch-logs/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.CloudWatchLogsAddon({\n  namespace: 'aws-for-fluent-bit',\n  createNamespace: true,\n  serviceAccountName: 'aws-fluent-bit-for-cw-sa',\n  logGroupPrefix: '/aws/eks/&lt;your-cluster-name&gt;',\n  logRetentionDays: 90 \n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/cloudwatch-logs/#validation","title":"Validation","text":"<p>To validate whether CloudWatch Logs add-on is installed properly, ensure that the required kubernetes resources are running in the cluster.</p> <pre><code>kubectl get all -n aws-for-fluent-bit\n</code></pre>"},{"location":"addons/cloudwatch-logs/#output","title":"Output","text":"<pre><code>NAME                                                                  READY   STATUS    RESTARTS   AGE\npod/blueprints-addon-aws-fluent-bit-for-cw-aws-for-fluent-bit-46kkk   1/1     Running   0          3m15s\npod/blueprints-addon-aws-fluent-bit-for-cw-aws-for-fluent-bit-6x4xq   1/1     Running   0          3m18s\n\nNAME                                                                       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/blueprints-addon-aws-fluent-bit-for-cw-aws-for-fluent-bit   2         2         2       2            2           &lt;none&gt;          3h53m\n</code></pre> <p>Navigate to Log groups in Amazon CloudWatch console to see log groups starting with <code>/aws/eks/&lt;your-cluster-name&gt;/workload</code>.</p>"},{"location":"addons/cloudwatch-logs/#functionality","title":"Functionality","text":"<p>Applies the CloudWatch Logs add-on to an Amazon EKS cluster. </p>"},{"location":"addons/cloudwatch-logs/#performance-tuning","title":"Performance Tuning","text":"<p>By default, we send Fluent Bit application logs and Kubernetes metadata to CloudWatch. If you want to reduce the volume of data being sent to CloudWatch, you can stop one or both of these data sources from being sent to CloudWatch. Please take a look at Amazon CloudWatch documentation on Reducing the log volume from Fluent Bit and please look at the filter section of aws-for-fluent-bit chart to find ways to apply filters to performance tune your logs that are sent to Cloudwatch.</p>"},{"location":"addons/cluster-autoscaler/","title":"Cluster Autoscaler Add-on","text":"<p>The Cluster Autoscaler add-on adds support for Cluster Autoscaler to an EKS cluster. Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when:</p> <ul> <li>pods fail due to insufficient resources, or </li> <li>pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time.</li> </ul>"},{"location":"addons/cluster-autoscaler/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.ClusterAutoScalerAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/cluster-autoscaler/#functionality","title":"Functionality","text":"<ol> <li>Configure proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) as a Policy. </li> <li>Configures IAM Role for Service Account (IRSA) with the generated policy.</li> <li>Resolves proper CA image to pull based on the Kubernetes version.</li> <li>Applies proper tags for discoverability to the EC2 instances.</li> <li>Supports standard helm configuration options.</li> </ol> <p>The add-on automatically sets the following Helm Chart values, and it is highly recommended not to pass these values in (as it will result in a failed deployment):</p> <ul> <li><code>cloudProvider</code></li> <li><code>autoDiscovery.clusterName</code></li> <li><code>awsRegion</code></li> <li><code>rbac.serviceAccount.create</code></li> <li><code>rbac.serviceAccount.name</code></li> </ul>"},{"location":"addons/cluster-autoscaler/#testing-the-scaling-functionality","title":"Testing the scaling functionality","text":"<p>The following steps will help test and validate Cluster Autoscaler functionality in your cluster.</p> <ol> <li>Deploy a sample app as a deployment.</li> <li>Create a Horizontal Pod Autoscaler (HPA) resource.</li> <li>Generate load to trigger scaling.</li> </ol>"},{"location":"addons/cluster-autoscaler/#deploy-a-sample-app","title":"Deploy a sample app","text":"<p>Take a note of the number of nodes available:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-189-107.us-west-2.compute.internal   Ready    &lt;none&gt;   80m   v1.19.6-eks-49a6c0\n</code></pre> <p>The first step is to create a sample application via deployment and request 20m of CPU:</p> <pre><code>kubectl create deployment php-apache --image=us.gcr.io/k8s-artifacts-prod/hpa-example\nkubectl set resources deploy php-apache --requests=cpu=20m \nkubectl expose deployment php-apache --port 80\n</code></pre> <p>You can see that there's 1 pod currently running:</p> <pre><code>kubectl get pod -l app=php-apache\n</code></pre> <pre><code>NAME                          READY   STATUS    RESTARTS   AGE\nphp-apache-55c4584468-vsbl7   1/1     Running   0          63s\n</code></pre>"},{"location":"addons/cluster-autoscaler/#create-hpa-resource","title":"Create HPA resource","text":"<p>Now we can create Horizontal Pod Autoscaler resource with 50% CPU target utilization, and the minimum number of pods at 1 and max at 20: <pre><code>kubectl autoscale deployment php-apache \\\n    --cpu-percent=50 \\\n    --min=1 \\\n    --max=50\n</code></pre></p> <p>You can verify by looking at the hpa resource:</p> <pre><code>kubectl get hpa\n</code></pre> <pre><code>NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache   Deployment/php-apache   10%/50%   1         50        2          52s\n</code></pre>"},{"location":"addons/cluster-autoscaler/#generate-load","title":"Generate load","text":"<p>With the resources created, you can generate load on the apache server with a busybox container:</p> <pre><code>kubectl --generator=run-pod/v1 run -i --tty load-generator --image=busybox /bin/sh\n</code></pre> <p>You can generate the actual load on the shell by running a while loop:</p> <pre><code>while true; do wget -q -O - http://php-apache; done\n</code></pre>"},{"location":"addons/cluster-autoscaler/#verify-that-cluster-autoscaler-works","title":"Verify that Cluster Autoscaler works","text":"<p>While the load is being generated, access another terminal to verify that HPA is working. The following command should return a list of many nods created (as many as 10):</p> <pre><code>kubectl get pods -l app=php-apache -o wide --watch\n</code></pre> <p>With more pods being created, you would expect more nodes to be created; you can access the Cluster Autoscaler logs to confirm:</p> <pre><code>kubectl -n kube-system logs -f deployment/blueprints-addon-cluster-autoscaler-aws-cluster-autoscaler\n</code></pre> <p>Lastly, you can list all the nodes and see that there are now multiple nodes:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                         STATUS   ROLES    AGE   VERSION\nip-10-0-187-70.us-west-2.compute.internal    Ready    &lt;none&gt;   73s   v1.19.6-eks-49a6c0\nip-10-0-189-107.us-west-2.compute.internal   Ready    &lt;none&gt;   84m   v1.19.6-eks-49a6c0\nip-10-0-224-226.us-west-2.compute.internal   Ready    &lt;none&gt;   46s   v1.19.6-eks-49a6c0\nip-10-0-233-105.us-west-2.compute.internal   Ready    &lt;none&gt;   90s   v1.19.6-eks-49a6c0\n</code></pre>"},{"location":"addons/container-insights/","title":"Container Insights Add-on","text":"<p>!!  This add-on is deprecated in favour of CloudWatch insights to provide deeper integration with EKS through CloudWatch. Please use CloudWatch Insights instead linked here: AWS CloudWatch Insights !!</p> <p>The Container Insights add-on adds support for Container Insights to an EKS cluster.</p> <p>Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using an embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console.</p> <p>IMPORTANT</p> <p>CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events.</p> <p>Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing, see Amazon CloudWatch Pricing.</p> <p>Also it is important to note that this add-on can not co-exist with <code>adot-addon</code> on same EKS cluster. <code>adot-addon</code> and this add-on is mutually exclusive due to <code>adot-collector-sa</code> service account.</p>"},{"location":"addons/container-insights/#usage","title":"Usage","text":"<p>Add the following as an add-on to your main.ts file to add Containers Insights to your cluster</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.ContainerInsightsAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/container-insights/#prerequisites","title":"Prerequisites","text":"<p>Once the Container Insights add-on has been installed in your cluster, validate that the AWS Distro for Open Telemetry (ADOT) and the FluentBit daemons are running. </p> <pre><code>kubectl get all -n amazon-cloudwatch \nkubectl get all -n amazon-metrics\n</code></pre> <p>You should see output similar to the following respectively (assuming two node cluster): </p> <pre><code>NAME                   READY   STATUS    RESTARTS   AGE\npod/fluent-bit-5chvg   1/1     Running   2          100s\npod/fluent-bit-px7r6   1/1     Running   0          101s\n\nNAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/fluent-bit   2         2         2       2            2           &lt;none&gt;          100s\n\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/adot-collector-daemonset-k7n4p   1/1     Running   0          2m4s\npod/adot-collector-daemonset-qjdps   1/1     Running   0          114s\n\nNAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/adot-collector-daemonset   2         2         2       2            2           &lt;none&gt;          73m\n</code></pre> <p>To enable or disable control plane logs with the console, run the following command in your terminal.</p> <pre><code>aws eks update-cluster-config \\\n    --region us-east-2 \\\n    --name east-dev \\\n    --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}'\n</code></pre> <p>You should see a similar output as the following.</p> <pre><code>{\n    \"update\": {\n        \"id\": \"&lt;883405c8-65c6-4758-8cee-2a7c1340a6d9&gt;\",\n        \"status\": \"InProgress\",\n        \"type\": \"LoggingUpdate\",\n        \"params\": [\n            {\n                \"type\": \"ClusterLogging\",\n                \"value\": \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\"\n            }\n        ],\n        \"createdAt\": 1553271814.684,\n        \"errors\": []\n    }\n}\n</code></pre> <p>You can also monitor the status of your log configuration update to your cluster by running the following command. </p> <pre><code>aws eks describe-update \\\n    --region &lt;region-code&gt;\\\n    --name &lt;prod&gt; \\\n    --update-id &lt;883405c8-65c6-4758-8cee-2a7c1340a6d9&gt;\n</code></pre> <p>Once the update is complete, you should see a similar output.</p> <pre><code>{\n    \"update\": {\n        \"id\": \"&lt;883405c8-65c6-4758-8cee-2a7c1340a6d9&gt;\",\n        \"status\": \"Successful\",\n        \"type\": \"LoggingUpdate\",\n        \"params\": [\n            {\n                \"type\": \"ClusterLogging\",\n                \"value\": \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\"\n            }\n        ],\n        \"createdAt\": 1553271814.684,\n        \"errors\": []\n    }\n}\n</code></pre>"},{"location":"addons/container-insights/#view-metrics-for-cluster-and-workloads","title":"View metrics for cluster and workloads","text":"<p>Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance. </p> <p></p>"},{"location":"addons/container-insights/#view-cluster-level-logs","title":"View cluster level logs","text":"<p>After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console.</p> <p>To view these logs on the CloudWatch console follow these steps:</p> <ol> <li>Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks//cluster. <li>Choose the log stream to view. The following list describes the log stream name format for each log type.</li> <li>Kubernetes API server component logs (api) \u2013 kube-apiserver- <li>Audit (audit) \u2013 kube-apiserver-audit- <li>Authenticator (authenticator) \u2013 authenticator- <li>Controller manager (controllerManager) \u2013 kube-controller-manager- <li>Scheduler (scheduler) \u2013 kube-scheduler- <p>Next in the console, click on Log groups under Logs. </p> <p>You will see under log streams all the log streams from your Amazon EKS control plane. </p> <p></p>"},{"location":"addons/container-insights/#view-workload-level-logs","title":"View workload level logs","text":"<p>In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console</p> <p>In the navigation pane, choose Insights.</p> <p>Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events.</p> <p>In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at <code>/aws/containerinsights/east-dev/performance</code></p> <p>When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table.</p> <p>In the query editor, replace the default query with the following query and choose Run query.</p> <p>STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName <code>SORT avg_node_cpu_utilization DESC</code> This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like.</p> <p></p> <p>To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page.</p> <p>STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName <code>SORT avg_number_of_container_restarts DESC</code> This query displays a list of your pods, sorted by average number of container restarts as shown below</p> <p></p> <p>If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax.</p>"},{"location":"addons/container-insights/#view-containers-via-that-container-map-in-container-insights","title":"View containers via that container map in container insights.","text":"<p>In order to view a map of all of your containers running inside your cluster, click on <code>View your container map</code> in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.</p> <p></p>"},{"location":"addons/coredns/","title":"CoreDNS Amazon EKS Add-on","text":"<p>The <code>CoreDNS Amazon EKS Add-on</code> adds support for CoreDNS.</p> <p>CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. The CoreDNS Pods provide name resolution for all Pods in the cluster. For more information about CoreDNS, see  Using CoreDNS for Service Discovery in the Kubernetes documentation.</p> <p>Installing CoreDNS as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update CoreDNS. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable.</p> <p>Amazon EKS automatically installs CoreDNS as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.</p>"},{"location":"addons/coredns/#prerequisite","title":"Prerequisite","text":"<ul> <li>Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.</li> </ul>"},{"location":"addons/coredns/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.CoreDnsAddOn(\"v1.8.0-eksbuild.1\"); // optionally specify image version to pull or empty constructor\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/coredns/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Pass in the core-dns plugin version compatible with kubernetes-cluster version as shown below <pre><code># Assuming cluster version is 1.27, below command shows versions of the CoreDNS add-on available for the specified cluster's version.\naws eks describe-addon-versions \\\n--addon-name coredns \\\n--kubernetes-version 1.27 \\\n--query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text\n# Output\nv1.10.1-eksbuild.2\nFalse\nv1.10.1-eksbuild.1\nTrue\nv1.9.3-eksbuild.5\nFalse\nv1.9.3-eksbuild.3\nFalse\n</code></pre></li> <li>'configurationValues' : pass custom configurations to CoreDNS addon <pre><code>#Assuming coredns version is v1.10.1-eksbuild.1, below command shows the available configuration values that can be specified for coreDNS addon.\naws eks describe-addon-configuration \\\n--addon-name coredns \\\n--addon-version v1.10.1-eksbuild.1 \\\n--query configurationSchema \\\n--output text | jq '.definitions.Coredns.properties'\n\n# Output \n{\n  \"affinity\": {\n    \"default\": {\n      \"affinity\": {\n        \"nodeAffinity\": {\n          \"requiredDuringSchedulingIgnoredDuringExecution\": {\n            \"nodeSelectorTerms\": [\n              {\n                \"matchExpressions\": [\n                  {\n                    \"key\": \"kubernetes.io/os\",\n                    \"operator\": \"In\",\n                    \"values\": [\n                      \"linux\"\n                    ]\n                  },\n                  {\n                    \"key\": \"kubernetes.io/arch\",\n                    \"operator\": \"In\",\n                    \"values\": [\n                      \"amd64\",\n                      \"arm64\"\n                    ]\n                  }\n                ]\n              }\n            ]\n          }\n        },\n        \"podAntiAffinity\": {\n          \"preferredDuringSchedulingIgnoredDuringExecution\": [\n            {\n              \"podAffinityTerm\": {\n                \"labelSelector\": {\n                  \"matchExpressions\": [\n                    {\n                      \"key\": \"k8s-app\",\n                      \"operator\": \"In\",\n                      \"values\": [\n                        \"kube-dns\"\n                      ]\n                    }\n                  ]\n                },\n                \"topologyKey\": \"kubernetes.io/hostname\"\n              },\n              \"weight\": 100\n            }\n          ]\n        }\n      }\n    },\n    \"description\": \"Affinity of the coredns pods\",\n    \"type\": [\n      \"object\",\n      \"null\"\n    ]\n  },\n  \"computeType\": {\n    \"type\": \"string\"\n  },\n  \"corefile\": {\n    \"description\": \"Entire corefile contents to use with installation\",\n    \"type\": \"string\"\n  },\n  \"nodeSelector\": {\n    \"additionalProperties\": {\n      \"type\": \"string\"\n    },\n    \"type\": \"object\"\n  },\n  \"replicaCount\": {\n    \"type\": \"integer\"\n  },\n  \"resources\": {\n    \"$ref\": \"#/definitions/Resources\"\n  },\n  \"tolerations\": {\n    \"default\": [\n      {\n        \"key\": \"CriticalAddonsOnly\",\n        \"operator\": \"Exists\"\n      },\n      {\n        \"key\": \"node-role.kubernetes.io/master\",\n        \"operator\": \"NoSchedule\"\n      }\n    ],\n    \"description\": \"Tolerations of the coredns pod\",\n    \"items\": {\n      \"type\": \"object\"\n    },\n    \"type\": \"array\"\n  }\n}\n</code></pre> Note: To deploy fargate cluster, we need to pass configurationValue.computeType as \"Fargate\" as described in the EKS Fargate documentation</li> </ul>"},{"location":"addons/coredns/#validation","title":"Validation","text":"<p>To validate that coredns add-on is running, ensure that both the coredns pods are in Running state. <pre><code>$ kubectl get pods  -n kube-system|grep coredns\nNAME                           READY    STATUS    RESTARTS     AGE\ncoredns-644944ff4-2hjkj         1/1     Running     0          34d\ncoredns-644944ff4-fz6p5         1/1     Running     0          34d\n</code></pre> <pre><code># Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on\naws eks describe-addon \\\n    --cluster-name my-cluster \\\n    --addon-name coredns \\\n    --query \"addon.addonVersion\" \\\n    --output text\n# Output\nv1.8.0-eksbuild.1\n</code></pre></p>"},{"location":"addons/coredns/#functionality","title":"Functionality","text":"<p>Applies CoreDNS add-on to an Amazon EKS cluster. </p>"},{"location":"addons/datadog/","title":"Datadog Amazon EKS Blueprints AddOn","text":"<p>The Datadog Blueprints AddOn deploys the Datadog Agent on Amazon EKS using the eks-blueprints CDK construct.</p>"},{"location":"addons/datadog/#installation","title":"Installation","text":"<pre><code>npm install @datadog/datadog-eks-blueprints-addon\n</code></pre>"},{"location":"addons/datadog/#usage","title":"Usage","text":""},{"location":"addons/datadog/#using-an-existing-kubernetes-secret","title":"Using an existing Kubernetes secret","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { DatadogAddOn } from '@datadog/datadog-eks-blueprints-addon';\n\nconst app = new cdk.App();\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new DatadogAddOn({\n        // Kubernetes secret holding Datadog API key\n        // The value should be set with the `api-key` key in the secret object.\n        apiKeyExistingSecret: '&lt;secret name&gt;'\n    })\n];\n\nconst account = '&lt;aws account id&gt;'\nconst region = '&lt;aws region&gt;'\nconst props = { env: { account, region } }\nconst version = 'auto';\n\nnew blueprints.EksBlueprint(app, { id: '&lt;eks cluster name&gt;', addOns, version}, props)\n</code></pre>"},{"location":"addons/datadog/#using-aws-secrets-manager","title":"Using AWS Secrets Manager","text":"<p>Store the API key using AWS Secrets Manager:</p> <pre><code>aws secretsmanager create-secret --name &lt;secret name&gt; --secret-string &lt;api_key&gt; --region &lt;aws region&gt;\n</code></pre> <p>Refer to the previously created secret with <code>apiKeyAWSSecret</code>:</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { DatadogAddOn } from '@datadog/datadog-eks-blueprints-addon';\n\nconst app = new cdk.App();\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new DatadogAddOn({\n        apiKeyAWSSecret: '&lt;secret name&gt;'\n    })\n];\n\nconst account = '&lt;aws account id&gt;'\nconst region = '&lt;aws region&gt;'\nconst props = { env: { account, region } }\nconst version = 'auto'\n\nnew blueprints.EksBlueprint(app, { id: '&lt;eks cluster name&gt;', addOns, version}, props)\n</code></pre>"},{"location":"addons/datadog/#addon-options","title":"AddOn Options","text":"Option Description Default <code>apiKey</code> Your Datadog API key \"\" <code>appKey</code> Your Datadog APP key \"\" <code>apiKeyExistingSecret</code> Existing k8s Secret storing the API key \"\" <code>appKeyExistingSecret</code> Existing k8s Secret storing the APP key \"\" <code>apiKeyAWSSecret</code> Secret in AWS Secrets Manager storing the API key \"\" <code>appKeyAWSSecret</code> Secret in AWS Secrets Manager storing the APP key \"\" <code>namespace</code> Namespace to install the Datadog Agent \"default\" <code>version</code> Version of the Datadog Helm chart \"2.28.13\" <code>release</code> Name of the Helm release \"datadog\" <code>repository</code> Repository of the Helm chart \"https://helm.datadoghq.com\" <code>values</code> Configuration values passed to the chart. See options. {}"},{"location":"addons/datadog/#support","title":"Support","text":"<p>https://www.datadoghq.com/support/</p>"},{"location":"addons/ebs-csi-driver/","title":"EBS CSI Driver Amazon EKS Add-on","text":"<p>The <code>EBS CSI Driver Amazon EKS Add-on</code> allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. The driver allows you to use AWS KMS keys to encrypt EBS volumes (optionally).</p> <p>This driver is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage EBS volumes.</p> <p>For more information on the driver, please review the user guide.</p>"},{"location":"addons/ebs-csi-driver/#prerequisites","title":"Prerequisites","text":"<ul> <li>Amazon EKS EBS CSI Driver add-on is only available on Amazon EKS clusters running Kubernetes version 1.20 and later.</li> <li>Note that the version of the driver that can be used on an EKS cluster depends on the version of Kubernetes running in the cluster. See the configuration options section below for more details</li> </ul>"},{"location":"addons/ebs-csi-driver/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.EbsCsiDriverAddOn({\n                addOnName: \"aws-ebs-csi-driver\",\n                version: \"auto\",\n                versionMap: versionMap,\n                saName: \"ebs-csi-controller-sa\", \n                kmsKeys: [\n                  blueprints.getResource( context =&gt; new kms.Key(context.scope, \"ebs-csi-driver-key\", { alias: \"ebs-csi-driver-key\"})),\n                ],\n                storageClass: \"gp3\"\n              }\n            )\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/ebs-csi-driver/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Version of the EBS CSI Driver add-on to be installed. The version must be compatible with kubernetes cluster version.</li> <li><code>storageClass</code>: Storage Class type for AWS EBS Volumes, example: gp2, gp3</li> </ul> <pre><code># Command to show versions of the EBS CSI Driver add-on available for cluster version is 1.20\n aws eks describe-addon-versions \\\n--addon-name aws-ebs-csi-driver \\\n--kubernetes-version 1.20 \\\n--query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text\n\n# Output\nv1.4.0-eksbuild.preview\n</code></pre>"},{"location":"addons/ebs-csi-driver/#validation","title":"Validation","text":"<p>To validate that EBS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster</p> <pre><code>kubectl get pods -n kube-system | grep ebs\n\n# Output\nebs-csi-controller-95848f4d9-hlrzs   4/4     Running   0          5m8s\nebs-csi-controller-95848f4d9-m4f54   4/4     Running   0          4m38s\nebs-csi-node-c9xdf                   3/3     Running   0          5m8s\n</code></pre> <p>To validate, storageClass type and default across cluster: <pre><code>kubectl get storageclass\n\n# Output\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\napache-airflow-sc   efs.csi.aws.com         Delete          Immediate              false                  5d8h\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  5d8h\ngp3 (default)       ebs.csi.aws.com         Delete          WaitForFirstConsumer   false                  163m\n</code></pre></p> <p>Additionally, the <code>aws cli</code> can be used to determine which version of the add-on is installed in the cluster</p> <pre><code># Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on\naws eks describe-addon \\\n    --cluster-name my-cluster \\\n    --addon-name aws-ebs-csi-driver \\\n    --query \"addon.addonVersion\" \\\n    --output text\n\n# Output\nv1.4.0-eksbuild.preview\n</code></pre>"},{"location":"addons/ebs-csi-driver/#functionality","title":"Functionality","text":"<p>Applies the EBS CSI Driver add-on to an Amazon EKS cluster.</p>"},{"location":"addons/efs-csi-driver/","title":"EFS CSI Driver Amazon EKS Add-on","text":"<p>The <code>EFS CSI Driver Amazon EKS Add-on</code> provides a CSI interface that allows Kubernetes clusters running on AWS to manage the lifecycle of Amazon EFS volumes for persistent storage. EFS CSI driver supports both dynamic and static provisioning of storage.</p> <p>A couple of things to note:</p> <ul> <li>Driver is not compatible with Windows-based container images</li> <li>The number of replicas to be deployed must be less or equal to the number of nodes in the cluster</li> </ul> <p>For more information on the driver, please review the user guide.</p>"},{"location":"addons/efs-csi-driver/#prerequisites","title":"Prerequisites","text":"<ul> <li>The EFS file system itself must be created in AWS separately as the driver uses the EFS for storage, but it does not create it. You can create an EFS file system using the <code>CreateEfsFileSystemProvider</code>, e.g.: <code>.resourceProvider(\"efs-file-system\", new CreateEfsFileSystemProvider({name: 'efs-file-system'}))</code></li> </ul>"},{"location":"addons/efs-csi-driver/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.EfsCsiDriverAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/efs-csi-driver/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Version of the EFS CSI Driver add-on to be installed. Version 2.2.3 will be installed by default if a value is not provided</li> <li><code>replicaCount</code>: Number of replicas to be deployed. If not provided, two replicas will be deployed. Note that the number of replicas   should be less than or equal to the number of nodes in the cluster otherwise some pods will be left of pending state</li> <li><code>kmsKeys</code>: List of KMS keys used for encryption-at-rest, so that the IAM policy can be updated to allow the EFS CSI driver to access the keys</li> </ul>"},{"location":"addons/efs-csi-driver/#validation","title":"Validation","text":"<p>To validate that EFS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster</p> <pre><code>kubectl get pods -n kube-system | grep efs\n\n# Output\nefs-csi-controller-7c9bd5d86d-v7jtk            3/3     Running   0          155m\nefs-csi-node-2c29j                             3/3     Running   0          155m\n</code></pre> <p>Additionally, the driver documentation shows how to create an EFS file system to test the driver</p>"},{"location":"addons/efs-csi-driver/#functionality","title":"Functionality","text":"<p>Applies the EFS CSI Driver add-on to an Amazon EKS cluster.</p>"},{"location":"addons/efs-csi-driver/#notes","title":"Notes","text":"<p>If you run into this error: <code>Output: Could not start amazon-efs-mount-watchdog, unrecognized init system \"aws-efs-csi-dri\" b'mount.nfs4: access denied by server while mounting 127.0.0.1:/'</code></p> <p>Take a look at this workshop which shows you how to set up KMS keys for EFS and EBS: https://catalog.us-east-1.prod.workshops.aws/workshops/90c9d1eb-71a1-4e0e-b850-dba04ae92887/en-US/security/065-data-encryption/1-stack-setup.</p> <p>Similarly, if you face a mounting issue, take a look at this thread regarding EFS mounting issues: https://repost.aws/knowledge-center/eks-troubleshoot-efs-volume-mount-issues.</p>"},{"location":"addons/eks-pod-identity-agent/","title":"Amazon EKS Pod Identity Agent Add-on","text":"<p>Amazon EKS Pod Identity associations provide the ability to manage credentials for your applications, similar to the way that Amazon EC2 instance profiles provide credentials to Amazon EC2 instances.</p> <p>Amazon EKS Pod Identity provides credentials to your workloads with an additional EKS Auth API and an agent pod that runs on each node</p> <p>For more information on the driver, please review the user guide.</p>"},{"location":"addons/eks-pod-identity-agent/#prerequisites","title":"Prerequisites","text":"<ul> <li>Amazon EKS Pod Identity Driver add-on is only available on Amazon EKS clusters running Kubernetes version 1.24 and later.</li> <li>EKS Pod Identities are available on Linux Amazon EC2 instances</li> </ul>"},{"location":"addons/eks-pod-identity-agent/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.EksPodIdentityAgentAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/eks-pod-identity-agent/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Version of the Amazon EKS Pod Identity Agent add-on to be installed. The version must be compatible with kubernetes cluster version.</li> </ul> <pre><code># Command to show versions of the EKS Pod Identity Agent add-on available for cluster version is 1.24\n aws eks describe-addon-versions \\\n--addon-name eks-pod-identity-agent \\\n--kubernetes-version 1.24 \\\n--query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text\n\n# Output\nv1.0.0-eksbuild.1\n</code></pre>"},{"location":"addons/eks-pod-identity-agent/#validation","title":"Validation","text":"<p>To validate that EKS Pod Identity Agent add-on is installed properly, ensure that the pods are running in the cluster</p> <pre><code>kubectl get pods -n kube-system | grep 'eks-pod-identity-agent'\n\n# Output\neks-pod-identity-agent-gmqp7                                          1/1     Running   1 (24h ago)   24h\neks-pod-identity-agent-prnsh                                          1/1     Running   1 (24h ago)   24h\n</code></pre> <p>Additionally, the <code>aws cli</code> can be used to determine which version of the add-on is installed in the cluster <pre><code># Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on\naws eks describe-addon \\\n    --cluster-name my-cluster \\\n    --addon-name eks-pod-identity-agent \\\n    --query \"addon.addonVersion\" \\\n    --output text\n\n# Output\nv1.0.0-eksbuild.1\n</code></pre></p>"},{"location":"addons/eks-pod-identity-agent/#functionality","title":"Functionality","text":"<p>Applies the EKS Pod Identity Agent add-on to an Amazon EKS cluster.</p>"},{"location":"addons/emr-eks/","title":"EMR on EKS Add-on","text":"<p>The <code>Amazon EMR on EKS Add-on</code> enables EMR on EKS service to use an EKS cluster. It creates the EMR on EKS IAM Service Linked Role and adds it to <code>awsAuth</code> configmap. This Add-on MUST be used with EMR on EKS Team.</p>"},{"location":"addons/emr-eks/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.EmrEksAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/emr-eks/#verify","title":"Verify","text":"<p>Once the AddOn is deployed you can execute the following command:</p> <p><pre><code>kubectl describe -n kube-system configmap/aws-auth\n</code></pre> The output of the command would show a list of IAM role and mapping to Kuberenets users, one fo the mapping would be for EMR on EKS role and would be similar to below.</p> <pre><code>  mapRoles: |\n    - rolearn: arn:aws:iam::&lt;your-account-id&gt;:role/AWSServiceRoleForAmazonEMRContainers\n      username: emr-containers\n</code></pre>"},{"location":"addons/external-dns/","title":"External DNS Add-on","text":"<p>External DNS add-on is based on the ExternalDNS open source project and allows integration of exposed Kubernetes services and Ingresses with DNS providers, in particular Amazon Route 53.</p> <p>The add-on provides functionality to configure IAM policies and Kubernetes service accounts for Route 53 integration support based on AWS Tutorial for External DNS.</p>"},{"location":"addons/external-dns/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst hostedZoneName = ...\n\nconst addOn = new blueprints.addons.ExternalDnsAddOn({\n    hostedZoneResources: [hostedZoneName]; // can be multiple\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .resourceProvider(hostedZoneName, new blueprints.LookupHostedZoneProvider(hostedZoneName))\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that external DNS add-on is running ensure that the add-on deployment is in <code>RUNNING</code> state:</p> <pre><code># Assuming add-on is installed in the external-dns namespace.\n$ kubectl get po -n external-dns\nNAME                           READY   STATUS    RESTARTS   AGE\nexternal-dns-fcf6c9c66-xd8f4   1/1     Running   0          3d3h\n</code></pre>"},{"location":"addons/external-dns/#using-external-dns","title":"Using External DNS","text":"<p>You can now provision external services and ingresses integrating with Route 53. For example to provision an NLB you can use the following service manifest:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '60'\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n    external-dns.alpha.kubernetes.io/hostname: &lt;MyDomainName&gt;\n  name: udp-test1\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    name: your-app\n</code></pre> <p>Note the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation for the service name that allows specifying its domain name. </p>"},{"location":"addons/external-dns/#hosted-zone-providers","title":"Hosted Zone Providers","text":"<p>In order for external DNS to work, you need to supply one or more hosted zones. Hosted zones are expected to be supplied leveraging resource providers.</p> <p>To help customers handle common use cases for Route 53 provisioning the framework provides a few convenience providers that can be registered with the EKS Blueprint Stack. </p> <p>Name look-up and direct import provider: This provider will allow to bind to an existing hosted zone based on its name.</p> <pre><code>const myDomainName = \"\";\n\nblueprints.EksBlueprint.builder()\n    //  Register hosted zone1 under the name of MyHostedZone1\n    .resourceProvider(\"MyHostedZone1\", new blueprints.LookupHostedZoneProvider(myDomainName))\n    .addOns(new blueprints.addons.ExternalDnsAddOn({\n        hostedZoneResources: [\"MyHostedZone1\"];\n    }))\n    .build(...);\n</code></pre> <p>If the hosted zone ID is known, then the recommended approach is to use a <code>ImportHostedZoneProvider</code> which bypasses any lookup calls.</p> <pre><code>const myHostedZoneId = \"\";\nblueprints.EksBlueprint.builder()\n    //  Register hosted zone1 under the name of MyHostedZone1\n    .resourceProvider(\"MyHostedZone1\",  new blueprints.ImportHostedZoneProvider(myHostedZoneId))\n    .addOns(new blueprints.addons.ExternalDnsAddOn({\n        hostedZoneResources: [\"MyHostedZone1\"];\n    }))\n    .version(\"auto\")\n    .build(...);\n</code></pre> <p>Delegating Hosted Zone Provider</p> <p>In many cases, enterprises choose to decouple provisioning of root domains and subdomains. A common pattern is to have a specific DNS account (AWS account) hosting the root domain, while subdomains may be created within individual workload accounts. </p> <p>This implies cross-account access from child account to the parent DNS account for subdomain delegation. </p> <p>Prerequisites:</p> <ol> <li> <p>Parent account defines an IAM role with the following managed policies: <code>AmazonRoute53DomainsFullAccess</code> <code>AmazonRoute53ReadOnlyAccess</code> <code>AmazonRoute53AutoNamingFullAccess</code></p> </li> <li> <p>Create trust relationship between this role and the child account that is expected to add subdomains. For info see IAM tutorial.</p> </li> </ol> <p>Example:</p> <p>Let's assume that parent DNS account <code>parentAccountId</code> has domain named <code>myglobal-domain.com</code>. Now when provisioned an EKS Blueprints cluster you would like to use a stage specific name like <code>dev.myglobal-domain.com</code> or <code>test.myglobal-domain.com</code>. In addition to these requirements, you would like to enable tenant specific access to those domains such as <code>my-tenant1.dev.myglobal-domain.com</code> or team specific access <code>team-riker.dev.myglobal-domain.com</code>. </p> <p>The setup will look the following way:</p> <ol> <li>In the <code>parentAccountId</code> account you create a role for delegation (<code>DomainOperatorRole</code> in this example) and a trust relationship to the child account in which EKS Blueprints will be provisioned. In a general case, the number of child accounts can be large, so each of them will have to be listed in the trust relationship.</li> <li>In the <code>parentAccountId</code> you create a public hosted zone for <code>myglobal-domain.com</code>. With that the setup that may require separate automation (or a manual process) is complete. </li> <li>Use the following configuration of the add-on:</li> </ol> <pre><code>blueprints.EksBlueprint.builder()\n    //  Register hosted zone1 under the name of MyHostedZone1\n    .resourceProvider(\"MyHostedZone1\",  new blueprints.DelegatingHostedZoneProvider({\n        parentDomain: 'myglobal-domain.com',\n        subdomain: 'dev.myglobal-domain.com', \n        parentAccountId: parentDnsAccountId,\n        delegatingRoleName: 'DomainOperatorRole',\n        wildcardSubdomain: true\n    }))\n    .addOns(new blueprints.addons.ExternalDnsAddOn({\n        hostedZoneResources: [\"MyHostedZone1\"];\n    }))\n    .version(\"auto\")\n</code></pre> <p>The parameter <code>wildcardSubdomain</code> above when set to true will also create a CNAME for <code>*.dev.myglobal-domain.com</code>. This is at the moment used for host based routing within EKS (e.g. with NGINX ingress).</p>"},{"location":"addons/external-dns/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>namespace</code>: Optional target namespace where add-on will be installed. Changing this value on the operating cluster is not recommended. Set to <code>external-dns</code> by default.</li> <li><code>version</code>: The add-on is leveraging a Bitnami helm chart. This parameter allows overriding the helm chart version used.</li> <li><code>hostedZone</code>: Hosted zone provider is a interface that provides one or more hosted zones that the add-on will leverage for the service and ingress configuration.</li> </ul>"},{"location":"addons/external-dns/#functionality","title":"Functionality","text":"<ol> <li>Applies External-DNS configuration for AWS DNS provider. See AWS Tutorial for External DNS for more information.</li> <li>Supports standard helm configuration options.</li> </ol>"},{"location":"addons/external-secrets/","title":"External Secrets Add-On","text":"<p>External Secrets add-on is based on External Secrets Operator (ESO) and allows integration with third-party secret stores like AWS Secrets Manager, AWS Systems Manager Parameter Store and inject the values into the EKS cluster as Kubernetes Secrets.</p>"},{"location":"addons/external-secrets/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.ExternalsSecretsAddOn({});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(addOn)\n    .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/external-secrets/#cluster-secret-store","title":"Cluster Secret Store","text":"<p>Create a ClusterSecretStore which can be used by all ExternalSecrets from all namespaces.</p> <p>Below example is for integration with AWS Secrets Manager:</p> <pre><code>import * as eks from 'aws-cdk-lib/aws-eks';\n\nconst cluster = blueprint.getClusterInfo().cluster;\n\nconst clusterSecretStore = new eks.KubernetesManifest(scope, \"ClusterSecretStore\", {\n    cluster: cluster,\n    manifest: [\n        {\n            apiVersion: \"external-secrets.io/v1beta1\",\n            kind: \"ClusterSecretStore\",\n            metadata: {name: \"default\"},\n            spec: {\n                provider: {\n                    aws: {\n                        service: \"SecretsManager\",\n                        region: region,\n                        auth: {\n                            jwt: {\n                                serviceAccountRef: {\n                                    name: \"external-secrets-sa\",\n                                    namespace: \"external-secrets\",\n                                },\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    ],\n});\n</code></pre> <p>Below example is for integration with AWS Systems Manager Parameter Store:</p> <pre><code>import * as eks from 'aws-cdk-lib/aws-eks';\n\nconst cluster = blueprint.getClusterInfo().cluster;\n\nconst clusterSecretStore = new eks.KubernetesManifest(scope, \"ClusterSecretStore\", {\n    cluster: cluster,\n    manifest: [\n        {\n            apiVersion: \"external-secrets.io/v1beta1\",\n            kind: \"ClusterSecretStore\",\n            metadata: {name: \"default\"},\n            spec: {\n                provider: {\n                    aws: {\n                        service: \"ParameterStore\",\n                        region: region,\n                        auth: {\n                            jwt: {\n                                serviceAccountRef: {\n                                    name: \"external-secrets-sa\",\n                                    namespace: \"external-secrets\",\n                                },\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    ],\n});\n</code></pre>"},{"location":"addons/external-secrets/#external-secret","title":"External Secret","text":"<p>Create an ExternalSecret which can be used to fetch, transform and inject secret data</p> <pre><code>import * as eks from 'aws-cdk-lib/aws-eks';\n\nconst cluster = blueprint.getClusterInfo().cluster;\nconst keyfiles = new eks.KubernetesManifest(scope, \"ExternalSecret\", {\n    cluster: cluster,\n    manifest: [\n        {\n            apiVersion: \"external-secrets.io/v1beta1\",\n            kind: \"ExternalSecret\",\n            metadata: {name: \"the-external-secret-name\"},\n            spec: {\n                secretStoreRef: {\n                    name: \"default\",\n                    kind: \"ClusterSecretStore\",\n                },\n                target: {\n                    name: \"the-kubernetes-secret-name\",\n                    creationPolicy: \"Merge\",\n                },\n                data: [\n                    {\n                        secretKey: \"secret-key-to-be-managed\",\n                        remoteRef: {\n                            key: \"the-providers-secret-name\",\n                            property: \"the-provider-secret-property\",\n                        },\n                    },\n                ],\n            },\n        },\n    ],\n});\n</code></pre>"},{"location":"addons/fluxcd/","title":"FluxCD Add-on","text":"<p>This add-on installs fluxcd.</p> <p>Flux is a declarative, GitOps-based continuous delivery tool that can be integrated into any CI/CD pipeline. The ability to manage deployments to multiple remote Kubernetes clusters from a central management cluster, support for progressive delivery, and multi-tenancy are some of the notable features of Flux.</p>"},{"location":"addons/fluxcd/#usage","title":"Usage","text":"<p>Only specify unique values for the repository and bucket <code>name</code> field.</p>"},{"location":"addons/fluxcd/#single-bootstrap-repo-path","title":"Single bootstrap repo path","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.FluxCDAddOn({\n    repositories:[{\n         name: \"aws-observability-accelerator\",\n         namespace: undefined,\n         repository: {\n             repoUrl: 'https://github.com/aws-observability/aws-observability-accelerator',\n             targetRevision: \"main\",\n         },\n         values: {\n             \"region\": \"us-east-2\"\n         },\n         kustomizations: [{kustomizationPath: \"./artifacts/grafana-operator-manifests/eks/infrastructure\"}],\n    }],\n})\n...\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/fluxcd/#multiple-bootstrap-repo-paths","title":"Multiple bootstrap repo paths","text":"<p>Multiple bootstrap repo paths are useful when you want to create multiple Kustomizations, pointing to different paths, e.g. to deploy manifests from specific subfolders in your repository:</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.FluxCDAddOn({\n    repositories: [{\n        name: \"aws-observability-accelerator\",\n        namespace: undefined,\n        repository: {\n            repoUrl: 'https://github.com/aws-observability/aws-observability-accelerator',\n            targetRevision: \"main\",\n        },\n        values: {\n            \"region\": \"us-east-2\"\n        },\n        kustomizations: [{kustomizationPath:\"./artifacts/grafana-operator-manifests/eks/infrastructure\"}, {kustomizationPath: \"./artifacts/grafana-operator-manifests/eks/java\"}]\n    }],\n})\n...\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(addOn)\n    .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/fluxcd/#single-bootstrap-s3-bucket-repository","title":"Single bootstrap S3 bucket repository","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport * as s3 from 'aws-cdk-lib/aws-s3';\n\nconst app = new cdk.App();\n\nconst fluxBootstrap = new s3.Bucket(this, \"FluxBootstrap\", {\n    removalPolicy: cdk.RemovalPolicy.RETAIN,\n});\n\nconst addOn = new blueprints.addons.FluxCDAddOn({\n    buckets: [{\n        name: \"bootstrap-bucket\",\n        bucketName: fluxBootstrap.bucketName,\n        bucketRegion: cdk.Aws.REGION,\n    }],\n})\n...\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>By default the FluxCD source-controller attempts to access the S3 bucket by using the IAM instance profile. To grant access assign node group instances IAM role granting read access to the S3 bucket. Alternatively reference to a Secret containing the <code>accesskey</code> and <code>secretkey</code> values with the <code>secretRef</code> parameter to authenticate using IAM user authentication. See FluxCD Bucket Source Controller documentation.</p>"},{"location":"addons/fluxcd/#workload-repositories","title":"Workload Repositories","text":"<ol> <li>To add workload repositories as well as the bootstrap repository, please follow this example below </li> </ol> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst nginxDashUrl = \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/nginx/nginx.json\"\nconst javaDashUrl = \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\"\n\nconst addOn = new blueprints.addons.FluxCDAddOn({\n    repositories: [\n        {\n            name: \"aws-observability-accelerator\",\n            namespace: undefined,\n            repository: {\n                repoUrl: 'https://github.com/aws-observability/aws-observability-accelerator',\n                targetRevision: \"main\",\n            },\n            values: {\n                \"GRAFANA_NGINX_DASH_URL\" : nginxDashUrl,\n                \"GRAFANA_JAVA_JMX_DASH_URL\": javaDashUrl,\n            },\n            kustomizations: [{kustomizationPath:\"./artifacts/grafana-operator-manifests/eks/infrastructure\"}, {kustomizationPath: \"./artifacts/grafana-operator-manifests/eks/java\"}]\n        },\n        {\n            name: \"podinfo\",\n            namespace: undefined,\n            repository: {\n                repoUrl: 'https://github.com/stefanprodan/podinfo',\n                targetRevision: \"master\",\n            },\n            values: {\n                \"region\": \"us-east-2\"\n            },\n            kustomizations: [{kustomizationPath: \"./kustomize\", kustomizationTargetNamespace: \"default\"}],\n        }\n    ],\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(\n        new blueprints.addons.GrafanaOperatorAddon,\n        addOn,\n    )\n    .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/fluxcd/#secret-management-for-private-git-repositories-with-fluxcd","title":"Secret Management for private Git repositories with FluxCD","text":"<p>Please follow the below steps if you are looking to setup FluxCD addon to read secrets and sync private Git repos.</p> <p>Note that different git servers may require different authentication methods. For example, GitHub requires a basic access token, while others may require a bearer token. For more information, refer to the FluxCD documentation.</p> <ol> <li>Please use the following CLI command to create an AWS Secrets Manager secret for <code>basic-access-auth</code>.</li> </ol> <pre><code>export SECRET_NAME=basic-access-auth\nexport GIT_USERNAME=&lt;YOUR_GIT_USERNAME&gt;\nexport GIT_TOKEN=&lt;YOUR_GIT_TOKEN&gt;\nexport AWS_REGION=&lt;YOUR_AWS_REGION&gt;\naws secretsmanager create-secret \\\n  --name $SECRET_NAME \\\n  --description \"Your GIT BASIC ACCESS SECRET\" \\\n  --secret-string \"{ 'username': '${GIT_USERNAME}', 'password': '${GIT_TOKEN}' }\" \\\n  --region $AWS_REGION\n</code></pre> <ol> <li>Below is the snippet showing the usage of adding <code>ExternalsSecretsAddOn</code> to read secrets from AWS Secrets Manager and configuring <code>FluxCDAddOn</code> to read private repositories.</li> </ol> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { ExternalOperatorSecretAddon } from './externaloperatorsecretaddon';\n\nconst app = new cdk.App();\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n  new blueprints.addons.ExternalsSecretsAddOn(),\n  new blueprints.addons.FluxCDAddOn({\n    repositories:[\n      {\n        name: \"&lt;&lt;YOUR_FLUX_APP_NAME&gt;&gt;\",\n        namespace: \"&lt;&lt;YOUR_FLUX_ADDON_NAMESPACE&gt;&gt;\",\n        repository: {\n            repoUrl: '&lt;&lt;YOUR_PRIVATE_GIT_REPOSITORY&gt;&gt;',\n            targetRevision: \"&lt;&lt;YOUR_TARGET_REVISION&gt;&gt;\",\n        },\n        values: {\n            \"region\": \"us-east-1\"\n        },\n        kustomizations: [{kustomizationPath: \"&lt;&lt;YOUR_FLUX_SYNC_PATH&gt;&gt;\"}],\n        // This is the name of the kubernetes secret to be created by `ExternalSecret` shown in step 3.\n        secretRefName: \"repository-creds\" \n      }\n    ],\n\n  }),\n  new ExternalOperatorSecretAddon(),\n];\n\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOns)\n  .build(app, 'my-stack-name');\n</code></pre> <ol> <li>Below is the code snippet <code>externaloperatorsecretaddon.ts</code> which shows the mechanism to setup <code>ClusterSecretStore</code> and <code>ExternalSecret</code> to read AWS Secrets Manager <code>basic-access-auth</code> secret which is a GIT basic access authentication username and password to sync private repositories :</li> </ol> <pre><code>import 'source-map-support/register';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport * as eks from \"aws-cdk-lib/aws-eks\";\nimport { Construct } from 'constructs';\nimport { dependable } from '@aws-quickstart/eks-blueprints/dist/utils';\n\nexport class ExternalOperatorSecretAddon implements blueprints.ClusterAddOn {\n    id?: string | undefined;\n    @dependable(blueprints.addons.ExternalsSecretsAddOn.name)\n    deploy(clusterInfo: blueprints.ClusterInfo): void | Promise&lt;Construct&gt; {\n        const cluster = clusterInfo.cluster;\n        const secretStore = new eks.KubernetesManifest(clusterInfo.cluster.stack, \"ClusterSecretStore\", {\n            cluster: cluster,\n            manifest: [\n                {\n                    apiVersion: \"external-secrets.io/v1beta1\",\n                    kind: \"ClusterSecretStore\",\n                    metadata: {\n                        name: \"secret-manager-store\",\n                        namespace: \"default\"\n                    },\n                    spec: {\n                        provider: {\n                            aws: {\n                                service: \"SecretsManager\",\n                                region: clusterInfo.cluster.stack.region,\n                                auth: {\n                                    jwt: {\n                                        serviceAccountRef: {\n                                            name: \"external-secrets-sa\",\n                                            namespace: \"external-secrets\",\n                                        },\n                                    },\n                                },\n                            },\n                        },\n                    },\n                },\n            ],\n        });\n\n        const externalSecret = new eks.KubernetesManifest(clusterInfo.cluster.stack, \"ExternalSecret\", {\n            cluster: cluster,\n            manifest: [\n                {\n                    apiVersion: \"external-secrets.io/v1beta1\",\n                    kind: \"ExternalSecret\",\n                    metadata: {\n                        name: \"git-admin-credentials\",\n                        namespace: \"flux-system\"\n                    },\n                    spec: {\n                        secretStoreRef: {\n                            name: \"secret-manager-store\",\n                            kind: \"ClusterSecretStore\",\n                        },\n                        target: {\n                            name: \"repository-creds\"\n                        },\n                        dataFrom: [\n                            {\n                                extract: {\n                                    key: \"basic-access-auth\"\n                                },\n                            },\n                        ],\n                    },\n                },\n            ],\n        });\n        externalSecret.node.addDependency(secretStore);\n        return Promise.resolve(secretStore);\n    }\n}\n</code></pre> <ol> <li>Upon execution of the above blueprint, the above <code>ExternalSecret</code> Kubernetes resource will take care of creating a Kubernetes Secret as shown below in <code>flux-system</code> namespace :</li> </ol> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: repository-creds\n  namespace: flux-system\ntype: Opaque\ndata:\n  username: &lt;&lt;BASE64 ENCODED SECRET OF YOUR_GIT_USERNAME STORE in AWS SECRETS MANAGER&gt;&gt;\n  password: &lt;&lt;BASE64 ENCODED SECRET OF YOUR_GIT_TOKEN STORE in AWS SECRETS MANAGER&gt;&gt;\n</code></pre> <p>As pointed, if you are looking to use <code>secretRef</code> to reference a secret for FluxCD addon to sync private Git repos, please make sure the referenced secret is already created in the namespace ahead of time in AWS Secrets Manager as shown above. You can use External Secrets Addon to learn more about external secrets operator which allows integration with third-party secret stores like AWS Secrets Manager, AWS Systems Manager Parameter Store and inject the values into the EKS cluster as Kubernetes Secrets.</p>"},{"location":"addons/fluxcd/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you.</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to the FluxCD Helm Chart documentation for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options).</li> </ul>"},{"location":"addons/fluxcd/#validation","title":"Validation","text":"<p>To validate that fluxcd is installed properly in the cluster, check if the namespace is created and fluxcd pods are running.</p> <p>Verify if the namespace is created correctly</p> <pre><code>  kubectl get ns | grep \"flux-system\"\n</code></pre> <p>There should be list the flux-system namespace</p> <pre><code>flux-system      Active   31m\n</code></pre> <p>Verify if the pods are running correctly in flux-system namespace</p> <pre><code>  kubectl get pods -n flux-system  \n</code></pre> <p>There should list 3 pods starting with name flux-system For Eg:</p> <pre><code>NAME                                          READY   STATUS    RESTARTS   AGE\nhelm-controller-65cc46469f-v4hnr              1/1     Running   0          6m13s\nimage-automation-controller-d8f7bfcb4-v4pz8   1/1     Running   0          6m13s\nimage-reflector-controller-68979dfd49-t4dpj   1/1     Running   0          6m13s\nkustomize-controller-767677f7f5-7j26b         1/1     Running   0          6m13s\nnotification-controller-55d8c759f5-zqd6f      1/1     Running   0          6m13s\nsource-controller-58c66d55cd-4f6vh            1/1     Running   0          6m13s\n</code></pre>"},{"location":"addons/fluxcd/#testing","title":"Testing","text":"<p>The Flux CLI is available as a binary executable for all major platforms, the binaries can be downloaded form GitHub releases page.</p> <p>Install Fluxcd client <pre><code>curl -s https://fluxcd.io/install.sh | sudo bash\n. &lt;(flux completion bash)\n</code></pre></p> <p>Run the below command to check on the <code>GitRepository</code> setup with Flux :</p> <pre><code>kubectl get gitrepository -A\nNAME      URL                                       AGE   READY   STATUS                                                                        \npodinfo   https://github.com/stefanprodan/podinfo   5s    True    stored artifact for revision 'master@sha1:132f4e719209eb10b9485302f8593fc0e680f4fc'\n</code></pre> <p>Run the below command to check on the <code>Kustomization</code> setup with Flux :</p> <pre><code>\u276f kubectl get kustomizations.kustomize.toolkit.fluxcd.io -A\nNAMESPACE     NAME      AGE   READY   STATUS\nflux-system   podinfo   12m   True    Applied revision: master@sha1:073f1ec5aff930bd3411d33534e91cbe23302324\n</code></pre>"},{"location":"addons/gmaestro/","title":"gMaestro add-on for Amazon EKS Blueprints","text":"<p>This add-on deploys the gMaestro Agent on Amazon EKS using the eks-blueprints CDK construct.</p> <p>gMaestro is a Kubernetes cost optimization solution that helps companies reduce spending on un-utilized resources by up to 60%. With gMaestro, you gain full visibility into K8s clusters, seamlessly interact with HPA scaling policies, and achieve your cost-performance goals by applying custom rightsizing recommendations based on actual usage in production.</p> <p>This add-on will deploy gMaestro agent on a namespace of your choice and create it if it doesn't exist.</p>"},{"location":"addons/gmaestro/#prerequisites","title":"Prerequisites","text":"<p>Before using gMaestro, you need to: 1. Sign up to the gMaestro platform 2. Download a config YAML file - After signing up to gMaestro, navigate to the Deploy on the left-hand menu, fill in the required fields and click on \"Generate Config File\" as shown bellow:</p> <p></p> <p></p> <ol> <li>Create a secret (as a plaintext, not key/value) in AWS Secrets Manager:</li> </ol> <pre><code>export MAESTRO_CLIENT_ID=\"&lt;MAESTRO_CLIENT_ID value from the deployment section in the downloaded config file&gt;\"\nexport MAESTRO_SECRET_NAME=\"&lt;MAESTRO_SECRET_NAME your preferred secret name&gt;\"\naws secretsmanager create-secret --name &lt;MAESTRO_SECRET_NAME&gt; --region $AWS_REGION \\\n    --description \"Encrypted client ID for Granulate gMaestro\" \\\n    --secret-string \"&lt;MAESTRO_CLIENT_ID&gt;\"\n</code></pre>"},{"location":"addons/gmaestro/#installation","title":"Installation","text":"<p>Using npm:</p> <pre><code>$ npm i @granulate/gmaestro-eks-blueprints-addon\n</code></pre>"},{"location":"addons/gmaestro/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport {GmaestroAddOn} from '@granulate/gmaestro-eks-blueprints-addon';\n\nconst app = new cdk.App();\n\nconst addOn = new GmaestroAddOn({ // please see further section for parameter details\n        clientIdSecretName: \"&lt;MAESTRO_SECRET_NAME&gt;\",\n        clusterName: \"&lt;MAESTRO_SERVICE_NAME&gt;\",\n        createNamespace: &lt;true/false&gt;,\n        namespace: \"&lt;namespace&gt;\"\n    });\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .build(app, '&lt;my-stack-name&gt;');\n</code></pre>"},{"location":"addons/gmaestro/#add-on-options","title":"Add-on Options","text":"Option Description Default <code>clientIdSecretName</code> Please assign the <code>MAESTRO_SECRET_NAME</code> value from section Prerequisites, step 3 above. <code>clusterName</code> Navigate to Deployment section in the downloaded config file and use the value of <code>MAESTRO_SERVICE_NAME</code> environment variable <code>createNamespace</code> Set to <code>true</code> if you want the CDK to create the namespace for you false <code>namespace</code> The namespace where gMaestro will be deployed \"default\""},{"location":"addons/gmaestro/#support","title":"Support","text":"<p>If you have questions about gMaestro, catch us on Slack!</p>"},{"location":"addons/gpu-operator/","title":"NVIDIA GPU-Operator Add-on","text":"<p>Configuring and managing nodes with NVIDIA GPUs requires configuration of multiple software components such as drivers, container runtimes or other libraries which is difficult and error-prone. The gpu-operator automates the management of all NVIDIA software components needed to provision GPU. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling, DCGM based monitoring and others. The GPU Operator allows administrators of Kubernetes clusters to manage GPU nodes just like CPU nodes in the cluster. Instead of provisioning a special OS image for GPU nodes, administrators can rely on a standard OS image for both CPU and GPU nodes and then rely on the GPU Operator to provision the required software components for GPUs.</p>"},{"location":"addons/gpu-operator/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.GpuOperatorAddon(),\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/gpu-operator/#configuration-options","title":"Configuration Options","text":"<ul> <li> <p><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you.</p> </li> <li> <p><code>values</code>: Arbitrary values to pass to the chart. Refer to the GPU Operator Chart Customization Options for additional details. It also supports all standard helm configuration options.</p> </li> </ul>"},{"location":"addons/gpu-operator/#validation","title":"Validation","text":"<p>To validate that Grafana Operator is installed properly in the cluster, check if the namespace is created and pods are running.</p> <p>Verify if the namespace is created correctly <pre><code>kubectl get ns | grep \"gpu-operator\"\n</code></pre> There should be list the gpu-operator namespace <pre><code>gpu-operator         Active   8h\n</code></pre> Verify if everything is running correctly in the gpu-operator namespace <pre><code>kubectl get all -n gpu-operator  \n</code></pre> This should list 3 service i.e. gpu-operator, node feature discovery and dcgm exporter, 2 deployment, and 2 replica-set starting with name grafana-operator  For Eg: <pre><code>NAME                                                                  READY   STATUS      RESTARTS   AGE\npod/gpu-feature-discovery-z46kh                                       1/1     Running     0          8h\npod/gpu-operator-5f9dfcb867-ccwm5                                     1/1     Running     0          8h\npod/nvidia-container-toolkit-daemonset-nrb2d                          1/1     Running     0          8h\npod/nvidia-cuda-validator-ln9mf                                       0/1     Completed   0          8h\npod/nvidia-dcgm-exporter-p9n2w                                        1/1     Running     0          8h\npod/nvidia-device-plugin-daemonset-lwbb7                              1/1     Running     0          8h\npod/nvidia-device-plugin-validator-rlh86                              0/1     Completed   0          8h\npod/nvidia-gpu-operator-node-feature-discovery-master-6fb7d9469w4hw   1/1     Running     0          8h\npod/nvidia-gpu-operator-node-feature-discovery-worker-82b48           1/1     Running     0          8h\npod/nvidia-operator-validator-5b7j2                                   1/1     Running     0          8h\n\nNAME                                                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/gpu-operator                                        ClusterIP   172.20.181.207   &lt;none&gt;        8080/TCP   8h\nservice/nvidia-dcgm-exporter                                ClusterIP   172.20.81.118    &lt;none&gt;        9400/TCP   8h\nservice/nvidia-gpu-operator-node-feature-discovery-master   ClusterIP   172.20.198.97    &lt;none&gt;        8080/TCP   8h\n\nNAME                                                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE\ndaemonset.apps/gpu-feature-discovery                               1         1         1       1            1           nvidia.com/gpu.deploy.gpu-feature-discovery=true   8h\ndaemonset.apps/nvidia-container-toolkit-daemonset                  1         1         1       1            1           nvidia.com/gpu.deploy.container-toolkit=true       8h\ndaemonset.apps/nvidia-dcgm-exporter                                1         1         1       1            1           nvidia.com/gpu.deploy.dcgm-exporter=true           8h\ndaemonset.apps/nvidia-device-plugin-daemonset                      1         1         1       1            1           nvidia.com/gpu.deploy.device-plugin=true           8h\ndaemonset.apps/nvidia-driver-daemonset                             0         0         0       0            0           nvidia.com/gpu.deploy.driver=true                  8h\ndaemonset.apps/nvidia-gpu-operator-node-feature-discovery-worker   1         1         1       1            1           &lt;none&gt;                                             8h\ndaemonset.apps/nvidia-mig-manager                                  0         0         0       0            0           nvidia.com/gpu.deploy.mig-manager=true             8h\ndaemonset.apps/nvidia-operator-validator                           1         1         1       1            1           nvidia.com/gpu.deploy.operator-validator=true      8h\n\nNAME                                                                READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/gpu-operator                                        1/1     1            1           8h\ndeployment.apps/nvidia-gpu-operator-node-feature-discovery-master   1/1     1            1           8h\n\nNAME                                                                           DESIRED   CURRENT   READY   AGE\nreplicaset.apps/gpu-operator-5f9dfcb867                                        1         1         1       8h\nreplicaset.apps/nvidia-gpu-operator-node-feature-discovery-master-6fb7d94695   1         1         1       8h\n</code></pre></p>"},{"location":"addons/gpu-operator/#testing","title":"Testing","text":""},{"location":"addons/grafana-operator/","title":"Grafana-Operator Add-on","text":"<p>The grafana-operator is a Kubernetes operator built to help you manage your Grafana instances inside and outside Kubernetes. Grafana Operator makes it possible for you to manage and create Grafana dashboards, datasources etc. declaratively between multiple instances in an easy and scalable way. Using grafana-operator, it will be possible to add AWS data sources such as Amazon Managed Service for Prometheus, Amazon CloudWatch, AWS X-Ray to Amazon Managed Grafana and create Grafana dashboards on Amazon Managed Grafana from your Amazon EKS cluster. This enables customers to use our Kubernetes cluster to create and manage the lifecyle of resources in Amazon Managed Grafana in a Kubernetes native way. This ultimately enables us to use GitOps mechanisms using CNCF projects such as FluxCD to create and manage the lifecyle of resources in Amazon Managed Grafana.</p>"},{"location":"addons/grafana-operator/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.GrafanaOperatorAddon(),\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/grafana-operator/#configuration-options","title":"Configuration Options","text":"<ul> <li> <p><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you.</p> </li> <li> <p><code>values</code>: Arbitrary values to pass to the chart. Refer to the Grafana Operator Helm Chart documentation for additional details. It also supports all standard helm configuration options.</p> </li> </ul>"},{"location":"addons/grafana-operator/#validation","title":"Validation","text":"<p>To validate that Grafana Operator is installed properly in the cluster, check if the namespace is created and pods are running.</p> <p>Verify if the namespace is created correctly <pre><code>kubectl get ns | grep \"grafana-operator\"\n</code></pre> There should be list the grafana-operator namespace <pre><code>grafana-operator      Active   31m\n</code></pre> Verify if everything is running correctly in the grafana-operator namespace <pre><code>kubectl get all -n grafana-operator  \n</code></pre> This should list 1 pod, 1 service, 1 deployment, and 1 replica-set starting with name grafana-operator  For Eg: <pre><code>NAME                                                READY   STATUS    RESTARTS   AGE\npod/grafana-operator-779956546b-q5tlf               2/2     Running   0          3m7s\n\nNAME                                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service          ClusterIP   172.20.255.216   &lt;none&gt;        8443/TCP   3m7s\n\nNAME                                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator               1/1     1            1           3m7s\n\nNAME                                                      DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-779956546b               1         1         1       3m7s\n</code></pre></p>"},{"location":"addons/grafana-operator/#testing","title":"Testing","text":"<p>Please refer to the AWS Blog Using Open Source Grafana Operator on your Kubernetes cluster to manage Amazon Managed Grafana on testing the following features :</p> <ul> <li>Setting up Grafana Identity to Amazon Managed Grafana.</li> <li>Adding AWS data sources such as Amazon Managed Service For Prometheus, Amazon CloudWatch, AWS X-Ray.</li> <li>Creating Grafana Dashboards on Amazon Managed Grafana with Grafana Operator.</li> </ul>"},{"location":"addons/ingress-nginx/","title":"Ingress NGINX Add-on","text":"<p>This add-on installs Ingress Nginx Controller on Amazon EKS. Ingress Nginx controller uses NGINX as a reverse proxy and load balancer.</p> <p>Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path-based routing).</p> <p>IMPORTANT: This add-on depends on AWS Load Balancer Controller Add-on in order to enable NLB support.</p> <p>AWS Load Balancer Controller add-on must be present in add-on array and must be in add-on array before the NGINX ingress controller add-on for it to work, as shown in below example. Otherwise will run into error <code>Assertion failed: Missing a dependency for AwsLoadBalancerControllerAddOn</code>.</p>"},{"location":"addons/ingress-nginx/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst externalDnsHostname = ...;\nconst awsLbControllerAddOn = new blueprints.addons.AwsLoadBalancerControllerAddOn();\nconst IngressNginxAddOn = new blueprints.addons.IngressNginxAddOn({ externalDnsHostname });\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [ awsLbControllerAddOn, IngressNginxAddOn ];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(...addOns)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that the installation is successful, run the following command:</p> <pre><code>$ kubectl get po -n kube-system\nNAME                                                              READY   STATUS    RESTARTS   AGE\nk8s-ingress-ingress-nginx-controller-75886597f6-n9qnn   1/1     Running   0          119m   1/1     Running   0          4d10h\n</code></pre> <p>Note that the ingress controller is deployed in the <code>kube-system</code> namespace.</p> <p>Once deployed, it allows applications to create ingress objects and use host-based routing with external DNS support, if the External DNS Add-on is installed.</p>"},{"location":"addons/ingress-nginx/#configuration","title":"Configuration","text":"<ul> <li> <p><code>backendProtocol</code>: Indication for AWS Load Balancer controller with respect to the protocol supported on the load balancer. TCP by default.</p> </li> <li> <p><code>crossZoneEnabled</code>: Whether to create a cross-zone load balancer with the service that backs NGINX.</p> </li> <li> <p><code>internetFacing</code>: Whether the created load balancer is internet-facing. Defaults to <code>true</code> if not specified. An internal load balancer is provisioned if set to <code>false</code>.</p> </li> <li> <p><code>targetType:</code>ip<code>or</code>instance mode<code>. Defaults to</code>ip`, which requires VPC-CNI and has better performance by eliminating a hop through kube-proxy. Instance mode leverages traditional NodePort mode on the instances.</p> </li> <li> <p><code>externaDnsHostname</code>: Used in conjunction with the external DNS add-on to handle automatic registration of the service with Route 53.</p> </li> <li> <p><code>ingressClassName</code>: Class of the ingress controller (default: nginx).</p> </li> <li> <p><code>controllerClass</code>: Class used for handling ingress in a cluster.</p> </li> <li> <p><code>electionId</code>: Identifier for leader election.</p> </li> <li> <p><code>isDefaultClass</code>: Sets the ingress controller as the default for handling ingress resources (default: false).</p> </li> <li> <p><code>certificateResourceName</code>: Name of the certificate resource provider.</p> </li> <li> <p><code>certificateResourceARN</code>: ARN of the AWS Certificate Manager certificate for HTTPS.</p> </li> <li> <p><code>sslPort</code>: Protocol for the load balancer SSL port (default: https).</p> </li> <li> <p><code>httpTargetPort</code>: Protocol for the load balancer HTTP target port (default: http).</p> </li> <li> <p><code>httpsTargetPort</code>: Protocol for the load balancer HTTPS target port (default: https).</p> </li> <li> <p><code>forceSSLRedirect</code>: Forces SSL redirection (default: true).</p> </li> <li> <p><code>loadBalancerType</code>: Type of the load balancer (default: external).</p> </li> <li> <p><code>idleTimeout</code>: Idle timeout for the load balancer (default: 3600).</p> </li> <li> <p><code>values</code>: Arbitrary values to pass to the chart as per https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/</p> </li> </ul>"},{"location":"addons/ingress-nginx/#dns-integration-and-routing","title":"DNS Integration and Routing","text":"<p>If the External DNS Add-on is installed, it is possible to configure the Ingress Nginx with an external NLB load balancer and leverage wild-card DNS domains (and public certificate) to route external traffic to individual workloads.</p> <p>The following example provides support for AWS Load Balancer Controller, External DNS, and Ingress Nginx add-ons to enable such routing:</p> <pre><code>blueprints.EksBlueprint.builder()\n    // Register hosted zone1 under the name of MyHostedZone1\n    .resourceProvider(\"MyHostedZone1\",  new blueprints.DelegatingHostedZoneProvider({\n        parentDomain: 'myglobal-domain.com',\n        subdomain: 'dev.myglobal-domain.com', \n        parentAccountId: parentDnsAccountId,\n        delegatingRoleName: 'DomainOperatorRole',\n        wildcardSubdomain: true\n    }))\n    .addOns(new blueprints.addons.ExternalDnsAddOn({\n        hostedZoneProviders: [\"MyHostedZone1\"]\n    }))\n    .addOns(new blueprints.IngressNginxAddOn({ \n        internetFacing: true, \n        backendProtocol: \"tcp\", \n        externalDnsHostname: subdomain, \n        crossZoneEnabled: false \n    }))\n    .version(\"auto\")\n    .build(...);\n</code></pre> <p>Assuming the subdomain in the above example is <code>dev.my-domain.com</code> and wildcard is enabled for the external DNS add-on, customers can now create ingress objects for host-based routing. Let's define an ingress object for <code>team-riker</code> that is currently deploying the guestbook application with no ingress:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  name: ingress-riker\n  namespace: team-riker\nspec:\n  rules:\n  - host: riker.dev.my-domain.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: guestbook-ui\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\n</code></pre> <p>A similar ingress may be defined for <code>team-troi</code>, routing to the workloads deployed by that team:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  name: ingress-troi\n  namespace: team-troi\nspec:\n  rules:\n  - host: troi.dev.my-domain.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: guestbook-ui\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\n</code></pre> <p>After the above ingresses are applied (ideally through a GitOps engine), you can now navigate to the specified hosts respectively:</p> <p><code>http://riker.dev.my-domain.com</code> <code>http://troi.dev.my-domain.com</code></p>"},{"location":"addons/ingress-nginx/#tls-termination-and-certificates","title":"TLS Termination and Certificates","text":"<p>You can configure the Ingress Nginx add-on to terminate TLS at the load balancer and supply an ACM certificate through the platform blueprint.</p> <p>A certificate can be registered using a named resource provider.</p> <p>For convenience, the framework provides a couple of common certificate providers:</p> <p>Import Certificate</p> <p>This case is used when a certificate is already created and you just need to reference it with the blueprint stack:</p> <pre><code>const myCertArn = \"\";\nblueprints.EksBlueprint.builder()\n    .resourceProvider(GlobalResources.Certificate, new ImportCertificateProvider(myCertArn, \"cert1-id\"))\n    .addOns(new IngressNginxAddOn({\n        certificateResourceName: GlobalResources.Certificate,\n        externalDnsHostname: 'my.domain.com'\n    }))\n    .teams(...)\n    .version(\"auto\")\n    .build(app, 'stack-with-cert-provider');\n</code></pre> <p>Create Certificate</p> <p>This approach is used when a certificate should be created with the blueprint stack. In this case, the new certificate requires DNS validation, which can be accomplished automatically if the corresponding Route 53 hosted zone is provisioned (either along with the stack or separately) and registered as a resource provider.</p> <pre><code>blueprints.EksBlueprint.builder()\n    .resourceProvider(GlobalResources.HostedZone ,new ImportHostedZoneProvider('hosted-zone-id1', 'my.domain.com'))\n    .resourceProvider(GlobalResources.Certificate, new CreateCertificateProvider('domain-wildcard-cert', '*.my.domain.com', GlobalResources.HostedZone)) // referencing hosted zone for automatic DNS validation\n    .addOns(new AwsLoadBalancerControllerAddOn())\n    // Use hosted zone for External DNS\n    .addOns(new ExternalDnsAddOn({ hostedZoneResources: [GlobalResources.HostedZone] }))\n    // Use certificate registered before with IngressNginxAddOn\n    .addOns(new IngressNginxAddOn({\n        certificateResourceName: GlobalResources.Certificate,\n        externalDnsHostname: 'my.domain.com'\n    }))\n    .teams(...)\n    .version(\"auto\")\n    .build(app, 'stack-with-resource-providers');\n</code></pre>"},{"location":"addons/ingress-nginx/#managing-multiple-ingress-controllers-with-ingressclasses","title":"Managing Multiple Ingress Controllers with IngressClasses","text":"<p>The IngressNginxAddOn leverages the Ingress Nginx Controller, which supports using IngressClasses to avoid conflicts. Here's how you can set up and use IngressClasses to manage multiple Ingress controllers effectively.</p> <p>Using IngressClasses with IngressNginxAddOn To deploy multiple instances of the Ingress Nginx controller, grant them control over different IngressClasses and select the appropriate IngressClass using the ingressClassName field in your Ingress resources. The IngressNginxAddOn simplifies this setup by allowing you to define these parameters directly.</p>"},{"location":"addons/ingress-nginx/#add-on-configuration-example","title":"Add-on Configuration Example**","text":"<pre><code>const IngressNginxAddOn = new blueprints.addons.IngressNginxAddOn({\n    crossZoneEnabled: true,\n    internetFacing: true,\n    targetType: 'ip',\n    externalDnsHostname: myDomainName,\n    certificateResourceName: blueprints.GlobalResources.Certificate,\n    ingressClassName: 'internal-nginx',\n    controllerClass: 'k8s.io/internal-ingress-nginx',\n    electionId: 'ingress-controller-leader-internal'\n});\n</code></pre> <p>Helm Chart Values The add-on configuration sets up the necessary values for the Helm chart:</p> <pre><code>const values: Values = {\n    controller: {\n        service: {\n            annotations: presetAnnotations\n        },\n        ingressClassResource: {\n            name: props.ingressClassName || \"nginx\",\n            enabled: true,\n            default: props.isDefaultClass ?? false,\n            controllerValue: props.controllerClass || \"k8s.io/ingress-nginx\"\n        },\n        electionID: props.electionId || \"ingress-controller-leader\"\n    }\n};\n</code></pre>"},{"location":"addons/ingress-nginx/#benefits","title":"Benefits","text":"<ul> <li>Service Annotations: Customize the Kubernetes Service resource exposing the Ingress Nginx controller for better control over AWS integrations.</li> <li>Ingress Class Resource: Manage multiple Ingress configurations by specifying different ingress classes, ensuring proper routing and avoiding conflicts.</li> <li>Election ID: Ensure high availability and reliability by using a unique election ID for each controller instance, avoiding conflicts between multiple instances.</li> </ul>"},{"location":"addons/ingress-nginx/#differences-between-ingress-nginx-controller-and-nginx-inc-ingress-controller","title":"Differences between Ingress Nginx Controller and NGINX Inc. Ingress Controller","text":"<p>The Ingress-Nginx Controller and the NGINX Inc(F5). Ingress Controller both use NGINX, but they have different implementations and configurations:</p> <ol> <li>Repository Source:</li> </ol> <p>Ingress Nginx: Available at kubernetes/ingress-nginx.</p> <p>NGINX Inc.(F5): Available at nginxinc/kubernetes-ingress.</p> <ol> <li>Configuration and Features:</li> </ol> <p>Ingress Nginx: Maintained by: Kubernetes community with extensive community support and documentation.</p> <p>NGINX Inc.(F5): Maintained by: NGINX Inc. (part of F5 Networks), potentially with enterprise features and different configurations.</p> <ol> <li>Annotations and Settings:</li> </ol> <p>Ingress Nginx: May have different annotations and settings specific to Kubernetes community practices.</p> <p>NGINX Inc.(F5): May offer additional enterprise-grade features and require different annotations.</p> <ol> <li>Support and Updates:</li> </ol> <p>Ingress Nginx: Community-supported with regular updates and improvements based on community contributions.</p> <p>NGINX Inc.(F5): Commercial support available from NGINX Inc, with a focus on security and enterprise features.</p>"},{"location":"addons/ingress-nginx/#functionality","title":"Functionality","text":"<ol> <li>Installs Ingress Nginx controller</li> <li>Provides convenience options to integrate with AWS Load Balancer Controller to leverage NLB for the load balancer</li> <li>Provides convenience options to integrate with External DNS add-on for integration with Amazon Route 53</li> <li>Allows configuring TLS termination at the load balancer provisioned with the add-on</li> <li>Supports standard helm configuration options</li> </ol>"},{"location":"addons/instana-addon/","title":"IBM\u00ae Instana\u00ae Addon for Amazon EKS Blueprints","text":"<p>The IBM\u00ae Instana\u00ae Addon for Amazon EKS Blueprints is designed to enhance observability, monitoring, and management capabilities for applications running on Amazon Elastic Kubernetes Service (EKS). IBM Instana collects data from monitored systems by using a single agent on each host. The agent runs on your hosts to collect and aggregate data from various sensors before it sends the data to the Instana backend.</p> <p>The IBM\u00ae Instana\u00ae Addon focuses on enhancing the user experience by reducing the complexity and time required to install and configure an Instana host agent on Amazon EKS cluster. Once you configure the addon for a Amazon EKS blueprints, it will be automatically provisioned during deployment.</p> <p>This Addon will use IBM Instana Kubernetes operator in the namespace <code>instana-agent</code> to install and manage the Instana agent. It also configures custom resource values to configure the operator.</p>"},{"location":"addons/instana-addon/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>Instana backend application - Use SaaS (eg aws) or Install self-hosted Instana backend (on-premises)</li> </ol>"},{"location":"addons/instana-addon/#installation","title":"Installation","text":"<p>To create a new project and install dependencies, follow these steps from Amazon EKS Blueprints Quick Start</p> <p>Use following command to Install IBM Instana Addon:</p> <pre><code>npm i @instana/aws-eks-blueprint-addon\n</code></pre>"},{"location":"addons/instana-addon/#instana-agent-configuration","title":"Instana Agent Configuration","text":"<p>Go to your Instana installation (Instana User Interface), click ... More &gt; Agents &gt; Installing Instana Agents and select 'Kubernetes' platform to get the Instana Agent Key, Instana Service Endpoint, Instana Service port. These steps are also described here or in the screenshot below.</p> <p></p>"},{"location":"addons/instana-addon/#aws-secret-manager-secrets-optional","title":"AWS Secret Manager Secrets (Optional)","text":"<p>If you wish to use AWS Secret Manager Secrets to pass Instana props (key, endpoint, and port), then you will be required to setup Secrets first.</p> <p><pre><code>export SECRET_NAME=&lt;aws_secret_name&gt;\nexport INSTANA_AGENT_KEY=&lt;instana_key&gt;\nexport INSTANA_ENDPOINT_HOST_URL=&lt;instana_host_endpoint&gt;\nexport INSTANA_ENDPOINT_HOST_PORT=&lt;instana_port&gt;\"\naws secretsmanager create-secret \\\n  --name $SECRET_NAME \\\n  --secret-string \"{\\\"INSTANA_AGENT_KEY\\\":\\\"${INSTANA_AGENT_KEY}\\\",\n    \\\"INSTANA_ENDPOINT_HOST_URL\\\":\\\"${INSTANA_ENDPOINT_HOST_URL}\\\",\n    \\\"INSTANA_ENDPOINT_HOST_PORT\\\":\\\"${INSTANA_ENDPOINT_HOST_PORT}\\\"\n   }\"\n</code></pre> secret_name = AWS Secret Manager Secret name (eg. instana-secret-params).</p>"},{"location":"addons/instana-addon/#usage-using-aws-secret-manager-secrets","title":"Usage : Using AWS Secret Manager Secrets","text":"<p>To use AWS Secret Manager Secrets follow these steps:</p> <ol> <li> <p>The actual settings for the secret name (<code>secretParamName</code>) are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory  <code>json \"context\": {      \"secretParamName\": \"instana-secret-params\"  }</code></p> </li> <li> <p>Go to project/bin/.ts     <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { loadYaml } from '@aws-quickstart/eks-blueprints/dist/utils';\nimport { InstanaOperatorAddon } from '@instana/aws-eks-blueprint-addon';\n\nconst app = new cdk.App();\nconst instanaSecretName = app.node.tryGetContext('&lt;instana-secret-param&gt;');\n\nexport const instanaProps = {\n    // In this example, the secret is called instana-secret-param\n    secretParamName: instanaSecretName\n};\n\nconst yamlObject = loadYaml(JSON.stringify(instanaProps));\n\n// AddOns for the cluster.\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new InstanaOperatorAddon(yamlObject)\n];\n\nconst account = '&lt;aws account id&gt;';\nconst region = '&lt;aws region&gt;';\n\nblueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .version(\"auto\")\n    .addOns(...addOns)\n    .useDefaultSecretEncryption(true)\n    .build(app, '&lt;eks cluster name&gt;');\n</code></pre>"},{"location":"addons/instana-addon/#usage-using-secrets-in-the-code","title":"Usage : Using Secrets in the Code","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { loadYaml } from '@aws-quickstart/eks-blueprints/dist/utils';\nimport { InstanaOperatorAddon } from '@instana/aws-eks-blueprint-addon';\n\nconst app = new cdk.App();\n\nexport const instanaProps = {\n    agent: {\n        key: \"&lt;instana agent key&gt;\", // Mandatory Parameter\n        endpointHost: \"&lt;instana backend host&gt;\",// Mandatory Parameter\n        endpointPort: \"&lt;instana backend port&gt;\",// Mandatory Parameter\n    }\n};\n\nconst yamlObject = loadYaml(JSON.stringify(instanaProps));\n\n// AddOns for the cluster.\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new InstanaOperatorAddon(yamlObject)\n];\n\nconst account = '&lt;aws account id&gt;';\nconst region = '&lt;aws region&gt;';\n\nblueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .version(\"auto\")\n    .addOns(...addOns)\n    .useDefaultSecretEncryption(true)\n    .build(app, '&lt;eks cluster name&gt;');\n</code></pre>"},{"location":"addons/instana-addon/#addon-configuration-options","title":"AddOn Configuration Options","text":"Option Description Default <code>agent.endpointHost</code> Instana Agent backend endpoint host https://ingress-red-saas.instana.io/ (US and ROW) <code>agent.endpointPort</code> Instana Agent backend endpoint port \"443\" <code>agent.key</code> Your Instana Agent key nil <code>agent.downloadKey</code> Your Instana Download key nil <code>agent.env</code> Additional environment variables for the agent {} <code>agent.configuration_yaml</code> Custom content for the agent configuration.yaml file nil <code>cluster.name</code> Display name of the monitored cluster \"Value of zone.name\" <code>zone.name</code> Zone that detected technologies will be assigned to nil"},{"location":"addons/instana-addon/#bootstraping","title":"Bootstraping","text":"<p>Bootstrap your environment with the following command.</p> <pre><code>cdk bootstrap\n</code></pre> <p>and finally you can deploy the stack with the following command. <pre><code>cdk deploy\n</code></pre></p>"},{"location":"addons/instana-addon/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found here.</p> <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/eks-blue1-eksblue1AccessRole32C5DF05-1NBFCH8INI08A\n</code></pre>"},{"location":"addons/instana-addon/#testing","title":"Testing","text":"<p>To validate if Instana Agent configured properly in Amazon EKS. You can run the following command after Amazon EKS cluster in deployed and running. <pre><code>kubectl get pods -n instana-agent\n</code></pre> Output of the above command will be silimar to below one:</p> <pre><code>NAMESPACE       NAME                                                READY       STATUS    RESTARTS   AGE\ninstana-agent   controller-manager-78479cb596-sktg9     1/1         Running                     0          56m\ninstana-agent   controller-manager-78479cb596-xz8kn     1/1         Running                     0          56m\ninstana-agent   instana-agent-gsqx8                                 1/1         Running                     0          56m\n</code></pre> <p>Run following command to verify Instana Agent logs <pre><code>kubectl logs &lt;instana-agent-pod-name&gt; -n instana-agent # Output shows instana agent logs. pod name in this example is instana-agent-gsqx8\n</code></pre></p> <p>Once you see Instana Agent is running in your Amazon EKS Cluster, you can go to Instana Installation (User Interface) to get the APM metrices.</p> <p></p>"},{"location":"addons/instana-addon/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>cdk destroy\n</code></pre>"},{"location":"addons/instana-addon/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open source NPM package aws-eks-blueprint-addon. Please refer to the package npm site for more information. <pre><code>https://www.npmjs.com/package/@instana/aws-eks-blueprint-addon'\n</code></pre></p>"},{"location":"addons/istio-base/","title":"Istio Base Add-on","text":"<p>The Base add-on adds support for Istio base chart which contains cluster-wide resources and CRDs used by the Istio control plane to an EKS cluster.</p>"},{"location":"addons/istio-base/#usage","title":"Usage","text":"<p>Add the following as an add-on to your main.ts file to add Istio Base to your cluster</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.IstioBaseAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that installation is successful run the following command:</p> <pre><code>$ kubectl get crd -n istio-system\nNAME                                         CREATED AT\nauthorizationpolicies.security.istio.io      2022-05-05T02:16:23Z\ndestinationrules.networking.istio.io         2022-05-05T02:16:23Z\neniconfigs.crd.k8s.amazonaws.com             2022-05-05T02:04:10Z\nenvoyfilters.networking.istio.io             2022-05-05T02:16:23Z\ngateways.networking.istio.io                 2022-05-05T02:16:23Z\nistiooperators.install.istio.io              2022-05-05T02:16:23Z\npeerauthentications.security.istio.io        2022-05-05T02:16:23Z\nproxyconfigs.networking.istio.io             2022-05-05T02:16:23Z\nrequestauthentications.security.istio.io     2022-05-05T02:16:23Z\nsecuritygrouppolicies.vpcresources.k8s.aws   2022-05-05T02:04:12Z\nserviceentries.networking.istio.io           2022-05-05T02:16:23Z\nsidecars.networking.istio.io                 2022-05-05T02:16:23Z\ntelemetries.telemetry.istio.io               2022-05-05T02:16:23Z\nvirtualservices.networking.istio.io          2022-05-05T02:16:23Z\nwasmplugins.extensions.istio.io              2022-05-05T02:16:23Z\nworkloadentries.networking.istio.io          2022-05-05T02:16:23Z\nworkloadgroups.networking.istio.io           2022-05-05T02:16:23Z\n</code></pre> <p>Note that the all the CRDs are created in provided namespace (istio-system by default).</p> <p>Once deployed, it allows the control plane to be installed.</p>"},{"location":"addons/istio-base/#configuration","title":"Configuration","text":"<ul> <li><code>enableAnalysis</code>: Enable istioctl analysis which provides rich analysis of Istio configuration state in order to identity invalid or suboptimal configurations. Defaults to <code>false</code> if not specified.</li> <li><code>configValidation</code>: Enable the istio base config validation. Defaults to <code>true</code> if not specified.</li> <li><code>externalIstiod</code>: If this is set to true, one Istiod will control remote clusters including CA. Defaults to <code>false</code> if not specified.</li> <li><code>remotePilotAddress</code>: The address or hostname of the remote pilot.</li> <li><code>validationURL</code>: Validation webhook configuration url. For example: <code>https://$remotePilotAddress:15017/validate</code>.</li> <li><code>enableIstioConfigCRDs</code>: For istioctl usage to disable istio config crds in base. Defaults to <code>true</code> if not specified.</li> </ul>"},{"location":"addons/istio-base/#functionality","title":"Functionality","text":"<ol> <li>Installs cluster-wide resources and CRDs used by the Istio control plane</li> <li>Provides convenience options for istioctl commands</li> <li>Works as the basis for the Istio Control Plane AddOn</li> </ol>"},{"location":"addons/istio-cni/","title":"Istio Cni Add-on","text":"<p>Istio is an open platform for providing a uniform way to integrate microservices, manage traffic flow across microservices, enforce policies and aggregate telemetry data. The Istio CNI plugin performs the Istio mesh pod traffic redirection in the Kubernetes pod lifecycle\u2019s network setup phase, thereby removing the requirement for the NET_ADMIN and NET_RAW capabilities for users deploying pods into the Istio mesh. This addo-on does not replace the <code>VpcCniAddOn</code> which helps with native VPC networking with the Amazon VPC Container Network Interface (CNI).</p> <p>IMPORTANT:</p> <ol> <li>This add-on depends on Istio Base and istio Control Plane Add-ons for cluster-wide resources and CRDs.</li> </ol> <p>Istio Base add-on and Istio Control Plane addon-on must be present in add-on array and must be in add-on array before the Istio Cni add-on for it to work, as shown in below example. Otherwise will run into error <code>Assertion failed: Missing a dependency for IstioBaseAddOn</code>.</p>"},{"location":"addons/istio-cni/#usage","title":"Usage","text":"<p>Add the following as an add-on to your main.ts file to add Istio Control Plane to your cluster</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst istioBase = new blueprints.addons.IstioBaseAddOn();\nconst istioControlPlane = new blueprints.addons.IstioControlPlaneAddOn()\nconst istioCni = new blueprints.addons.IstioCniAddOn()\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [ istioBase, istioControlPlane, istioCni ];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(...addOns)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/istio-cni/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you</li> <li><code>version</code>: Version fo the Helm Chart to be used to install</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to Helm Chart documentation for additional details</li> <li><code>values.cni.chained</code> is enabled by default which allows Istio CNI plugin to operate as a chained CNI plugin, and it is designed to be used with another CNI pluginenables. In-case <code>values.cni.chained</code> is disabled, <code>VpcCniAddOn</code> addon-on should be removed manually before disabling this option.</li> </ul>"},{"location":"addons/istio-cni/#validation","title":"Validation","text":"<p>To validate that installation is successful run the following command:</p> <pre><code>$ kubectl get all -n istio-system\nNAME                                  READY   STATUS    RESTARTS   AGE\npod/istio-cni-node-6w5fb              1/1     Running   0          4m25s\npod/istio-cni-node-nbwbn              1/1     Running   0          4m25s\npod/istiod-6c7b79d8cc-mwk4c           1/1     Running   0          4m43s\n\nNAME                     TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)                                      AGE\nservice/istiod           ClusterIP      172.20.237.63    &lt;none&gt;                                                                   15010/TCP,15012/TCP,443/TCP,15014/TCP        4m43s\n\nNAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/istio-cni-node   2         2         2       2            2           kubernetes.io/os=linux   4m25s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/istiod           1/1     1            1           4m43s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/istiod-6c7b79d8cc           1         1         1       4m43s\n\nNAME                                                 REFERENCE                   TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/istiod           Deployment/istiod           0%/80%    1         5         1          4m43s\n</code></pre>"},{"location":"addons/istio-cni/#functionality","title":"Functionality","text":"<ol> <li>Installs Istio CNI plugin performs the Istio mesh pod traffic redirection in the Kubernetes pod lifecycle\u2019s network setup.</li> </ol>"},{"location":"addons/istio-control-plane/","title":"Istio Control Plane Add-on","text":"<p>Istio is an open platform for providing a uniform way to integrate microservices, manage traffic flow across microservices, enforce policies and aggregate telemetry data. Istio's control plane provides an abstraction layer over the underlying cluster management platform, such as Kubernetes.</p> <p>IMPORTANT:</p> <ol> <li>This add-on depends on Istio Base Add-on for cluster-wide resources and CRDs.</li> </ol> <p>Istio Base add-on must be present in add-on array and must be in add-on array before the Istio Control Plane add-on for it to work, as shown in below example. Otherwise will run into error <code>Assertion failed: Missing a dependency for IstioBaseAddOn</code>.</p>"},{"location":"addons/istio-control-plane/#usage","title":"Usage","text":"<p>Add the following as an add-on to your main.ts file to add Istio Control Plane to your cluster</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst istioBase = new blueprints.addons.IstioBaseAddOn();\nconst istioControlPlane = new blueprints.addons.IstioControlPlaneAddOn()\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [ istioBase, istioControlPlane ];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(...addOns)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that installation is successful run the following command:</p> <pre><code>$ kubectl get po -n istio-system\nNAME                      READY   STATUS    RESTARTS   AGE\nistiod-5797797b4b-fjrq2   1/1     Running   0          28m\n</code></pre>"},{"location":"addons/istio-control-plane/#configuration","title":"Configuration","text":"<ul> <li><code>values</code>: Arbitrary values to pass to the chart as per https://istio.io/v1.4/docs/reference/config/installation-options/</li> </ul> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@edgarsilva948/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst istioControlPlaneAddOnProps = {\n  values: {\n    pilot: {\n      autoscaleEnabled: true,\n      autoscaleMin: 1,\n      autoscaleMax: 5,\n      replicaCount: 1,\n      rollingMaxSurge: \"100%\",\n      rollingMaxUnavailable: \"25%\",\n      resources: {\n        requests: {\n          cpu: \"500m\",\n          memory: \"2048Mi\",\n        }\n      }\n    }\n  }\n}\n\nconst istioBase = new blueprints.addons.IstioBaseAddOn();\nconst istioControlPlane = new blueprints.addons.IstioControlPlaneAddOn(IstioControlPlaneAddOnProps)\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [ istioBase, istioControlPlane ];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(...addOns)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/istio-control-plane/#functionality","title":"Functionality","text":"<ol> <li>Installs Istio Control Plane deployment</li> </ol>"},{"location":"addons/istio-ingress-gateway/","title":"Istio Ingress Gateway Add-on","text":"<p>Istio is an open platform for providing a uniform way to integrate microservices, manage traffic flow across microservices, enforce policies and aggregate telemetry data. An Ingress gateway is a load balancer that handles incoming HTTP and HTTPS traffic to the mesh. It can be used to expose services to the internet, or to enable communication between services within the mesh. Istio Ingress Gateway Add-on installs Istio Ingress Gateway implementing a Kubernetes gateway resource and a set of Envoy proxy instances.</p> <p>IMPORTANT:</p> <ol> <li>This add-on depends on Istio Base and istio Control Plane Add-ons for cluster-wide resources and CRDs.</li> </ol> <p>Istio Base add-on and Istio Control Plane addon-on must be present in add-on array and must be in add-on array before the Istio Ingress Gateway add-on for it to work, as shown in below example. Otherwise will run into error <code>Assertion failed: Missing a dependency for IstioBaseAddOn</code>.</p>"},{"location":"addons/istio-ingress-gateway/#usage","title":"Usage","text":"<p>Add the following as an add-on to your main.ts file to add Istio Control Plane to your cluster</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst istioBase = new blueprints.addons.IstioBaseAddOn();\nconst istioControlPlane = new blueprints.addons.IstioControlPlaneAddOn()\nconst istioIngressGateway = new blueprints.addons.IstioIngressGatewayAddOn()\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [ istioBase, istioControlPlane, istioIngressGateway ];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(...addOns)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/istio-ingress-gateway/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you</li> <li><code>version</code>: Version fo the Helm Chart to be used to install</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to Helm Chart documentation for additional details</li> </ul>"},{"location":"addons/istio-ingress-gateway/#validation","title":"Validation","text":"<p>To validate that installation is successful run the following command:</p> <pre><code>$ kubectl get all -n istio-system\nNAME                                  READY   STATUS    RESTARTS   AGE\npod/ingressgateway-686c75b54c-qgmd4   1/1     Running   0          4m25s\npod/istiod-6c7b79d8cc-mwk4c           1/1     Running   0          4m43s\n\nNAME                     TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)                                      AGE\nservice/ingressgateway   LoadBalancer   172.20.141.148   a2b87c2b0a6d64bfe9e99b29308ae0ad-449071982.us-east-1.elb.amazonaws.com   15021:30586/TCP,80:32662/TCP,443:30891/TCP   4m25s\nservice/istiod           ClusterIP      172.20.237.63    &lt;none&gt;                                                                   15010/TCP,15012/TCP,443/TCP,15014/TCP        4m43s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ingressgateway   1/1     1            1           4m25s\ndeployment.apps/istiod           1/1     1            1           4m43s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ingressgateway-686c75b54c   1         1         1       4m25s\nreplicaset.apps/istiod-6c7b79d8cc           1         1         1       4m43s\n\nNAME                                                 REFERENCE                   TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/ingressgateway   Deployment/ingressgateway   2%/80%    1         5         1          4m25s\nhorizontalpodautoscaler.autoscaling/istiod           Deployment/istiod           0%/80%    1         5         1          4m43s\n</code></pre>"},{"location":"addons/istio-ingress-gateway/#functionality","title":"Functionality","text":"<ol> <li>Istio Ingress Gateway Add-on installs Istio Ingress Gateway implementing a Kubernetes gateway resource and a set of Envoy proxy instances.</li> </ol>"},{"location":"addons/jupyterhub/","title":"JupyterHub Add-on","text":"<p>\u26a0WARNING: This add-on is currently experience deployment issues due to the x509 certificate update. At the moment the deployment fails as is, however customer can still continue to use the add-on if they are willing to temporarily migrate the helm chart for version 2.0.0 their private helm (or OCI) registry and override the repository attribute of the add-on when deploying. </p> <p>JupyterHub add-on is based on the JupyterHub project that supports a multi-user Hub to spawn, manage, and proxy multiple instances of single user Jupyter notebook server.</p> <p>The Hub can offer notebook servers to a class of students, a corporate data science workgroup, a scientific research project, or a high-performance computing group.</p> <p>For more information regarding a Jupyter notebook, please consult the official documentation.</p> <p>IMPORTANT: This add-on depends on EBS CSI Driver Add-on for using EBS as persistent storage.</p> <p>EBS CSI Driver or EFS CSI Driver add-on must be present in add-on array and must be in add-on array before the Jupyter add-on for it to work, as shown in below example (with EBS). Otherwise will run into error <code>Assertion failed: Missing a dependency for &lt;EbsCsiDriverAddOn or EfsCsiDriverAddOn&gt;</code>.</p>"},{"location":"addons/jupyterhub/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst subdomain: string = utils.valueFromContext(scope, \"dev.subzone.name\", \"jupyterhub.some.example.com\");\nconst parentDnsAccountId = scope.node.tryGetContext(\"parent.dns.account\")!;\nconst parentDomain = utils.valueFromContext(scope, \"parent.hostedzone.name\", \"some.example.com\");\n\nconst jupyterHubAddOn = new blueprints.addons.JupyterHubAddOn({\n  efsConfig:{\n    removalPolicy: cdk.RemovalPolicy.DESTROY,\n    pvcName: \"efs-persist\",\n    capacity: \"120Gi\",\n  },\n  oidcConfig?: {\n    callbackUrl: \"&lt;Callback URL&gt;\",\n    authUrl: \"&lt;Authorization URL&gt;\",\n    tokenUrl: \"&lt;Token URL&gt;\",\n    userDataUrl: \"&lt;User Data URL&gt;\",\n    clientId: \"&lt;Client ID&gt;\",\n    clientSecret: \"&lt;Client Secret&gt;\",\n    scope: [], //list of OIDC provider scopes\n    usernameKey: \"&lt;username key&gt;\",\n  },\n  serviceType: blueprints.JupyterHubServiceType.ALB,\n  ingressHosts: [jupyterDNSname],\n  ingressAnnotations: {\n    'external-dns.alpha.kubernetes.io/hostname': `${jupyterDNSname}`,\n  },\n  notebookStack: 'jupyter/datascience-notebook',\n  certificateResourceName?: 'your-certificate',\n});\nconst awsAlbAddOn = new blueprints.addons.AwsLoadBalancerControllerAddOn(),\nconst efsCsiAddOn = new blueprints.addons.EfsCsiDriverAddOn();\nconst externalDnsAddOn = new blueprints.addons.ExternalDnsAddOn({\n  hostedZoneResources: [GlobalResources.HostedZone]\n}),\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [ awsAlbAddOn, externalDnsAddOn, efsCsiAddOn, jupyterHubAddOn ];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .resourceProvider(GlobalResources.HostedZone, new DelegatingHostedZoneProvider({\n    parentDomain,\n    subdomain,\n    parentDnsAccountId,\n    delegatingRoleName: 'DomainOperatorRole',\n    wildcardSubdomain: true\n  }))\n  .addOns(...addOns)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that the JupyterHub add-on is running ensure that the add-on deployments for the controller and the webhook are in <code>RUNNING</code> state:</p> <pre><code># Assuming add-on is installed in the jupyterhub.\n$ kubectl get po -n jupyterhub\nNAME                              READY   STATUS    RESTARTS   AGE\ncontinuous-image-puller-2skff     1/1     Running   0          23m\ncontinuous-image-puller-m6s5f     1/1     Running   0          23m\nhub-7dc8888f68-67hnl              1/1     Running   0          23m\njupyter-admin                     1/1     Running   0          16m\nproxy-5df778944c-brrbf            1/1     Running   0          23m\nuser-scheduler-7dbd789bc4-8zrjs   1/1     Running   0          23m\nuser-scheduler-7dbd789bc4-gcb8z   1/1     Running   0          23m\n</code></pre>"},{"location":"addons/jupyterhub/#functionality","title":"Functionality","text":"<ol> <li>Deploys the jupyterhub helm chart in <code>jupyterhub</code> namespace by default.</li> <li>JupyterHub is backed with persistent storage. You must provide one (and only one) of the following configuration (or otherwise will receive an error):</li> <li>Leverage EBS as persistent storage with storage type and capacity provided. If you provide this configuration, EBS CSI Driver add-on must be present in add-on array and must be in add-on array before the JupyterHub add-on for it to work, as shown in above example. Otherwise it will not work.</li> <li>Leverage EFS as persistent storage with the name, capacity and file system removal policy provided. If you provide this configuration, EFS CSI Driver add-on must be present in add-on array and must be in add-on array before the JupyterHub add-on for it to work, as shown in above example. Otherwise it will not work.</li> <li>(Optional) Leverage OIDC Provider as a way to manage authentication and authorization. If not provided, the default creates no user, and the user will be able to login with any arbitrary username and password. It is highly recommended to leverage an Identity provider for any production use case.</li> <li>Exposes the proxy service in three different way based on configuration:</li> <li>Expose using Ingress controller and AWS Application Load Balancer. This requires AWS Load Balancer Controller add-on and it must be in add-on array before the JupyterHub add-on. This will also look for any additional Ingress annotations provided by the user to be tagged.</li> <li>Expose using Loadbalancer Service and AWS Network Load Balancer. This requires AWS Load Balancer Controller add-on and it must be in add-on array before the JupyterHub add-on.</li> <li>Expose using ClusterIP Service. </li> <li>(Optional) Annotates Ingress with user-provided AWS Certificate Manager certificate name. It will be looked up and automatically tagged to be used with Ingress. It will require user to provide a DNS name and External DNS add-on to be added in add-on array before the JupyterHub add-on.</li> <li>(Optional) User can choose a different notebook stack than the standard one provided. Jupyter team maintains a set of Docker image definition in a GitHub repository as explained here.</li> <li>Supports standard helm configuration options.</li> </ol> <p>Note: For custom helm values, please consult the official documentation. </p>"},{"location":"addons/jupyterhub/#using-jupyterhub","title":"Using JupyterHub","text":"<p>JupyterHub, by default, creates a proxy service called <code>proxy-public</code> that will be accessible in different way based on the user configuration setting under <code>serviceType</code>. For example, if you set it as <code>NLB</code>, then it is exposed to a <code>LoadBalancer</code> type Kubernetes service, which will integrate with AWS Network Load Balancer as indicated when running the following command:</p> <pre><code>kubectl get svc -n jupyterhub\nNAME           TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)        AGE\nhub            ClusterIP      172.20.171.28   &lt;none&gt;                                                                    8081/TCP       26m\nproxy-api      ClusterIP      172.20.31.32    &lt;none&gt;                                                                    8001/TCP       26m\nproxy-public   LoadBalancer   172.20.14.210   xxxxxxxx-1234567890.us-west-2.elb.amazonaws.com   80:32733/TCP   26m\n</code></pre> <p>You can log into the JupyterHub portal by accessing the Load Balancer endpoint in any browser. </p> <p></p> <p>A default arbirary username with password can be entered to log in. Once logged in, you should be able to access the main portal page.</p> <p></p> <p>As stated above, it is highly recommended to leverage an Identity provider for any production use case. Please consult the official guide here for various OAuth2 based authentication methods.</p>"},{"location":"addons/karpenter/","title":"Karpenter Add-on","text":"<p>Karpenter add-on is based on the Karpenter open source node provisioning project. For this add-on, it will utilize the AWS provider, to ensure a more efficient and cost-effective way to manage workloads by launching just the right compute resources to handle a cluster's application on your EKS cluster.</p> <p>Karpenter works by:</p> <ul> <li>Watching for pods that the Kubernetes scheduler has marked as unschedulable,</li> <li>Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods,</li> <li>Provisioning nodes that meet the requirements of the pods,</li> <li>Scheduling the pods to run on the new nodes, and</li> <li>Removing the nodes when the nodes are no longer needed</li> </ul>"},{"location":"addons/karpenter/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>There is no support for utilizing both Cluster Autoscaler and Karpenter. Therefore, any addons list that has both will result in an error <code>Deploying &lt;name of your stack&gt; failed due to conflicting add-on: ClusterAutoscalerAddOn.</code>.</p> </li> <li> <p>(If using Spot), EC2 Spot Service Linked Role should be created. See here for more details.</p> </li> <li> <p>Amazon EKS cluster with supported Kubernetes version. Karpenter provides minimum supported Karpenter versions for each Kubernetes version in form of a matrix here.</p> </li> </ol>"},{"location":"addons/karpenter/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { EbsDeviceVolumeType } from 'aws-cdk-lib/aws-ec2';\nimport { KubernetesVersion } from 'aws-cdk-lib/aws-eks';\n\nconst app = new cdk.App();\nconst blueprintID = \"my-stack-name\"\nconst region = \"us-west-2\"\n\nconst karpenterAddOn = new blueprints.addons.KarpenterAddOn({\n    version: 'v0.33.1',\n    nodePoolSpec: {\n      labels: {\n          type: \"karpenter-test\"\n      },\n      annotations: {\n          \"eks-blueprints/owner\": \"platform-team\"\n      },\n      requirements: [\n          { key: 'node.kubernetes.io/instance-type', operator: 'In', values: ['m5.large'] },\n          { key: 'topology.kubernetes.io/zone', operator: 'In', values: [`${region}a`, `${region}b`, `${region}c`] },\n          { key: 'kubernetes.io/arch', operator: 'In', values: ['amd64','arm64']},\n          { key: 'karpenter.sh/capacity-type', operator: 'In', values: ['on-demand']}, // spot is also supported for cost savings, please see #2 above\n      ],\n      disruption: {\n          consolidationPolicy: \"WhenEmpty\",\n          consolidateAfter: \"30s\",\n          expireAfter: \"20m\",\n          // budgets: [{nodes: \"10%\"}] // budgets are supported in versions 0.34+\n      }\n    },\n    ec2NodeClassSpec: {\n      amiFamily: \"AL2\",\n      subnetSelectorTerms: [{ tags: { \"Name\": `${blueprintID}/${blueprintID}-vpc/PrivateSubnet*` } }],\n      securityGroupSelectorTerms: [{ tags: { \"aws:eks:cluster-name\": `${blueprintID}` } }],\n    },\n    interruptionHandling: true,\n    podIdentity: false, // Recommended true if using version 0.35+; otherwise, set false (as default) to use IRSA.\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(KubernetesVersion.V1_28)\n  .addOns(karpenterAddOn)\n  .build(app, blueprintID);\n</code></pre> <p>The add-on automatically sets Helm Chart values, and it is recommended not to pass custom values for the following: - settings.clusterName - settings.interruptionQueue (if interruption handling is enabled) - serviceAccount.create - serviceAccount.name - serviceAccount.annotations.eks.amazonaws.com/role-arn</p> <p>To validate that Karpenter add-on is running ensure that the add-on deployments for the controller and the webhook are in <code>RUNNING</code> state:</p> <pre><code># Assuming add-on is installed in the karpenter namespace.\n$ kubectl get po -n karpenter\nNAME                                          READY   STATUS    RESTARTS   AGE\nblueprints-addon-karpenter-54fd978b89-hclmp   2/2     Running   0          99m\n</code></pre>"},{"location":"addons/karpenter/#functionality","title":"Functionality","text":"<ol> <li>Creates Karpenter Node Role, Karpenter Instance Profile, and Karpenter Controller Policy (Please see Karpenter documentation here for more details on what is required and why).</li> <li>Creates <code>karpenter</code> namespace.</li> <li>Creates Kubernetes Service Account, and manage credentials for karpenter with one of the following methods</li> <li>Associate AWS IAM Role with Karpenter Controller Policy attached using IRSA.</li> <li>Use EKS Pod Identities by creating EKS Pod Identity association maps to the IAM role with the Karpenter Controller Policy attached to a service account in the Karpenter namespace.</li> <li>Deploys Karpenter helm chart in the <code>karpenter</code> namespace, configuring the cluster name, endpoint, Instance Profile, and others necessary for functional addon.</li> <li>If the user provides <code>nodePoolSpec</code> (and <code>ec2NodeClassSpec</code>), the addon will provisions a default Karpenter NodePool and EC2NodeClass CRDs. <code>nodePoolSpec</code> requires requirements while <code>ec2NodeClassSpec</code> requires subnets and security groups. Based on what version of Karpenter you provide, you will need either <code>subnetSelector</code> and <code>securityGroupSelector</code> (for versions v0.31.x or down), or <code>subnetSelectorTerms</code> and <code>securityGroupSelectorTerms</code> (for versions v0.32.x and up).</li> <li>As mentioned above, the CRDs installed will be different from v0.32.0, since Karpenter as a project graduated to beta in October 2023. This meant significant API changes, going from alpha to beta. The addon has reflected those changes and will deploy NodePool and EC2NodeClass for v1beta1 CRDs, versus Provisioner and AWSNodeTemplate for v1alpha5. You can read more about the changes in this blog. This addon can install the new CRDs by setting the <code>installCRDs</code> add-on option to true.</li> </ol> <p>NOTE: EKS Blueprints npm v1.14 and above introduces breaking changes to the addon. Please see Upgrade Path for more details.</p>"},{"location":"addons/karpenter/#using-karpenter","title":"Using Karpenter","text":"<p>To use Karpenter, you need to provision a Karpenter NodePool and EC2NodeClass. NodePool sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. EC2NodeClass, once associated with a NodePool, will then provision those nodes (in the form of EC2 instances) based on the AWS specific settings. Multiple NodePools may point to the same EC2NodeClass.</p> <p>This can be done in 2 ways:</p> <ol> <li> <p>Provide the properties as show in Usage. If the NodePoolSpec is not provided, the addon will not deploy a NodePool or EC2NodeClass.</p> </li> <li> <p>Use <code>kubectl</code> to apply a sample NodePool and EC2NodeClass: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: karpenter.sh/v1beta1\nkind: NodePool\nmetadata:\n  name: default\nspec:\n  template:\n    spec:\n      nodeClassRef:\n        name: default\n---\napiVersion: karpenter.k8s.aws/v1beta1\nkind: EC2NodeClass\nmetadata:\n  name: default\nspec:\n  amiFamily: AL2\n  subnetSelectorTerms:\n    - tags:\n        karpenter.sh/discovery: \"${CLUSTER_NAME}\"\n\n  securityGroupSelectorTerms:\n    # Select on any security group that has both the \"karpenter.sh/discovery: ${CLUSTER_NAME}\" tag\n    # AND the \"environment: test\" tag OR any security group with the \"my-security-group\" name\n    # OR any security group with ID \"sg-063d7acfb4b06c82c\"\n    - tags:\n        karpenter.sh/discovery: \"${CLUSTER_NAME}\"\n\n  role: \"KarpenterNodeRole-${CLUSTER_NAME}\"\n\n  userData: |\n    echo \"Hello world\"\n\n  tags:\n    team: team-a\n    app: team-a-app\n\n  metadataOptions:\n    httpEndpoint: enabled\n    httpProtocolIPv6: disabled\n    httpPutResponseHopLimit: 2\n    httpTokens: required\n\n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 100Gi\n        volumeType: gp3\n        iops: 10000\n        encrypted: true\n        kmsKeyID: \"1234abcd-12ab-34cd-56ef-1234567890ab\"\n        deleteOnTermination: true\n        throughput: 125\n        snapshotID: snap-0123456789\n\n  detailedMonitoring: true\n\nEOF\n</code></pre></p> </li> </ol> <p>If you choose to create NodePool and EC2NodeClass manually, you MUST provide the tags that match the subnet and the security group from the Blueprints EKS cluster that you plan to use.</p>"},{"location":"addons/karpenter/#testing-with-a-sample-deployment","title":"Testing with a sample deployment","text":"<p>Now that the provisioner is deployed, Karpenter is active and ready to provision nodes. Create some pods using a deployment:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inflate\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: inflate\n  template:\n    metadata:\n      labels:\n        app: inflate\n    spec:\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: inflate\n          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2\n          resources:\n            requests:\n              cpu: 1\nEOF\n</code></pre> <p>Now scale the deployment:</p> <pre><code>kubectl scale deployment inflate --replicas 10\n</code></pre> <p>The provisioner will then start deploying more nodes to deploy the scaled replicas. You can verify by either looking at the karpenter controller logs,</p> <pre><code>kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name)\n</code></pre> <p>or, by looking at the nodes being created:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"addons/karpenter/#troubleshooting","title":"Troubleshooting","text":"<p>The following are common troubleshooting issues observed when implementing Karpenter:</p> <ol> <li> <p>For Karpenter version older than <code>0.14.0</code> deployed on Fargate Profiles, <code>values.yaml</code> must be overridden, setting <code>dnsPolicy</code> to <code>Default</code>. Versions after <code>0.14.0</code> has <code>dnsPolicy</code> value set default to <code>Default</code>. This is to ensure CoreDNS is set correctly on Fargate nodes.</p> </li> <li> <p>With the upgrade to the new OCI registry starting with <code>v0.17.0</code>, if you try to upgrade you may get a following error:</p> </li> </ol> <pre><code>Received response status [FAILED] from custom resource. Message returned: Error: b'Error: path \"/tmp/tmpkxgr57q5/blueprints-addon-karpenter\" not found\\n'\n</code></pre> <p>Karpenter, starting from the OCI registry versions, will untar the files under <code>karpenter</code> release name only. So if you have previous version deployed under a different release name, you will run into the above error. Therefore, in order to upgrade, you will have to take the following steps:</p> <ol> <li>Remove the existing add-on.</li> <li>Re-deploy the Karpenter add-on with the release name <code>karpenter</code>.</li> </ol>"},{"location":"addons/karpenter/#upgrade-path","title":"Upgrade Path","text":"<p>The addon introduces breaking changes for Blueprints npm version v0.14 and later. Here are the details:</p> <ul> <li>EKS Blueprints will only support minimum Karpenter version that matches the supporter EKS Kubernetes version. Please see the compatibility matrix here. If you provide incompatible version (i.e. providing version 0.27.x for EKS version 1.27), you will see warnings in the logs but will proceed deployment. You will run into compatibility issues.</li> <li>The add-on will no longer support any versions below v0.21.0</li> <li>User provided properties have been refactored to better reflect the parameters of the various Karpenter resources (i.e. NodePool, EC2NodeClass)</li> <li>For NodePool and EC2NodeClass, the parameters will apply to either the v1alpha5 CRDs ( provisioner, AWSNodeTemplate, for Karpenter versions v0.31.x or earlier) or v1beta1 CRDs (NodePool, EC2NodeClass, for Karpenter versions v0.32.x and later). If you provide non-matching parameters, i.e. providing <code>consolidation</code> instead of <code>disruption</code> for Karpenter version v0.33.1, you will see an error with stack failing to provision. Please consult the upgrade guide to see the changes for various versions.</li> <li>To have the add-on install new CRDs after an initial install, set the <code>installCRDs</code> option to true.</li> </ul> <p>If you are upgrading from earlier version of Blueprints and need to add the Karpenter addon, please ensure the following:</p> <ol> <li> <p>You are using the minimum Karpenter version supported by the Kubernetes version of your blueprint cluster. Not doing so will cause incompatibility issues.</p> </li> <li> <p>Starting v0.32.0, Karpenter introduces the new beta APIs (v1beta1), and therefore the addon will make v1alpha5 CRDs obsolete. Ensure that you are providing the corresponding, matching parameters.</p> </li> </ol>"},{"location":"addons/kasten-k10/","title":"Kasten K10 Add-On for Amazon EKS Blueprints for CDK","text":"<p>Kasten K10 by Veeam Overview</p> <p>The K10 data management platform, purpose-built for Kubernetes, provides enterprise operations teams an easy-to-use, scalable, and secure system for backup/restore, disaster recovery, and mobility of Kubernetes applications.</p>"},{"location":"addons/kasten-k10/#kasten-k10-overview","title":"Kasten-K10 Overview","text":"<p>K10\u2019s application-centric approach and deep integrations with relational and NoSQL databases, Amazon EKS and AWS Services provides teams the freedom of infrastructure choice without sacrificing operational simplicity. Policy-driven and extensible, K10 provides a native Kubernetes API and includes features such full-spectrum consistency, database integrations, automatic application discovery, application mobility, and a powerful web-based user interface.</p> <p>Given K10\u2019s extensive ecosystem support you have the flexibility to choose environments (public/ private/ hybrid cloud/ on-prem) and Kubernetes distributions (cloud vendor managed or self managed) in support of three principal use cases:</p> <ul> <li> <p>Backup and Restore</p> </li> <li> <p>Disaster Recovery</p> </li> <li> <p>Application Mobility</p> </li> </ul>"},{"location":"addons/kasten-k10/#kasten-k10-use-cases","title":"Kasten-K10 Use Cases","text":"<p>The Kasten K10 add-on installs Kasten K10 into your Amazon EKS cluster. </p>"},{"location":"addons/kasten-k10/#architecture","title":"Architecture","text":"<p>Deploying this Quickstart for a new virtual private cloud (VPC) with default parameters builds the following K10 platform in the AWS Cloud. The diagram shows three Availability Zones, leveraging multiple AWS services.</p>"},{"location":"addons/kasten-k10/#installation","title":"Installation","text":"<p>Follow new project setup guidelines from https://github.com/aws-quickstart/cdk-eks-blueprints</p> <pre><code>npm install @kastenhq/kasten-eks-blueprints-addon\n</code></pre>"},{"location":"addons/kasten-k10/#basic-usage","title":"Basic Usage","text":"<pre><code>import { App } from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { KastenK10AddOn } from '@kastenhq/kasten-eks-blueprints-addon';\n\nconst app = new App();\n\nblueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(new blueprints.ClusterAutoScalerAddOn)\n    .addOns(new KastenK10AddOn)\n    .build(app, 'eks-with-kastenk10');\n</code></pre>"},{"location":"addons/kasten-k10/#add-on-options","title":"Add-on Options","text":"Option Description Default <code>repository</code> Repository of the Helm chart \"https://charts.kasten.io/\" <code>release</code> Name of the Helm release \"k10\" <code>namespace</code> Namespace to install Kasten K10 \"kasten-io\" <code>version</code> Version of Kasten K10, defaults to latest release \"\" <code>chart</code> Helm Chart Name \"k10\" <code>values</code> Configuration values passed to the chart, options are documented here {}"},{"location":"addons/kasten-k10/#functionality","title":"Functionality","text":"<ol> <li>Create the IAM Role for Service Account for Kasten K10 pod to make API calls to AWS S3 and EC2 to backup and restore.</li> <li>Installs Kasten K10 in a new namespace \"kasten-io\".</li> </ol> <p>The IAM Policy is as follows:</p> <pre><code>    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:CopySnapshot\",\n                \"ec2:CreateSnapshot\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteTags\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DescribeSnapshotAttribute\",\n                \"ec2:ModifySnapshotAttribute\",\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:DescribeSnapshots\",\n                \"ec2:DescribeTags\",\n                \"ec2:DescribeVolumeAttribute\",\n                \"ec2:DescribeVolumesModifications\",\n                \"ec2:DescribeVolumeStatus\",\n                \"ec2:DescribeVolumes\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:DeleteSnapshot\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"ec2:ResourceTag/Name\": \"Kasten: Snapshot*\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:PutBucketPolicy\",\n                \"s3:ListBucket\",\n                \"s3:DeleteObject\",\n                \"s3:DeleteBucketPolicy\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetBucketPolicy\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"addons/kasten-k10/#validating-the-install","title":"Validating the Install","text":"<p>To validate that K10 has been installed properly, the following command can be run in K10's namespace (the install default is <code>kasten-io</code>) to watch for the status of all K10 pods:</p> <p>kubectl get pods --namespace kasten-io --watch</p> <p>It may take a couple of minutes for all pods to come up but all pods should ultimately display the status of <code>Running</code>. <pre><code>kubectl get pods --namespace kasten-io\nNAMESPACE     NAME                                    READY   STATUS    RESTARTS   AGE\nkasten-io     aggregatedapis-svc-b45d98bb5-w54pr      1/1     Running   0          1m26s\nkasten-io     auth-svc-8549fc9c59-9c9fb               1/1     Running   0          1m26s\nkasten-io     catalog-svc-f64666fdf-5t5tv             2/2     Running   0          1m26s\n</code></pre></p> <p>In the unlikely scenario that pods that are stuck in any other state, please follow the support documentation to debug further.</p>"},{"location":"addons/kasten-k10/#validate-dashboard-access","title":"Validate Dashboard Access","text":"<p>By default, the K10 dashboard will not be exposed externally. To establish a connection to it, use the following <code>kubectl</code> command to forward a local port to the K10 ingress port:</p> <p><code>kubectl --namespace kasten-io port-forward service/gateway 8080:8000</code></p> <p>The K10 dashboard will be available at  http://127.0.0.1:8080/k10/#/.</p> <p>For a complete list of options for accessing the Kasten K10 dashboard through a LoadBalancer, Ingress or OpenShift Route you can use the instructions here.</p> <p>For full project documentation visit the KastenHQ Github Repo at (https://github.com/kastenhq/kasten-eks-blueprints-addon).</p>"},{"location":"addons/keda/","title":"Keda Add-on","text":"<p>This add-on installs Keda Kubernetes-based Event Driven Autoscaling.</p> <p>KEDA allows for fine-grained autoscaling (including to/from zero) for event driven Kubernetes workloads. KEDA serves as a Kubernetes Metrics Server and allows users to define autoscaling rules using a dedicated Kubernetes custom resource definition.</p>"},{"location":"addons/keda/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n# Normal Usage\n# const addOn = new blueprints.addons.KedaAddOn();\n# In case of AWS SQS, there is a workaround required when initializing keda (Please refer https://github.com/kedacore/keda/issues/837#issuecomment-789037326 )\nconst kedaParams = {\n    podSecurityContextFsGroup: 1001,\n    securityContextRunAsGroup: 1001,\n    securityContextRunAsUser: 1001,\n    irsaRoles: [\"CloudWatchFullAccess\", \"AmazonSQSFullAccess\"]\n}\nconst addOn = new blueprints.addons.KedaAddOn(kedaParams)\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/keda/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Version fo the Helm Chart to be used to install Keda</li> <li><code>kedaOperatorName</code>: Name of the KEDA operator</li> <li><code>createServiceAccount</code>: Specifies whether a service account should be created by Keda. </li> <li><code>kedaServiceAccountName</code>: The name of the service account to use. If not set and create is true, a name is generated.</li> <li><code>podSecurityContextFsGroup</code>: PodSecurityContext holds pod-level security attributes and common container settings. fsGroup is a special supplemental group that applies to all containers in a pod. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326</li> <li><code>securityContextRunAsUser</code>: SecurityContext holds security configuration that will be applied to a container. runAsUser is the UID to run the entrypoint of the container process. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326</li> <li><code>securityContextRunAsGroup</code>: SecurityContext holds security configuration that will be applied to a container. runAsGroup is the GID to run the entrypoint of the container process. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326</li> <li><code>irsaRoles</code> - An array of Managed IAM Policies which Service Account needs for IRSA Eg: irsaRoles:[\"CloudWatchFullAccess\",\"AmazonSQSFullAccess\"]. If not empty the Service Account will be created by the CDK with IAM Roles Mapped (IRSA). In case if its empty, Keda will create the Service Account with out IAM Roles</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to the Keda Helm Chart documentation for additional details</li> </ul>"},{"location":"addons/keda/#validation","title":"Validation","text":"<p>To validate that Keda is installed properly in the cluster, check that the Keda deployments, services and stateful sets are running.</p> <p><pre><code>  kubectl get deployments -n keda  \n</code></pre> There should be 2 deployments are created once for operator/agent and another for metrics server</p> <p><pre><code>  kubectl get pods -n keda  \n</code></pre> There should be 2 pods one startin with name keda-operator- (Eg: \"keda-operator-56bb8ddd5b-7fqb7\") Verify IRSA is working properly <pre><code>kubectl describe pods keda-operator-8c5967bcd-vlhpd  -n keda | grep -i aws  \n</code></pre></p> <p><pre><code>  kubectl get sa -n keda\n  kubectl describe sa keda-operator -n keda  \n</code></pre> There should be an Service Account with name KedaOperator created and you can see an annotation on Service Account which have the AWS IAM Role ARN ( with required access for AWS Services like AWS SQS)</p>"},{"location":"addons/keda/#testing","title":"Testing","text":"<p>We will test AWS SQS Scalar here 1) Create SQS Queue in desired region Replace ${AWS_REGION} with your target region <pre><code>    aws sqs create-queue --queue-name keda-test --region ${AWS_REGION} --output json\n</code></pre> 2) Create a deployment with SQS Consumer, For simplicity we will create an nginx deployment (in real case scenarion it should be an SQS Consumer) After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080 <pre><code>kubectl create ns sqs-consumer\nkubectl create deployment sqs-consumer --image nginx -n sqs-consumer\n</code></pre> 3) Create SQS Scalar by creating and applying following yaml file for eg: keda-scalar.yaml Replace ${AWS_REGION} with your target region <pre><code>---\napiVersion: keda.sh/v1alpha1 # https://keda.sh/docs/2.0/concepts/scaling-deployments/\nkind: ScaledObject\nmetadata:\n  name: sqs-consumer-keda-scaler\n  namespace: sqs-consumer\n  labels:\n    app: sqs-consumer\n    deploymentName: sqs-consumer\nspec:\n  scaleTargetRef:\n    kind: Deployment\n    name: sqs-consumer\n  minReplicaCount: 1\n  maxReplicaCount: 50\n  pollingInterval: 10\n  cooldownPeriod:  500\n  triggers:\n  - type: aws-sqs-queue\n    metadata:\n      queueURL: https://sqs.${AWS_REGION}.amazonaws.com/ACCOUNT_NUMBER/sqs-consumer\n      queueLength: \"5\"\n      awsRegion: \"${AWS_REGION}\"\n      identityOwner: operator\n---\n</code></pre> <pre><code> kubectl apply -f  keda-scalar.yaml\n</code></pre> 4) Verify HPA is triggered for sqs-consumer namespace <pre><code> kubectl get hpa -n sqs-consumer\n</code></pre> 5) Now send 10 Messages to sqs queue Replace ${AWS_REGION} with your target region <pre><code>x=10\na=0\nwhile [ $a -lt $x ]\ndo\n   aws sqs send-message --region ${AWS_REGION} --endpoint-url https://sqs.${AWS_REGION}.amazonaws.com/ --queue-url https://sqs.${AWS_REGION}.amazonaws.com/ACCOUNT_NUMBER/sqs-consumer  --message-body '{\"key\": \"value\"}'\n   a=`expr $a + 1`\ndone\n</code></pre> 6) Verify if the nginx pod is autoscaled to 2 from 1 <pre><code> kubectl get pods -n sqs-consumer\n</code></pre> 7) Purge the SQS queue to test scale in event Replace ${AWS_REGION} with your target region <pre><code>aws sqs purge-queue --queue-url \"https://sqs.${AWS_REGION}.amazonaws.com/CCOUNT_NUMBER/sqs-consumer\"  \n</code></pre> 6) Verify if the nginx pod is scaledd in from 2 to 1 after teh cool down perion set (500 in this case) <pre><code> kubectl get pods -n sqs-consumer\n</code></pre></p>"},{"location":"addons/keda/#functionality","title":"Functionality","text":"<ol> <li>Installs keda in the cluster</li> <li>Sets up IRSA so that Pods can interact with AWS Services</li> <li>Supports standard helm configuration options.</li> </ol>"},{"location":"addons/knative-operator/","title":"Knative Operator Add-On","text":"<p>Knative is an open source enterprise-grade solution to build serverless and Event Driven applications on Kubernetes. The <code>Knative Operator</code> provides support for installing, configuring and managing Knative without using custom CRDs.</p> <p>Knative Add-on supports standard helm configuration options</p>"},{"location":"addons/knative-operator/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOns = [\n    new blueprints.addons.IstioBaseAddOn(),\n    new blueprints.addons.IstioControlPlaneAddOn(),\n    new blueprints.addons.KNativeOperator()\n];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(...addOns)\n    .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/knative-operator/#applying-knative-eventing","title":"Applying KNative Eventing","text":"<p>To apply KNative Eventing to a specific namespace, you can use the following YAML: <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-eventing\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\n</code></pre> You can also configure KNative to ingest from specific event sources. More configuration instructions can be found in their documentation about event source configurations</p>"},{"location":"addons/knative-operator/#applying-knative-serving","title":"Applying KNative Serving","text":"<p>To apply KNative Serving to a specific Namespace, you can use the following YAML: <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-serving\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\n</code></pre></p> <p>You will have to install a networking layer and configure it to ensure KNative Serving functions properly. The KNative  Setup website has better documentation.</p>"},{"location":"addons/knative-operator/#applying-knative-functions","title":"Applying KNative Functions","text":"<p>Currently, the Knative Operator does not support the deployment of Knative directly as they're directly run as services. For better instructions check their documentation.</p>"},{"location":"addons/konveyor/","title":"Konveyor Add-On for Amazon EKS Blueprints","text":"<p>Konveyor is an open-source application modernization platform that helps organizations safely and predictably modernize applications to new technologies, with an initial focus on accelerating the adoption of legacy applications to Kubernetes. Konveyor\u2019s goal is to deliver a Unified Experience to the organizations embarking on their modernization journey. It follows a simple yet effective approach of surfacing the information about the application to aid a \u2018Decision Maker\u2019 to make decisions about their modernization and migration needs, plan the work in the form of \u2018Migration waves\u2019 and provide guidance to the developers to complete the needed migration/modernization by providing assets as well as a catalog of integrated tools to aid specific workflows.</p> <p>Feature set</p> <ul> <li> <p>Konveyor Hub: Central interface from where you manage your application portfolio and integrate with other Konveyor tools.</p> </li> <li> <p>Categorize and group applications by different dimensions (pre-packaged or custom) aligned with technical criteria or your organization structure.</p> </li> <li> <p>Assess applications containerization suitablity and risks assessment.</p> </li> <li> <p>Assign priority, assess estimated migration effort, and define optimal migration strategy for individual applications.</p> </li> <li> <p>Evaluate required changes for Java applications containerization through automated analysis (pre-packaged or custom rules).</p> </li> <li> <p>Fully integrated Konveyor Data Intensive Validity Advisor (DiVA): Analyzes the data layer of applications and detect dependencies to different data stores and distributed transactions. Import target Java application source files to generate analysis results.</p> </li> </ul> <p>Konveyor is an Open Source software developed by the Konveyor Community, and is a CNCF Sandbox project.</p> <p>This Open Source solution is packaged by Claranet Switzerland GmbH.</p>"},{"location":"addons/konveyor/#arhcitecture","title":"Arhcitecture","text":""},{"location":"addons/konveyor/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ul> <li>AWS CLI (also ensure it is configured)</li> <li>cdk</li> <li>npm</li> <li>tsc</li> <li>make</li> </ul> <p>Let\u2019s start by setting the account and region environment variables:</p> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nAWS_REGION=$(aws configure get region)\n</code></pre>"},{"location":"addons/konveyor/#deployment","title":"Deployment","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Set the pattern's parameters in the CDK context by overriding the cdk.json file (edit PARENT_DOMAIN_NAME as it fits):</p> <pre><code>PARENT_DOMAIN_NAME=example.com\nHOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name $PARENT_DOMAIN_NAME --query \"HostedZones[].Id\" --output text | xargs basename)\ncat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"konveyor.parent.domain.name\":\"${PARENT_DOMAIN_NAME}\",\n        \"konveyor.hosted.zone.id\": \"${HOSTED_ZONE_ID}\"\n      }\n}\nEOF\n</code></pre> <p>(Optional) The full list of parameters you can set in the context is:</p> <pre><code>    \"context\": {\n        \"konveyor.namespace.name\": ...,\n        \"konveyor.parent.domain.name\": ...,\n        \"konveyor.subdomain.label\": ...,\n        \"konveyor.hosted.zone.id\": ...,\n        \"konveyor.certificate.resource.name\": ...,\n      }\n</code></pre> <p>You can assign values to the above keys according to the following criteria (values are required where you don't see default mentioned):</p> <ul> <li>\"konveyor.namespace.name\": Konveyor's namespace, the default is \"konveyor\"</li> <li>\"konveyor.parent.domain.name\": the parent domain in your Hosted Zone</li> <li>\"konveyor.subdomain.label\": to be used as {\"subdomain.label\"}.{\"parent.domain.name\"}, the default is \"backstage\"</li> <li>\"konveyor.hosted.zone.id\": the Hosted zone ID (format: 20x chars/numbers)</li> <li>\"konveyor.certificate.resource.name\": resource name of the certificate, registered by the resource provider, the default is \"konveyor-certificate\"</li> </ul> <p>If you haven't done it before, bootstrap your cdk account and region.</p> <p>Run the following commands:</p> <pre><code>make deps\nmake build\nmake pattern konveyor deploy\n</code></pre> <p>When deployment completes, the output will be similar to the following:</p> <p></p>"},{"location":"addons/konveyor/#example-configuration","title":"Example configuration","text":"<pre><code>import { StackProps } from \"aws-cdk-lib\";\nimport { Construct } from \"constructs\";\nimport * as blueprints from \"@aws-quickstart/eks-blueprints\";\nimport {\n  KonveyorAddOn,\n  OlmAddOn,\n} from \"@claranet-ch/konveyor-eks-blueprint-addon\";\n\nexport interface KonveyorConstructProps extends StackProps {\n  account: string;\n  region: string;\n  parentDomain: string;\n  konveyorLabel: string;\n  hostedZoneId: string;\n  certificateResourceName: string;\n}\n\nexport class KonveyorConstruct extends Construct {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    const props = {\n      account: process.env.CDK_DEFAULT_ACCOUNT,\n      region: process.env.CDK_DEFAULT_REGION,\n      namespace: blueprints.utils.valueFromContext(\n        scope,\n        \"konveyor.namespace.name\",\n        \"konveyor\"\n      ),\n      parentDomain: blueprints.utils.valueFromContext(\n        scope,\n        \"konveyor.parent.domain.name\",\n        \"example.com\"\n      ),\n      konveyorLabel: blueprints.utils.valueFromContext(\n        scope,\n        \"konveyor.subdomain.label\",\n        \"konveyor\"\n      ),\n      hostedZoneId: blueprints.utils.valueFromContext(\n        scope,\n        \"konveyor.hosted.zone.id\",\n        \"1234567890\"\n      ),\n      certificateResourceName: blueprints.utils.valueFromContext(\n        scope,\n        \"konveyor.certificate.resource.name\",\n        \"konveyor-certificate\"\n      ),\n    };\n\n    const subdomain = props.konveyorLabel + \".\" + props.parentDomain;\n\n    const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n      new blueprints.AwsLoadBalancerControllerAddOn(),\n      new blueprints.VpcCniAddOn(),\n      new blueprints.CoreDnsAddOn(),\n      new blueprints.KubeProxyAddOn(),\n      new blueprints.ExternalDnsAddOn({\n        hostedZoneResources: [blueprints.GlobalResources.HostedZone],\n      }),\n      new blueprints.EbsCsiDriverAddOn(),\n      new OlmAddOn(),\n      new KonveyorAddOn({\n        certificateResourceName: props.certificateResourceName,\n        subdomain,\n        featureAuthRequired: \"true\",\n      }),\n    ];\n\n    const blueprint = blueprints.EksBlueprint.builder()\n      .account(props.account)\n      .region(props.region)\n      .resourceProvider(\n        blueprints.GlobalResources.HostedZone,\n        new blueprints.ImportHostedZoneProvider(\n          props.hostedZoneId,\n          props.parentDomain\n        )\n      )\n      .resourceProvider(\n        props.certificateResourceName,\n        new blueprints.CreateCertificateProvider(\n          \"elb-certificate\",\n          subdomain,\n          blueprints.GlobalResources.HostedZone\n        )\n      )\n      .addOns(...addOns)\n      .build(scope, props.konveyorLabel + \"-cluster\");\n  }\n}\n</code></pre>"},{"location":"addons/konveyor/#log-in","title":"Log in","text":"<p>Once the deployment ends navigate to</p> <p><code>https://&lt;subdomain&gt;.&lt;parent-domain&gt;</code></p> <p>And enter the default admin credentials:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>Passw0rd!</code></li> </ul>"},{"location":"addons/konveyor/#koveyor-ui","title":"Koveyor UI","text":"<p>Login page</p> <p></p> <p>Home Page</p> <p></p>"},{"location":"addons/konveyor/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern konveyor destroy\n</code></pre>"},{"location":"addons/kube-proxy/","title":"Kube-proxy Amazon EKS Add-on","text":"<p>The <code>Kube-proxy Amazon EKS Add-on</code> adds support for kube-proxy.</p> <p>Kube-proxy maintains network rules on each Amazon EC2 node. It enables network communication to your pods. Kube-proxy is not deployed to Fargate nodes. For more information, see  kube-proxy  in the Kubernetes documentation.</p> <p>Installing Kube-proxy as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable.</p> <p>Amazon EKS automatically installs Kube-proxy as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.</p>"},{"location":"addons/kube-proxy/#prerequisite","title":"Prerequisite","text":"<ul> <li>Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.</li> </ul>"},{"location":"addons/kube-proxy/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.KubeProxyAddOn('v1.27.1-eksbuild.1'); // optionally specify the image version to pull or empty constructor for auto selection\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/kube-proxy/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Optionally pass in the kube-proxy plugin version compatible with kubernetes-cluster version as shown below <pre><code># Assuming cluster version is 1.27, below command shows versions of the Kube-proxy add-on available for the specified cluster's version.\naws eks describe-addon-versions \\\n    --addon-name kube-proxy \\\n    --kubernetes-version 1.27 \\\n    --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\\n    --output text\n# Output\nv1.27.3-eksbuild.1\nFalse\nv1.27.1-eksbuild.1\nTrue\n</code></pre></li> </ul>"},{"location":"addons/kube-proxy/#validation","title":"Validation","text":"<p>To validate that kube-proxy add-on is running, ensure that the pod is in Running state <pre><code>$ kubectl get pods  -n kube-system|grep kube-proxy\nNAME                READY    STATUS    RESTARTS     AGE\nkube-proxy-6lrjm     1/1     Running       0        34d\n</code></pre> <pre><code># Assuming cluster-name is my-cluster, below command shows the version of Kube-proxy installed. Check if it is same as the version installed via EKS add-on\naws eks describe-addon \\\n    --cluster-name my-cluster \\\n    --addon-name kube-proxy \\\n    --query \"addon.addonVersion\" \\\n    --output text\n# Output\nv1.27.1-eksbuild.1\n</code></pre></p>"},{"location":"addons/kube-proxy/#functionality","title":"Functionality","text":"<p>Applies Kube-proxy add-on to Amazon EKS cluster.</p>"},{"location":"addons/kube-state-metrics/","title":"Kube State Metrics Add-on","text":"<p>This add-on installs kube-state-metrics.</p> <p>kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. (See examples in the Metrics section below.) It is not focused on the health of the individual Kubernetes components, but rather on the health of the various objects inside, such as deployments, nodes and pods.</p> <p>kube-state-metrics Add-on is about generating metrics from Kubernetes API objects without modification. This ensures that features provided by kube-state-metrics have the same grade of stability as the Kubernetes API objects themselves.</p>"},{"location":"addons/kube-state-metrics/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.KubeStateMetricsAddOn()\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/kube-state-metrics/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you</li> <li><code>values</code>: Arbitrary values to pass to the chart. </li> <li>Standard helm configuration options.</li> </ul>"},{"location":"addons/kube-state-metrics/#validation","title":"Validation","text":"<p>To validate that kube-state-metrics is installed properly in the cluster, check if the kube-state-metrics pods are running.</p> <p>Verify if the pods are running correctly for kube-state-metrics <pre><code>  kubectl get pods -A | awk '/kube-state/ {print ;exit}'  \n</code></pre></p>"},{"location":"addons/kube-state-metrics/#output","title":"Output","text":"<p>There should list pod starting with name <code>kube-state</code> For Eg: <pre><code>kube-system                     kube-state-metrics-5d4c95885d-82bv4                               1/1     Running   0          13h\n</code></pre></p>"},{"location":"addons/kube-state-metrics/#functionality","title":"Functionality","text":"<p>Applies the kube-state-metrics add-on to an Amazon EKS cluster. </p>"},{"location":"addons/kubecost/","title":"Kubecost AddOn","text":"<p>Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts. By identifying root causes for negative patterns, customers using Kubecost save 30-50% or more of their Kubernetes cloud infrastructure costs. To read more about Kubecost and how to use it, see the product and technical docs.</p>"},{"location":"addons/kubecost/#installation","title":"Installation","text":"<p>Using npm:</p> <pre><code>$ npm install @kubecost/kubecost-eks-blueprints-addon\n</code></pre>"},{"location":"addons/kubecost/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { KubecostAddOn } from '@kubecost/kubecost-eks-blueprints-addon';\n\nconst app = new cdk.App();\n\nconst addOn = new KubecostAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/kubecost/#kubecostaddon-options-props","title":"<code>KubecostAddOn</code> Options (props)","text":""},{"location":"addons/kubecost/#namespace-string-optional","title":"<code>namespace: string</code> (optional)","text":"<p>The namespace where Kubecost will be installed. Defaults to <code>kubecost</code>.</p>"},{"location":"addons/kubecost/#kubecosttoken-string-optional","title":"<code>kubecostToken: string</code> (optional)","text":"<p>You may get one here.</p>"},{"location":"addons/kubecost/#version-string-optional","title":"<code>version: string</code> (optional)","text":"<p>The <code>cost-analyzer</code> helm chart version. Defaults to the latest stable version specified in this repo (<code>1.92.0</code> at the time of writing).</p>"},{"location":"addons/kubecost/#values-key-string-any-optional","title":"<code>values?: { [key: string]: any }</code> (optional)","text":"<p>Custom values to pass to the chart. Config options: https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options </p>"},{"location":"addons/kubecost/#customprometheus-string-optional","title":"<code>customPrometheus: string</code> (optional)","text":"<p>Kubecost comes bundled with a Prometheus installation. However, if you wish to integrate with an external Prometheus deployment, provide your local Prometheus service address with this format <code>http://..svc</code>. Note: integrating with an existing Prometheus is only officially supported under Kubecost paid plans and requires some extra configurations on your Prometheus: https://docs.kubecost.com/install-and-configure/advanced-configuration/custom-prom</p>"},{"location":"addons/kubecost/#installprometheusnodeexporter-boolean-optional","title":"<code>installPrometheusNodeExporter: boolean</code> (optional)","text":"<p>Set to false to use an existing Node Exporter DaemonSet. Note: this requires your existing Node Exporter endpoint to be visible from the namespace where Kubecost is installed. https://docs.kubecost.com/install-and-configure/install/getting-started#using-an-existing-node-exporter</p>"},{"location":"addons/kubecost/#repository-string-release-string-chart-string-optional","title":"<code>repository: string</code>, <code>release: string</code>, <code>chart: string</code> (optional)","text":"<p>Additional options for customers who may need to supply their own private Helm repository.</p>"},{"location":"addons/kubecost/#support","title":"Support","text":"<p>If you have any questions about Kubecost, get in touch with the team on Slack.</p>"},{"location":"addons/kubecost/#license","title":"License","text":"<p>The Kubecost Blueprints AddOn is licensed under the Apache 2.0 license. Project repository</p>"},{"location":"addons/kubeflow/","title":"Kubeflow AddOn","text":"<p>The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow.</p>"},{"location":"addons/kubeflow/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"addons/kubeflow/#installation","title":"Installation","text":"<p>Using npm:</p> <pre><code>$ npm install eks-blueprints-cdk-kubeflow-ext\n</code></pre>"},{"location":"addons/kubeflow/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { KubeflowAddOn } from 'eks-blueprints-cdk-kubeflow-ext';\n\nconst app = new cdk.App();\n\nconst addOn = new KubeflowAddOn(\n  {\n    namespace: 'kubeflow-pipelines'\n  }\n);\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/kubeflow/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>namespace</code>: the namespace of your kubernetes cluster to be used to install kubeflow</li> </ul>"},{"location":"addons/kubeflow/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/kubeflow-blueprint-kubeflowblueprintMastersRole0C1-saJBO\n</code></pre></p> <p>Let\u2019s verify the resources created by Steps above. <pre><code>kubectl get nodes # Output shows the EKS Managed Node group nodes\n\nkubectl get ns | kubeflow # Output shows kubeflow namespace\n\nkubectl get pods --namespace=kubeflow-pipelines  # Output shows kubeflow pods\n</code></pre></p>"},{"location":"addons/kubeflow/#execute-machine-learning-jobs-on-kubeflow","title":"Execute Machine learning jobs on Kubeflow","text":"<p>log into Kubeflow pipeline UI by creating a port-forward to the ml-pipeline-ui service</p> <p><pre><code>kubectl port-forward svc/ml-pipeline-ui 9000:80 -n =kubeflow-pipelines\n</code></pre> and open this browser: http://localhost:9000/#/pipelines more pipeline examples can be found at https://www.kubeflow.org/docs/components/pipelines/legacy-v1/tutorials/</p>"},{"location":"addons/kubeflow/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>cdk destroy kubeflow-blueprint \n</code></pre>"},{"location":"addons/kubeflow/#kubeflow-on-eks-pattern","title":"Kubeflow on EKS Pattern","text":"<p>For more information about the Kubeflow add module, please visit Kubeflow on EKS Pattern.</p>"},{"location":"addons/kubeflow/#license","title":"License","text":"<p>The Kubeflow CDK Blueprints AddOn is licensed under the Apache 2.0 license.</p>"},{"location":"addons/kubeflow/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open source NPM package eks-blueprints-cdk-kubeflow-ext. Please refer to the package npm site for more information. https://www.npmjs.com/package/eks-blueprints-cdk-kubeflow-ext</p>"},{"location":"addons/kuberay-operator/","title":"KubeRay Operator Add-on","text":"<p>Ray is a framework for scaling AI applications written in Python. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute. Ray allows scaling applications from a laptop to a cluster.</p> <p>KubeRay is a Kubernetes Operator that simplifies the deployment and management of Ray applications on Kubernetes. It is made of the following components:</p> <ul> <li>KubeRay core, the official, fully-maintained component of KubeRay that provides three custom resource definitions, RayCluster, RayJob, and RayService</li> <li>Community-managed components (optional): KubeRay APIServer, KubeRay Python client and KubeRay CLI</li> </ul> <p>Please refer to KubeRay Operator documentation for detailed information.</p> <p>This add-on installs the KubeRay Operator Helm chart.</p>"},{"location":"addons/kuberay-operator/#usage","title":"Usage","text":"<pre><code>import { Construct } from 'constructs';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nexport class KubeRayConstruct extends Construct {\n    constructor(scope: Construct, id: string) {\n        super(scope, id);\n\n        const stackID = `${id}-blueprint`;\n\n        const addOn = new blueprints.addons.KubeRayAddOn();\n\n        blueprints.EksBlueprint.builder()\n            .account(process.env.CDK_DEFAULT_ACCOUNT!)\n            .region(process.env.CDK_DEFAULT_REGION!)\n            .version('auto')\n            .addOns(addOn)\n            .build(scope, stackID);\n    }\n}\n</code></pre>"},{"location":"addons/kuberay-operator/#validation","title":"Validation","text":"<p>To validate the KubeRay Operator installation, please run the following command:</p> <pre><code>kubectl get pods \n</code></pre> <p>Expected output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\nkuberay-operator-58c98b495b-5k75l   1/1     Running   0          112m\n</code></pre>"},{"location":"addons/kubeshark/","title":"Kubeshark AddOn","text":"<p>kubeshark  is an API Traffic Analyzer for Kubernetes providing real-time, protocol-level visibility into Kubernetes\u2019 internal network, capturing and monitoring all traffic and payloads going in, out and across containers, pods, nodes and clusters.</p> <p>Kubeshark provide Real-time monitoring for all traffic going in, out and across containers, pods, namespaces, nodes and clusters, which allow you to pinpoint and resolve issues efficiently, ensuring stable network performance and enhancing application success in Kubernetes environments and identifying complex networking issue.</p>"},{"location":"addons/kubeshark/#usage","title":"Usage","text":"<ol> <li>import kubeshark <pre><code>npm i kubeshark\n</code></pre></li> <li> <p>import it in your <code>blueprint.ts</code> <pre><code>import { KubesharkAddOn } from 'kubeshark';\n</code></pre></p> </li> <li> <p>include the addon <pre><code>    new KubesharkAddOn({})  // Provide an empty object if no specific properties are needed\n</code></pre></p> </li> </ol>"},{"location":"addons/kubeshark/#full-example-indexts","title":"Full example <code>index.ts</code>","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { KubesharkAddOn } from 'kubeshark';\n\nconst app = new cdk.App();\nconst account = '1234123412341';\nconst region = 'us-east-1';\nconst version = 'auto';\n\nblueprints.HelmAddOn.validateHelmVersions = true; // optional if you would like to check for newer versions\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new blueprints.addons.MetricsServerAddOn(),\n    new blueprints.addons.ClusterAutoScalerAddOn(),\n    new blueprints.addons.AwsLoadBalancerControllerAddOn(),\n    new blueprints.addons.VpcCniAddOn(),\n    new blueprints.addons.CoreDnsAddOn(),\n    new blueprints.addons.KubeProxyAddOn(),\n    new KubesharkAddOn({})  // Provide an empty object if no specific properties are needed\n];\n\nconst stack = blueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .version(version)\n    .addOns(...addOns)\n    .useDefaultSecretEncryption(true) // set to false to turn secret encryption off (non-production/demo cases)\n    .build(app, 'eks-blueprint');```\n</code></pre>"},{"location":"addons/kubeshark/#validate-the-deployment","title":"validate the deployment","text":"<p>Once deployed, you can see kubeshark pod in the <code>kube-system</code> namespace.</p> <pre><code>$ kubectl get deployments -n kube-system\n\nNAME                                                          READY   UP-TO-DATE   AVAILABLE   AGE\nblueprints-addon-kubeshark                               1/1     1            1           20m\n</code></pre>"},{"location":"addons/kubeshark/#functionality","title":"Functionality","text":"<ol> <li>Deploys the kubeshark helm chart in <code>kube-system</code> namespace by default.</li> <li>Supports standard helm configuration options.</li> <li>Supports <code>createNamespace</code> configuration to deploy the addon to a customized namespace.</li> </ol>"},{"location":"addons/kubeshark/#access-kubeshark","title":"Access Kubeshark","text":"<p>Apply the kubernetes dashboard manifest.</p> <pre><code>$ kubectl -n kube-system port-forward svc/kubeshark-front 3000:80\n</code></pre> <p>Open the dashboard</p> <p>Then you should be able to see view like this </p>"},{"location":"addons/kubeshark/#example","title":"Example","text":"<p>1.) deploy nginx pod using the below command. <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\nEOF\n</code></pre></p> <p>2.) Try to access \"aws.com\" to generate traffic flow using the below command. <pre><code>kubectl exec nginx curl https://aws.com\n</code></pre></p> <p>3.) Access kubeshark using the below command. <pre><code>kubectl -n kube-system port-forward svc/kubeshark-front 3000:80\n</code></pre></p> <p>4.) Run Kubeshark query to identify the traffic flow. <pre><code>(src.pod.metadata.name == \"nginx\" or dst.pod.metadata name == \"nginx\") and request.questions[0].name == \"aws.com\" or (src.name == \"nginx\" and src.namespace == \"default\" and dst.name == \"kube-dns\" and dst.namespace == \"kube-system\")\n</code></pre></p> <p>As shown below, the Kubeshark query used to identify the traffic flowing from the pod \"nginx\" in the \"default\" namespace to \"aws.com\" and \"coredns\". The query is writen by Kubeshark Filter Language (KFL) is the language implemented inside kubeshark/worker that enables the user to filter the traffic efficiently and precisely.</p> <p></p> <p>Also you can visualize the traffic flow and bandwidth using service map feature as shown below. </p>"},{"location":"addons/kubevious/","title":"Kubevious Add-on","text":"<p>This add-on installs Kubevious open source Kubernetes dashboard on Amazon EKS.</p> <p>Kubevious provides logical grouping of application resources eliminating the need to dig through selectors and labels.  It also provides the ability identify potential misconfigurations using both standard and user created rules that  monitor the cluster</p>"},{"location":"addons/kubevious/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.KubeviousAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/kubevious/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Version fo the Helm Chart to be used to install Kubevious</li> <li><code>ingressEnabled</code>: Indicates whether to expose Kubevious using an ingress gateway. Set to false by default</li> <li><code>kubeviousServiceType</code>: Type of service used to expose Kubevious backend. Set to 'ClusterIP' by default</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to the Kubevious Helm Chart documentation for additional details</li> </ul>"},{"location":"addons/kubevious/#validation","title":"Validation","text":"<p>To validate that Kubevious is installed properly in the cluster, check that the Kubevious deployments, services and stateful sets are running.</p> <p><pre><code>  kubectl get all -n kubevious  \n</code></pre> Note that Kubevious is installed in its own <code>kubevious</code> namespace</p>"},{"location":"addons/kubevious/#accessing-the-kubevious-dashboard","title":"Accessing the Kubevious dashboard","text":"<p>To access the application, set up port-forwarding as follows: </p> <p><pre><code>    kubectl port-forward $(kubectl get pods -n kubevious -l \"app.kubernetes.io/component=kubevious-ui\" -o jsonpath=\"{.items[0].metadata.name}\") 8080:80 -n kubevious  \n</code></pre> After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080</p> <p>Alternatively, Kubevious can be exposed by enabling the ingress by setting the <code>ingressEnabled</code> configuration option to true. </p>"},{"location":"addons/kubevious/#mysql-root-password","title":"MySQL root password","text":"<p>Kubevious internally deploys and uses MySQL to persist data. The Kubevious add-on secures access to the database by generating a random password for the MySQL root user. While it is not usually necessary to access the Kubevious MySQL database externally, it is possible to retrieve the  generated value by executing the command below:</p> <pre><code>    echo $(kubectl get secret kubevious-mysql-secret-root  -o jsonpath='{.data.MYSQL_ROOT_PASSWORD}' -n kubevious) | base64 --decode\n</code></pre>"},{"location":"addons/kubevious/#persistent-volume-usage","title":"Persistent Volume usage","text":"<p>Kubevious automatically creates a Persistent Volume (PV) to store the MySQL database data. However, per the Kubevious documentation, the PV is not removed when Kubevious is  uninstalled and must be removed manually:</p> <pre><code>    kubectl delete pvc data-kubevious-mysql-0 -n kubevious\n</code></pre>"},{"location":"addons/kubevious/#functionality","title":"Functionality","text":"<ol> <li>Installs Kubevious in the cluster</li> <li>Sets up all AIM necessary roles to integrate Kubevious in AWS EKS</li> <li>Supports standard helm configuration options.</li> </ol>"},{"location":"addons/metrics-server/","title":"Metrics Server AddOn","text":"<p>Metrics Server  is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It is not deployed by default in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add ons, such as the Horizontal Pod Autoscaler, Vertical Autoscaling or the Kubernetes Dashboard.</p> <p>Important: Don't use Metrics Server when you need an accurate source of resource usage metrics or as a monitoring solution.</p>"},{"location":"addons/metrics-server/#usage","title":"Usage","text":""},{"location":"addons/metrics-server/#indexts","title":"<code>index.ts</code>","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.MetricsServerAddOn('v0.5.0');\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Once deployed, you can see metrics-server pod in the <code>kube-system</code> namespace.</p> <pre><code>$ kubectl get deployments -n kube-system\n\nNAME                                                          READY   UP-TO-DATE   AVAILABLE   AGE\nblueprints-addon-metrics-server                               1/1     1            1           20m\n</code></pre>"},{"location":"addons/metrics-server/#functionality","title":"Functionality","text":"<ol> <li>Deploys the metrics-server helm chart in <code>kube-system</code> namespace by default.</li> <li>Supports standard helm configuration options.</li> <li>Supports <code>createNamespace</code> configuration to deploy the addon to a customized namespace.</li> </ol>"},{"location":"addons/metrics-server/#testing-with-kubernetes-dashboard","title":"Testing with Kubernetes Dashboard","text":"<p>For testing, we will use the Kubernetes Dashboard to view CPU and memory metrics of our cluster.</p> <p>Apply the kubernetes dashboard manifest.</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml\n\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre> <p>Create a file called eks-admin-service-account.yaml with the text below. This manifest defines a service account and cluster role binding called eks-admin.</p> <pre><code>$ cat &lt;&lt; 'EOF' &gt;&gt; eks-admin-service-account.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: eks-admin\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: eks-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: eks-admin\n  namespace: kube-system\nEOF\n</code></pre> <p>Apply the service account and cluster role binding to your cluster.</p> <pre><code>$ kubectl apply -f eks-admin-service-account.yaml\n\nserviceaccount/eks-admin created\nclusterrolebinding.rbac.authorization.k8s.io/eks-admin created\n</code></pre> <p>Retrieve an authentication token for the eks-admin service account. Copy the  value from the output. You use this token to connect to the dashboard. <pre><code>$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}')\n\nName:         eks-admin-token-dwzb2\nNamespace:    kube-system\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/service-account.name: eks-admin\n              kubernetes.io/service-account.uid: 6fb4eb46-553e-44bf-b0e7-9ae8f5f500d6\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1066 bytes\nnamespace:  11 bytes\ntoken:      XXXXXXXXXXXXXXXXXXXXXX\n</code></pre> <p>Start the kubectl proxy.</p> <pre><code>$ kubectl proxy\n</code></pre> <p>Open the dashboard in your browser and login using the value for <code>token</code> above.</p> <p></p> <p>Note: It may take a few minutes before CPU and memory metrics appear in the dashboard</p>"},{"location":"addons/neuron-device-plugin-addon/","title":"Neuron Device Plugin Addon","text":"<p>AWS Neuron is the SDK used to run deep learning workloads on AWS Inferentia and AWS Trainium based instances. This addon will install the Neuron Device Plugin necessary to run the instances on Amazon EKS (and Blueprints). Note that you must use inf1, inf2, trn1, or trn1n instances.</p>"},{"location":"addons/neuron-device-plugin-addon/#usage","title":"Usage","text":""},{"location":"addons/neuron-device-plugin-addon/#indexts","title":"<code>index.ts</code>","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.NeuronDevicePluginAddon();\n\nconst clusterProvider = new blueprints.GenericClusterProvider({\n  version: KubernetesVersion.V1_27,\n  managedNodeGroups: [\n    inferentiaNodeGroup()\n  ]\n});\n\nfunction inferentiaNodeGroup(): blueprints.ManagedNodeGroup {\n  return {\n    id: \"mng1\",\n      instanceTypes: [new ec2.InstanceType('inf1.2xlarge')],\n      desiredSize: 1,\n      maxSize: 2, \n      nodeGroupSubnets: { subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS },\n  };\n}\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .clusterProvider(clusterProvider)\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Once deployed, you can see the plugin daemonset in the <code>kube-system</code> namespace.</p> <pre><code>$ kubectl get daemonset neuron-device-plugin-daemonset -n kube-system\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\nneuron-device-plugin-daemonset   1         1         1       1            1           &lt;none&gt;          24m   20m\n</code></pre>"},{"location":"addons/neuron-device-plugin-addon/#functionality","title":"Functionality","text":"<ol> <li>Deploys the plugin daemonset in <code>kube-system</code> namespace by default.</li> <li>Provides a plugin for the blueprint to leverage the Inferentia or Trainium instances to use the Neuron SDK.</li> </ol>"},{"location":"addons/neuron-monitor-addon/","title":"Neuron Monitor Addon","text":"<p>Neuron Monitor collects metrics and stats from the Neuron Applications running on the system and streams the collected data to stdout in JSON format.</p> <p>These metrics and stats are organized into metric groups which can be configured by providing a configuration file as described in Using neuron-monitor</p> <p>When running, neuron-monitor will:</p> <ul> <li>Collect the data for the metric groups which, based on the elapsed time since their last update, need to be updated</li> <li>Take the newly collected data and consolidate it into a large report</li> <li>Serialize that report to JSON and stream it to stdout from where it can be consumed by other tools - such as the sample neuron-monitor-cloudwatch.py and neuron-monitor-prometheus.py scripts.</li> <li>Wait until at least one metric group needs to be collected and repeat this flow</li> </ul>"},{"location":"addons/neuron-monitor-addon/#usage","title":"Usage","text":""},{"location":"addons/neuron-monitor-addon/#indexts","title":"<code>index.ts</code>","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst neuronMonitorAddon = new blueprints.addons.NeuronMonitorAddOn()\n\nconst clusterProvider = new blueprints.GenericClusterProvider({\n  version: KubernetesVersion.V1_27,\n  managedNodeGroups: [\n    inferentiaNodeGroup()\n  ]\n});\n\nfunction inferentiaNodeGroup(): blueprints.ManagedNodeGroup {\n  return {\n    id: \"mng1\",\n      instanceTypes: [new ec2.InstanceType('inf1.2xlarge')],\n      desiredSize: 1,\n      maxSize: 2, \n      nodeGroupSubnets: { subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS },\n  };\n}\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .clusterProvider(clusterProvider)\n  .addOns(neuronMonitorAddon)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Once deployed, you can see the monitor and device plugin deamonsets in the <code>kube-system</code> namespace.</p> <pre><code>$ kubectl get daemonset -n kube-system \n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\nneuron-monitor                   1         1         1       1            1           &lt;none&gt;          3m12s\n</code></pre>"},{"location":"addons/newrelic/","title":"New Relic","text":""},{"location":"addons/newrelic/#new-relic-addon-aws-eks-blueprints-for-cdk","title":"New Relic Addon - AWS EKS Blueprints for CDK","text":"<p>This repository contains the source code for the New Relic AddOn for AWS EKS Blueprints. EKS Blueprints for CDK is a framework that makes it easy for customers to configure and deploy New Relic's Kubernetes integration as part of an EKS Blueprints cluster on Amazon EKS.</p>"},{"location":"addons/newrelic/#installation","title":"Installation","text":"<p>Using npm:</p> <pre><code>npm install @newrelic/newrelic-eks-blueprints-addon\n</code></pre> <p>For a quick tutorial on EKS Blueprints, visit the Getting Started guide.</p>"},{"location":"addons/newrelic/#retrieving-keys","title":"Retrieving keys","text":"<p>The New Relic and Pixie keys can be obtained from the New Relic Guided Install for Kubernetes.</p>"},{"location":"addons/newrelic/#aws-secrets-manager-key-format","title":"AWS Secrets Manager key format","text":"<pre><code>{\n  \"nrLicenseKey\": \"xxxxNRAL\",\n  \"pixieDeployKey\": \"px-dep-xxxx\",\n  \"pixieApiKey\": \"px-api-xxxx\"\n}\n</code></pre>"},{"location":"addons/newrelic/#example-configuration-using-keys-stored-in-secrets-manager","title":"Example Configuration (using keys stored in Secrets Manager):","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { NewRelicAddOn } from '@newrelic/newrelic-eks-blueprints-addon';\n\nconst app = new cdk.App();\n\nblueprints.EksBlueprint.builder()\n    .addOns(new blueprints.MetricsServerAddOn)\n    .addOns(new blueprints.ClusterAutoScalerAddOn)\n    .addOns(new blueprints.addons.SSMAgentAddOn)\n    .addOns(new blueprints.addons.SecretsStoreAddOn)\n    .addOns(new NewRelicAddOn({\n        version: \"4.2.0-beta\",\n        newRelicClusterName: \"demo-cluster\",\n        awsSecretName: \"newrelic-pixie-combined\", // Secret Name in AWS Secrets Manager\n        installPixie: true,\n        installPixieIntegration: true,\n    }))\n    .region(process.env.AWS_REGION)\n    .account(process.env.AWS_ACCOUNT)\n    .version(\"auto\")\n    .build(app, 'demo-cluster');\n</code></pre>"},{"location":"addons/newrelic/#example-configuration-using-keys","title":"Example Configuration (using keys):","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { NewRelicAddOn } from '@newrelic/newrelic-eks-blueprints-addon';\n\nconst app = new cdk.App();\n\nblueprints.EksBlueprint.builder()\n    .addOns(new blueprints.MetricsServerAddOn)\n    .addOns(new blueprints.ClusterAutoScalerAddOn)\n    .addOns(new blueprints.addons.SSMAgentAddOn)\n    .addOns(new blueprints.addons.SecretsStoreAddOn)\n    .addOns(new NewRelicAddOn({\n        version: \"4.2.0-beta\",\n        newRelicClusterName: \"demo-cluster\",\n        newRelicLicenseKey: \"NEW RELIC LICENSE KEY\",\n        installPixie: true,\n        installPixieIntegration: true,\n        pixieApiKey: \"PIXIE API KEY\",\n        pixieDeployKey: \"PIXIE DEPLOY KEY\"\n    }))\n    .region(process.env.AWS_REGION)\n    .account(process.env.AWS_ACCOUNT)\n    .version(\"auto\")\n    .build(app, 'demo-cluster');\n</code></pre>"},{"location":"addons/newrelic/#validation","title":"Validation","text":""},{"location":"addons/newrelic/#nrql-query","title":"NRQL Query","text":"<p>Almost immediately after the New Relic pods enter a <code>Running</code> state in the cluster, data should be reported to New Relic.  You can validate that metrics are making it to New Relic with the following NRQL query:</p> <p><code>FROM K8sClusterSample, K8sNodeSample select latest(clusterK8sVersion), latest(agentVersion) as 'NR Agent Ver.', uniqueCount(nodeName) as 'Node Count' facet clusterName limit max</code></p> <p></p>"},{"location":"addons/newrelic/#new-relic-one-ui","title":"New Relic One UI","text":"<p>After installing the New Relic add-on, you can validate a successful installation by visiting New Relic's Entity Explorer filtered to Kubernetes Clusters.</p> <p></p>"},{"location":"addons/newrelic/#variables","title":"Variables","text":"Variable Type Required Description newRelicLicenseKey string True New Relic License Key (plain text). Use <code>awsSecretName</code> instead for AWS Secrets Manager support and added security. awsSecretName string True AWS Secret name containing the New Relic and Pixie keys in AWS Secrets Manager. Define secret in JSON format with the following keys:  <code>{   \"nrLicenseKey\": \"REPLACE WITH YOUR NEW RELIC LICENSE KEY\",   \"pixieDeployKey\": \"REPLACE WITH YOUR PIXIE LICENSE KEY\",   \"pixieApiKey\": \"REPLACE WITH YOUR PIXIE API KEY\" }</code>  Keys can be obtained in the New Relic Guided Install for Kubernetes newRelicClusterName string True Name for the cluster in the New Relic UI. pixieApiKey string Pixie Api Key can be obtained in New Relic's Guided Install for Kubernetes (plaintext).  Use <code>awsSecretName</code> instead for AWS Secrets Manager support and added security. pixieDeployKey string Pixie Deploy Key can be obtained in New Relic's Guided Install for Kubernetes -  (plaintext).  Use <code>awsSecretName</code> instead for AWS Secrets Manager support and added security. namespace string The namespace where New Relic components will be installed. Defaults to  <code>newrelic</code>. lowDataMode boolean Default  <code>true</code>.  Set to  <code>false</code>  to disable  <code>lowDataMode</code> .  For more details, visit the Reducing Data Ingest Docs installInfrastructure boolean Default  <code>true</code> .  Set to  <code>false</code>  to disable installation of the New Relic Infrastructure Daemonset. installKSM boolean Default  <code>true</code> .  Set to  <code>false</code>  to disable installation of Kube State Metrics.  An instance of KSM is required in the cluster for the New Relic Infrastructure Daemonset to function properly. installKubeEvents boolean Default  <code>true</code> .  Set to  <code>false</code>  to disable installation of the New Relic Kubernetes Events integration. installLogging boolean Default  <code>true</code> .  Set to  <code>false</code>  to disable installation of the New Relic Logging (Fluent-Bit) Daemonset. installMetricsAdapter boolean Default  <code>false</code> .  Set to  <code>true</code>  to enable installation of the New Relic Kubernetes Metrics Adapter. installPrometheus boolean Default  <code>true</code> .  Set to  <code>false</code>  to disable installation of the Prometheus OpenMetrics Integration. installPixie boolean Default  <code>false</code> .  Set to  <code>true</code>  to enable installation Pixie into the cluster. installPixieIntegration boolean Default   <code>false</code>  .  Set to   <code>true</code>   to enable installation the New Relic &lt;-&gt; Pixie integration pod into the cluster. version string Helm chart version. repository string Additional options for customers who may need to supply their own private Helm repository. release string Additional options for customers who may need to supply their own private Helm repository. chart string Additional options for customers who may need to supply their own private Helm repository. values { [key: string]: any } Custom values to pass to the chart. Config options: https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle#configuration"},{"location":"addons/newrelic/#support","title":"Support","text":"<p>New Relic hosts and moderates an online forum where customers can interact with New Relic employees as well as other customers to get help and share best practices.</p> <p>https://discuss.newrelic.com/</p>"},{"location":"addons/newrelic/#contributing","title":"Contributing","text":"<p>We encourage your contributions to improve the New Relic Addon for EKS Blueprints! Keep in mind when you submit your pull request, you'll need to sign the CLA via the click-through using CLA-Assistant. You only have to sign the CLA one time per project. If you have any questions, or to execute our corporate CLA, required if your contribution is on behalf of a company,  please drop us an email at opensource@newrelic.com.</p> <p>A note about vulnerabilities</p> <p>As noted in our security policy, New Relic is committed to the privacy and security of our customers and their data. We believe that providing coordinated disclosure by security researchers and engaging with the security community are important means to achieve our security goals.</p> <p>If you believe you have found a security vulnerability in this project or any of New Relic's products or websites, we welcome and greatly appreciate you reporting it to New Relic through HackerOne.</p>"},{"location":"addons/newrelic/#license","title":"License","text":"<p>The New Relic Addon for EKS Blueprints is licensed under the Apache 2.0 License.</p>"},{"location":"addons/nginx/","title":"NGINX Add-on","text":"<p>This add-on installs NGINX Ingress Controller on Amazon EKS. NGINX ingress controller is using NGINX as a reverse proxy and load balancer. </p> <p>Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path based routing). </p> <p>IMPORTANT:  This add-on depends on AWS Load Balancer Controller Add-on in order to enable NLB support.</p> <p>AWS Load Balancer Controller add-on must be present in add-on array and must be in add-on array before the NGINX ingress controller add-on for it to work, as shown in below example. Otherwise will run into error <code>Assertion failed: Missing a dependency for AwsLoadBalancerControllerAddOn</code>.</p>"},{"location":"addons/nginx/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst externalDnsHostname = ...;\nconst awsLbControllerAddOn = new blueprints.addons.AwsLoadBalancerControllerAddOn();\nconst nginxAddOn = new blueprints.addons.NginxAddOn({ externalDnsHostname })\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [ awsLbControllerAddOn, nginxAddOn ];\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(...addOns)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that installation is successful run the following command:</p> <pre><code>$ kubectl get po -n kube-system\nNAME                                                              READY   STATUS    RESTARTS   AGE\nblueprints-addon-nginx-ingress-78b8567p4q6   1/1     Running   0          4d10h\n</code></pre> <p>Note that the ingress controller is deployed in the <code>kube-system</code> namespace.</p> <p>Once deployed, it allows applications to create ingress objects and use host based routing with external DNS support, if External DNS Add-on is installed.</p>"},{"location":"addons/nginx/#configuration","title":"Configuration","text":"<ul> <li><code>backendProtocol</code>: indication for AWS Load Balancer controller with respect to the protocol supported on the load balancer. TCP by default.</li> <li><code>crossZoneEnabled</code>: whether to create a cross-zone load balancer with the service that backs NGINX.</li> <li><code>internetFacing</code>: whether the created load balancer is internet facing. Defaults to <code>true</code> if not specified. Internal load balancer is provisioned if set to <code>false</code></li> <li><code>targetType</code>: <code>IP</code> or <code>instance</code> mode. Defaults to <code>IP</code> which requires VPC-CNI and has better performance eliminating a hop through kubeproxy. Instance mode leverages traditional NodePort mode on the instances. </li> <li><code>externaDnsHostname</code>: Used in conjunction with the external DNS add-on to handle automatic registration of the service with Route53. </li> <li><code>values</code>: Arbitrary values to pass to the chart as per https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/#</li> </ul>"},{"location":"addons/nginx/#dns-integration-and-routing","title":"DNS Integration and Routing","text":"<p>If External DNS Add-on is installed, it is possible to configure NGINX ingress with an external NLB load balancer and leverage wild-card DNS domains (and public certificate) to route external traffic to individual workloads. </p> <p>The following example provides support for AWS Load Balancer controller, External DNS and NGINX add-ons to enable such routing:</p> <pre><code>blueprints.EksBlueprint.builder()\n    //  Register hosted zone1 under the name of MyHostedZone1\n    .resourceProvider(\"MyHostedZone1\",  new blueprints.DelegatingHostedZoneProvider({\n        parentDomain: 'myglobal-domain.com',\n        subdomain: 'dev.myglobal-domain.com', \n        parentAccountId: parentDnsAccountId,\n        delegatingRoleName: 'DomainOperatorRole',\n        wildcardSubdomain: true\n    })\n    .addOns(new blueprints.addons.ExternalDnsAddOn({\n        hostedZoneProviders: [\"MyHostedZone1\"];\n    })\n    .addOns(new blueprints.NginxAddOn({ internetFacing: true, backendProtocol: \"tcp\", externaDnsHostname: subdomain, crossZoneEnabled: false })\n    .version(\"auto\")\n    .build(...);\n</code></pre> <p>Assuming the subdomain in the above example is <code>dev.my-domain.com</code> and wildcard is enabled for the external DNS add-on customers can now create ingress objects for host-based routing. Let's define an ingress object for <code>team-riker</code> that is currently deploying guestbook application with no ingress:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  name: ingress-riker\n  namespace: team-riker\nspec:\n  rules:\n  - host: riker.dev.my-domain.com\n    http:\n      paths:\n      - backend:\n          serviceName: guestbook-ui\n          servicePort: 80\n        path: /\n        pathType: Prefix\n</code></pre> <p>A similar ingress may be defined for <code>team-troi</code> routing to the workloads deployed by that team:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  name: ingress-troi\n  namespace: team-troi\nspec:\n  rules:\n  - host: troi.dev.my-domain.com\n    http:\n      paths:\n      - backend:\n          serviceName: guestbook-ui\n          servicePort: 80\n        path: /\n        pathType: Prefix\n</code></pre> <p>After the above ingresses applied (ideally through a GitOps engine) you can now navigate to the specified hosts respectively:</p> <p><code>http://riker.dev.my-domain.com</code> <code>http://troi.dev.my-domain.com</code></p>"},{"location":"addons/nginx/#tls-termination-and-certificates","title":"TLS Termination and Certificates","text":"<p>You can configure the NGINX add-on to terminate TLS at the load balancer and supply an ACM certificate through the platform blueprint.</p> <p>A certificate can be registered using a named resource provider.</p> <p>For convenience the framework provides a couple of common certificate providers:</p> <p>Import Certificate</p> <p>This case is used when certificate is already created and you just need to reference it with the blueprint stack:</p> <pre><code>const myCertArn = \"\";\nblueprints.EksBlueprint.builder()\n    .resourceProvider(GlobalResources.Certificate, new ImportCertificateProvider(myCertArn, \"cert1-id\"))\n    .addOns(new NginxAddOn({\n        certificateResourceName: GlobalResources.Certificate,\n        externalDnsHostname: 'my.domain.com'\n    }))\n    .teams(...)\n    .version(\"auto\")\n    .build(app, 'stack-with-cert-provider');\n</code></pre> <p>Create Certificate</p> <p>This approach is used when certificate should be created with the blueprint stack. In this case, the new certificate requires DNS validation which can be accomplished automatically if the corresponding Route53 hosted zone is provisioned (either along with the stack or separately) and registered as a resource provider.</p> <pre><code>blueprints.EksBlueprint.builder()\n    .resourceProvider(GlobalResources.HostedZone ,new ImportHostedZoneProvider('hosted-zone-id1', 'my.domain.com'))\n    .resourceProvider(GlobalResources.Certificate, new CreateCertificateProvider('domain-wildcard-cert', '*.my.domain.com', GlobalResources.HostedZone)) // referencing hosted zone for automatic DNS validation\n    .addOns(new AwsLoadBalancerControllerAddOn())\n    // Use hosted zone for External DNS\n    .addOns(new ExternalDnsAddOn({hostedZoneResources: [GlobalResources.HostedZone]}))\n    // Use certificate registered before with NginxAddon\n    .addOns(new NginxAddOn({\n        certificateResourceName: GlobalResources.Certificate,\n        externalDnsHostname: 'my.domain.com'\n    }))\n    .teams(...)\n    .version(\"auto\")\n    .build(app, 'stack-with-resource-providers');\n</code></pre>"},{"location":"addons/nginx/#functionality","title":"Functionality","text":"<ol> <li>Installs NGINX ingress controller</li> <li>Provides convenience options to integrate with AWS Load Balancer controller to leverage NLB for the load balancer</li> <li>Provides convenience options to integrate with External DNS add-on for integration with Amazon Route 53. </li> <li>Allows configuring TLS termination at the load balancer provisioned with the add-on. </li> <li>Supports standard helm configuration options.</li> </ol>"},{"location":"addons/opa-gatekeeper/","title":"What is OPA Gatekeeper? (Not Currently Supported, In Progress)","text":"<p>The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link.</p> <p>OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper. </p> <p>)</p> <p>In the context of a development platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below:</p> <ul> <li>Which users can access which resources?</li> <li>Which subnets egress traffic is allowed to?</li> <li>Which clusters a workload must be deployed to?</li> <li>Which registries binaries can be downloaded from?</li> <li>Which OS capabilities a container can execute with?</li> <li>Which times of day the system can be accessed at?</li> </ul> <p>RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster.</p>"},{"location":"addons/opa-gatekeeper/#key-terminology","title":"Key Terminology","text":"<ul> <li>OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable</li> <li>Constraint -  A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.</li> <li>Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems.</li> <li>Constraint Template - Templates that allows users to declare new constraints </li> <li>Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context</li> </ul>"},{"location":"addons/opa-gatekeeper/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.OpaGatekeeperAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that OPA Gatekeeper is running within your cluster run the following command:</p> <pre><code>kubectl get pod -n gatekeeper-system\n</code></pre> <p>You should see the following output:</p> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\ngatekeeper-audit-7c5998d4c-b5n7j                 1/1     Running   0          1d\ngatekeeper-controller-manager-5894545cc9-b86zm   1/1     Running   0          1d\ngatekeeper-controller-manager-5894545cc9-bntdt   1/1     Running   0          1d\ngatekeeper-controller-manager-5894545cc9-tb7fz   1/1     Running   0          1d\n</code></pre> <p>You will notice the <code>gatekeeper-audit-7c5998d4c-b5n7j</code> pod that is created when we deploy the <code>OpaGatekeeperAddOn</code>. The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The <code>gatekeeper-controller-manager</code> is simply there to manage the <code>OpaGatekeeperAddOn</code>. </p>"},{"location":"addons/opa-gatekeeper/#example-with-opa-gatekeeper","title":"Example with OPA Gatekeeper","text":"<p>For the purposes of operating within a platform defined by <code>EKS Blueprints</code>, we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here. In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here.</p> <p>Run the following command to create the ConstraintTemplate:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml\n</code></pre> <p>To verify that the ConstraintTemplate was created run the following command:</p> <pre><code>kubectl get constrainttemplate\n</code></pre> <p>You should see the following output:</p> <pre><code>NAME                AGE\nk8srequiredlabels   45s\n</code></pre> <p>You will notice that if you create a new namespace without any labels, the request will go through and that is because we now need to create the individual <code>Constraint CRD</code> as defined by the <code>Constraint Template</code> that we created above. Let's create the individal <code>Constraint CRD</code> using the command below:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml\n</code></pre> <p>If we then try and create a namespace by running <code>kubectl create ns test</code> (notice that we are not adding any labels) you will get the following error message:</p> <pre><code>Error from server ([all-must-have-owner] All namespaces must have an `owner` label that points to your company username): admission webhook \"validation.gatekeeper.sh\" denied the request: [all-must-have-owner] All namespaces must have an `owner` label that points to your company username\n</code></pre> <p>For more information on OPA Gatekeeper please refer to the links below:</p> <ul> <li>https://github.com/open-policy-agent</li> <li>https://open-policy-agent.github.io/gatekeeper/website/docs/</li> <li>https://github.com/open-policy-agent/gatekeeper-library </li> </ul>"},{"location":"addons/paralus/","title":"Paralus Amazon EKS Addon","text":"<p>The Paralus project is a free open-source tool that enables controlled audited access to Kubernetes infrastructure. It comes with just-in-time service account creation and user-level credential management that integrates with your existing RBAC and SSO providers of choice. Learn more by visiting the offical documentation page: https://www.paralus.io/</p> <p>Paralus Blueprint Addon deploys paralus controller on your EKS cluster using paralus construct implemented with the EKS Blueprints CDK. Detailed documentation on the same can be accessed from here.</p> <p>The Paralus AddOn deploys the following resources:</p> <ul> <li>Creates a single EKS cluster with a public endpoint (for demo purpose only) that includes a managed node group</li> <li>Deploys supporting AddOn:  AwsLoadBalancerController, VpcCni, KubeProxy, EbsCsiDriverAddOn</li> <li>Deploy Paralus on the EKS cluster</li> </ul> <p>NOTE: Paralus installs a few dependent modules such as Postgres, Kratos, and also comes with a built-in dashboard. At it's core, Paralus works atop domain-based routing, inter-service communication, and supports the AddOns mentioned above.</p>"},{"location":"addons/paralus/#these-features-makes-kubernetes-rbac-management-centralized-with-a-seamless-experience","title":"These features makes Kubernetes RBAC management centralized with a seamless experience","text":"<ul> <li>Creation of custom roles, users, and groups.</li> <li>Dynamic and immediate changing and revoking of permissions.</li> <li>Ability to control access via pre-configured roles across clusters, namespaces, projects, and more.</li> <li>Seamless integration with Identity Providers (IdPs) allowing the use of external authentication engines for users and group definitions, such as GitHub, Google, Azure AD, Okta, and others.</li> <li>Automatic logging of all user actions performed for audit and compliance purposes.</li> <li>Interact with Paralus either with a modern web GUI (default), a CLI tool called pctl, or Paralus API.</li> </ul>"},{"location":"addons/paralus/#prerequisite","title":"Prerequisite","text":"<p>You must have a domain and access to updating it's DNS records as paralus works atop domain based routing. If you need to create a domain using Amazon Route53, follow these instructions to get started.</p>"},{"location":"addons/paralus/#usage","title":"Usage","text":"<p>Run the following command to install the paralus-eks-blueprints-addon dependency in your project.</p> <pre><code>npm i @paralus/paralus-eks-blueprints-addon\n</code></pre>"},{"location":"addons/paralus/#sample-eks-blueprint-using-paralus-addon","title":"Sample EKS Blueprint using Paralus addon","text":"<pre><code>import { App } from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { ParalusAddOn } from '../dist';\n\nconst app = new App();\n\nblueprints.EksBlueprint.builder()\n     .addOns(\n        new blueprints.AwsLoadBalancerControllerAddOn(),\n        new blueprints.VpcCniAddOn(),\n        new blueprints.KubeProxyAddOn(),\n        new blueprints.EbsCsiDriverAddOn(),\n        new blueprints.CertManagerAddOn(),\n        new ParalusAddOn({\n         namespace: 'paralus-system',\n         /**\n         * Values to pass to the chart as per https://github.com/paralus/helm-charts/blob/main/charts/ztka/values.yaml.\n         */\n         // update this to your domain, as paralus works based on domain based routing\n         values: {\n            fqdn: {\n                \"domain\": \"yourdomain.com\",\n                \"hostname\": \"console-eks\",\n                \"coreConnectorSubdomain\": \"*.core-connector.eks\",\n                \"userSubdomain\": \"*.user.eks\"\n            }        \n         }\n     }))\n     .teams()\n     .version(\"auto\")\n     .build(app, 'paralus-test-blueprint');\n</code></pre>"},{"location":"addons/paralus/#addon-options","title":"AddOn Options","text":"Option Description Default <code>deploy.contour.enable</code> Deploy and use Contour as the default ingress true <code>deploy.kratos.enable</code> Deploy and use Kratos true <code>deploy.postgresql.enable</code> Deploy and use postgres database false <code>deploy.postgresql.dsn</code> DSN of your existing postgres database for paralus to use \"\" <code>deploy.fluentbit.enable</code> Deploy and use fluentbit for auditlogs with database storage \"\" <code>paralus.initialize.adminEmail</code> Admin email to access paralus <code>admin@paralus.local</code> <code>paralus.initialize.org</code> Organization name using paralus \"ParalusOrg\" <code>auditLogs.storage</code> Default storage of auditlogs \"database\" <code>fqdn.domain</code> Root domain \"paralus.local\" <code>fqdn.hostname</code> subdomain used for viewing dashboard \"console\" <code>fqdn.coreConnectorSubdomain</code> a wildcard subdomain used for controller cluster to target cluster communication \"*.core-connector\" <code>fqdn.userSubdomain</code> a wildcard subdomain used for controller cluster to end user communication \"*.user\" <code>values</code> Configuration values passed to the chart. See options. {}"},{"location":"addons/paralus/#configure-dns-settings","title":"Configure DNS Settings","text":"<p>Once Paralus is installed continue with following steps to configure DNS settings, reset default password and start using paralus</p> <p>Obtain the external ip address by executing below command against the installation <code>kubectl get svc blueprints-addon-paralus-contour-envoy -n paralus-system</code></p> <pre><code>NAME                            TYPE           CLUSTER-IP       EXTERNAL-IP                                                                     PORT(S)                         AGE\nblueprints-addon-paralus-contour-envoy         LoadBalancer   10.100.101.216   a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com   80:31810/TCP,443:30292/TCP      10m\n</code></pre> <p>Update the DNS settings to add CNAME records</p> <pre><code>    name: console-eks \n    value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n\n    name: *.core-connector.eks  \n    value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n\n    name: *.user.eks \n    value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n</code></pre> <p>Obtain your default password and reset it upon first login</p> <p><code>kubectl logs -f --namespace paralus-system $(kubectl get pods --namespace paralus-system -l app.kubernetes.io/name='paralus' -o jsonpath='{ .items[0].metadata.name }') initialize | grep 'Org Admin default password:'</code></p> <p>You can now access dashboard with http://console-eks.yourdomain.com ( refers to the hostname.domain specified during installation ), start importing clusters and using paralus.</p> <p>Note: you can also refer to this paralus eks blogpost</p>"},{"location":"addons/paralus/#support","title":"Support","text":"<p>If you have any questions about Paralus, get in touch with the team on Slack.</p> <p>Paralus is maintained and supported by Rafay</p>"},{"location":"addons/pixie/","title":"Pixie Addon","text":"<p>The <code>Pixie Addon</code> deploys Pixie on Amazon EKS using the EKS Blueprints CDK. </p> <p>Pixie is an open source observability tool for Kubernetes applications. Use Pixie to view the high-level state of your cluster (service maps, cluster resources, application traffic) and also drill-down into more detailed views (pod state, flame graphs, individual full-body application requests).</p> <p>Three features enable Pixie's magical developer experience:</p> <ul> <li> <p>Auto-telemetry: Pixie uses eBPF to automatically collect telemetry data such as full-body requests, resource and network metrics, application profiles, and more. See the full list of data sources here.</p> </li> <li> <p>In-Cluster Edge Compute: Pixie collects, stores and queries all telemetry data locally in the cluster. Pixie uses less than 5% of cluster CPU, and in most cases less than 2%.</p> </li> <li> <p>Scriptability: PxL, Pixie\u2019s flexible Pythonic query language, can be used across Pixie\u2019s UI, CLI, and client APIs.</p> </li> </ul>"},{"location":"addons/pixie/#prerequisite","title":"Prerequisite","text":"<p>You must have either:</p> <ul> <li> <p>You need to have a Pixie account and deployment key on Community Cloud for Pixie.</p> </li> <li> <p>Or a Pixie account and deployment key on a self-hosted Pixie Cloud.</p> </li> </ul>"},{"location":"addons/pixie/#usage","title":"Usage","text":"<p>Run the following command to install the pixie-eks-blueprints-addon dependency in your project.</p> <pre><code>npm i @pixie-labs/pixie-eks-blueprints-addon\n</code></pre>"},{"location":"addons/pixie/#using-deploy-key","title":"Using deploy key:","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { PixieAddOn } from '@pixie-labs/pixie-ssp-addon';\n\nconst app = new App();\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new PixieAddOn({\n        deployKey: 'pixie-deploy-key', // Create and copy from Pixie Admin UI\n    }),\n];\n\nconst version = \"auto\";\n\nnew blueprints.EksBlueprint(\n    app, \n    {\n        id: 'my-stack-name', \n        addOns,\n        version,\n    },\n    {\n        env:{\n          account: &lt;AWS_ACCOUNT_ID&gt;,\n          region: &lt;AWS_REGION&gt;, \n        }       \n    });\n</code></pre>"},{"location":"addons/pixie/#using-deploy-key-stored-in-secrets-manager","title":"Using deploy key stored in Secrets Manager:","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { PixieAddOn } from '@pixie-labs/pixie-ssp-addon';\n\nconst app = new App();\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new blueprints.addons.SecretsStoreAddOn,\n    new PixieAddOn({\n        deployKeySecretName: \"pixie-deploy-key-secret\", // Name of secret in Secrets Manager. \n    }),\n];\nconst version = \"auto\";\n\nnew blueprints.EksBlueprint(\n    app,\n    {\n        id: 'my-stack-name',\n        addOns,\n        version,\n    },\n    {\n        env:{\n          account: &lt;AWS_ACCOUNT_ID&gt;,\n          region: &lt;AWS_REGION&gt;,\n        }\n    });\n</code></pre>"},{"location":"addons/pixie/#addon-options-props","title":"Addon Options (props)","text":""},{"location":"addons/pixie/#deploykey-string-optional","title":"<code>deployKey: string</code> (optional)","text":"<p>Pixie deployment key (plain text).  Log into the Admin UI in Pixie to generate a deployment key. This attaches your Pixie deployment to your org.</p>"},{"location":"addons/pixie/#deploykeysecretname-string-optional","title":"<code>deployKeySecretName: string</code> (optional)","text":"<p>The name of the Pixie deployment key secret in Secrets Manager. The value of the key in Secrets Manager should be the deploy key in plaintext. Do not nest it inside a JSON object.</p>"},{"location":"addons/pixie/#namespace-string-optional","title":"<code>namespace?: string</code> (optional)","text":"<p>Namespace to deploy Pixie to. Default: <code>pl</code></p>"},{"location":"addons/pixie/#cloudaddr-string-optional","title":"<code>cloudAddr?: string</code> (optional)","text":"<p>The address of Pixie Cloud. This should only be modified if you have deployed your own self-hosted Pixie Cloud. By default, it will be set to Community Cloud for Pixie.</p>"},{"location":"addons/pixie/#devcloudnamespace-string-optional","title":"<code>devCloudNamespace?: string</code> (optional)","text":"<p>If running in a self-hosted cloud with no DNS configured, the namespace in which the self-hosted cloud is running. </p>"},{"location":"addons/pixie/#clustername-string-optional","title":"<code>clusterName?: string</code> (optional)","text":"<p>The name of cluster. If none is specified, a random name will be generated.</p>"},{"location":"addons/pixie/#useetcdoperator-boolean-optional","title":"<code>useEtcdOperator?: boolean</code> (optional)","text":"<p>Whether the metadata store should use etcd to store metadata, or use a persistent volume store. If not specified, the operator will deploy based on the cluster's storageClass configuration.</p>"},{"location":"addons/pixie/#pemmemorylimit-string-optional","title":"<code>pemMemoryLimit?: string</code> (optional)","text":"<p>The memory limit applied to the PEMs (data collectors). Set to 2Gi by default.</p>"},{"location":"addons/pixie/#dataaccess-fullrestrictedpiirestricted-optional","title":"<code>dataAccess?: \"Full\"|\"Restricted\"|\"PIIRestricted\"</code> (optional)","text":"<p>DataAccess defines the level of data that may be accesssed when executing a script on the cluster. If none specified, assumes full data access.</p>"},{"location":"addons/pixie/#patches-key-string-string-optional","title":"<code>patches?: [key: string]: string</code> (optional)","text":"<p>Custom K8s patches which should be applied to the Pixie YAMLs. The key should be the name of the K8s resource, and the value is the patch that should be applied.</p>"},{"location":"addons/pixie/#version-string-optional","title":"<code>version?: string</code> (optional)","text":"<p>Helm chart version.</p>"},{"location":"addons/pixie/#repository-string-release-string-chart-string-optional","title":"<code>repository?: string</code>, <code>release?: string</code>, <code>chart?: string</code> (optional)","text":"<p>Additional options for customers who may need to supply their own private Helm repository.</p>"},{"location":"addons/prometheus-node-exporter/","title":"Prometheus Node Exporter Add-on","text":"<p>This add-on installs prometheus-node-exporter.</p> <p>prometheus-node-exporter Add-on enables you to measure various machine resources such as memory, disk and CPU utilization.</p>"},{"location":"addons/prometheus-node-exporter/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.PrometheusNodeExporterAddOn()\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/prometheus-node-exporter/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you</li> <li><code>values</code>: Arbitrary values to pass to the chart. </li> <li>Standard helm configuration options.</li> </ul>"},{"location":"addons/prometheus-node-exporter/#validation","title":"Validation","text":"<p>To validate that prometheus-node-exporter is installed properly in the cluster, check if the prometheus-node-exporter namespace is created and pods are running.</p> <p>Verify if the pods are running correctly for prometheus-node-exporter in <code>prometheus-node-exporter</code> namespace. <pre><code>  kubectl get pods -n prometheus-node-exporter\n</code></pre></p>"},{"location":"addons/prometheus-node-exporter/#output","title":"Output","text":"<p>There should list pods starting with name <code>prometheus-node-exporter</code> For Eg: <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\nprometheus-node-exporter-l7s25   1/1     Running   0          105m\nprometheus-node-exporter-zh5sn   1/1     Running   0          105m\n</code></pre></p>"},{"location":"addons/prometheus-node-exporter/#functionality","title":"Functionality","text":"<p>Applies the prometheus-node-exporter add-on to an Amazon EKS cluster. </p>"},{"location":"addons/rafay/","title":"Rafay Addon - AWS EKS Blueprints for CDK","text":"<p>EKS Blueprints for CDK is a framework that makes it easy for customers to configure and deploy Rafay Kubernetes Operator as part of an EKS Blueprints cluster on Amazon EKS.</p> <p>This Addon deploys Rafay\u2019s Kubernetes Operations Platform (KOP) for Amazon Elastic Kubernetes Service (Amazon EKS) management and operations. With KOP, your platform and site-reliability engineering (SRE) teams can deploy, operate, and manage the lifecycle of Kubernetes clusters and containerized applications in both AWS Cloud and on-premises environments.</p> <p>With the Rafay Kubernetes Operations Platform, enterprises use a single operations platform to manage the lifecycle of Amazon EKS clusters and containerized applications. You can speed up the deployment of new applications to production, reduce application downtimes, and reduce security and compliance risks associated with your infrastructure.</p> <p>Rafay automates the deployment of containerized applications and enables access to Kubernetes clusters through a zero-trust connectivity model. A unified dashboard provides enterprise-grade capabilities, such as monitoring across AWS Regions, role-based access control, and governance.</p>"},{"location":"addons/rafay/#installation","title":"Installation","text":"<p>Using npm:</p> <pre><code>npm install @rafaysystems/rafay-eks-blueprints-addon\n</code></pre> <p>For a quick tutorial on EKS Blueprints, visit the Getting Started guide.</p>"},{"location":"addons/rafay/#basic-usage","title":"Basic Usage","text":"<pre><code>import * as blueprints from '@aws-quickstart/eks-blueprints';\nimport * as rafayAddOn from '@rafaysystems/rafay-eks-blueprints-addon';\nimport { Construct } from \"constructs\";\n\n\nexport default class RafayConstruct extends Construct {\n    constructor(scope: Construct, id: string) {\n        super(scope, id);\n\n        const stackId = `${id}-blueprint`;\n\n        let rafayConfig = {\n            organizationName: \"rafay-eks-org-1\", // replace with your organization Name\n            email: \"abc@example.com\", // replace with your email\n            firstName: \"John\", // replace with your first Name\n            lastName: \"Doe\", // replace with your last Name\n            password: \"P@$$word\", // replace with a password of your own\n            clusterName: \"eks-cluster-1\", // replace with the name that you want the cluster to be created in Rafay Console\n            blueprintName: \"minimal\"\n        } as rafayAddOn.RafayConfig\n\n        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new rafayAddOn.RafayClusterAddOn(rafayConfig)\n        ];\n         blueprints.EksBlueprint.builder()\n            .version(\"auto\")\n            .account(process.env.CDK_DEFAULT_ACCOUNT!)\n            .region(process.env.CDK_DEFAULT_REGION)\n            .addOns(...addOns)\n            .build(scope, stackId);\n    }\n}\n</code></pre>"},{"location":"addons/rafay/#validation","title":"Validation","text":"<pre><code>kubectl get po -n rafay-system\nNAME                                     READY   STATUS    RESTARTS   AGE\ncontroller-manager-v3-54b4945f7f-5kvwk   1/1     Running   0          4m49s\nedge-client-65995fbb78-hvp25             1/1     Running   0          6m\nrafay-connector-v3-5dc986d5d9-hb2t7      1/1     Running   0          4m49s\nrelay-agent-6f555c4dbf-hrr4b             1/1     Running   0          6m\n</code></pre> <p>Validation from Rafay Console</p> <ul> <li>Login to Rafay console with the credentials used in the blueprint</li> <li>Navigate to Clusters and click on \"cluster-name\"</li> </ul> <p></p>"},{"location":"addons/s3-csi-driver/","title":"S3 CSI Driver Addon","text":"<p>The S3 CSI Driver Addon integrates Amazon S3 with your Kubernetes cluster, allowing you to use S3 buckets as persistent storage for your applications.</p>"},{"location":"addons/s3-csi-driver/#prerequisites","title":"Prerequisites","text":"<ul> <li>The S3 bucket must be created in AWS separately as the driver uses the S3 bucket for storage, but it does not create it.</li> <li>The S3 bucket must have a bucket policy that allows the EKS cluster to access the bucket.</li> </ul>"},{"location":"addons/s3-csi-driver/#usage","title":"Usage","text":"<pre><code>import { S3CsiDriverAddon } from '@aws-quickstart/eks-blueprints';\n\nconst addOns = [\n    new S3CsiDriverAddon({\n        s3BucketName: 'my-s3-bucket',\n    }),\n    // other addons\n];\n\nconst blueprint = EksBlueprint.builder()\n    .addOns(...addOns)\n    .build(app, 'my-stack');\n</code></pre>"},{"location":"addons/s3-csi-driver/#configuration","title":"Configuration","text":"<p>You can customize the S3 CSI Driver Addon by passing configuration options:</p> <pre><code>new S3CsiDriverAddon({\n    s3BucketName: 'my-s3-bucket',\n});\n</code></pre>"},{"location":"addons/s3-csi-driver/#use-in-eks-cluster","title":"Use in EKS Cluster","text":"<p>Once installed, you can create PersistentVolume and PersistentVolumeClaim resources that use the S3 CSI Driver:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n    name: s3-pv\nspec:\n    capacity:\n        storage: 5Gi\n    accessModes:\n        - ReadWriteOnce\n    csi:\n        driver: s3.csi.aws.com\n        volumeHandle: my-s3-bucket\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: s3-pvc\nspec:\n    accessModes:\n        - ReadWriteOnce\n    resources:\n        requests:\n            storage: 5Gi\n    volumeName: s3-pv\n</code></pre>"},{"location":"addons/s3-csi-driver/#references","title":"References","text":"<ul> <li>Amazon S3 CSI Driver Source</li> <li>Amazon EKS S3 CSI Driver Documentation</li> </ul>"},{"location":"addons/secrets-store/","title":"Secrets Store Add-on","text":"<p>The Secrets Store Add-on provisions the AWS Secrets Manager and Config Provider(ASCP) for Secret Store CSI Driver on your EKS cluster. With ASCP, you now have a plugin for the industry-standard Kubernetes Secrets Store Container Storage Interface (CSI) Driver used for providing secrets to applications operating on Amazon Elastic Kubernetes Service.</p> <p>With ASCP, you can securely store and manage your secrets in AWS Secrets Manager or AWS Systems Manager Parameter Store and retrieve them through your application workloads running on Kubernetes. You no longer have to write custom code for your applications.</p>"},{"location":"addons/secrets-store/#usage","title":"Usage","text":""},{"location":"addons/secrets-store/#indexts","title":"<code>index.ts</code>","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.SecretsStoreAddOn();\n\n/* Setup application team with secrets\n * Here we are generating a new SecretManager secret for AuthPassword\n * We are also looking up a pre-existing secret in Parameter Store called GITHUB_TOKEN\n */\n\nexport class TeamBurnham extends ApplicationTeam {\n    constructor(scope: Construct) {\n        super({\n            name: \"burnham\",\n            users: getUserArns(scope, \"team-burnham.users\"),\n            teamSecrets: [\n                {\n                    secretProvider: new blueprints.GenerateSecretManagerProvider('AuthPassword'),\n                    kubernetesSecret: {\n                        secretName: 'auth-password',\n                        data: [\n                            {\n                                key: 'password'\n                            }\n                        ]\n                    }\n                },\n                {\n                    secretProvider: new blueprints.LookupSsmSecretByAttrs('GITHUB_TOKEN', 1),\n                    kubernetesSecret: {\n                        secretName: 'github'\n                    }\n                }\n            ]\n        });\n    }\n}\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .teams(new TeamBurnham(app))\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/secrets-store/#functionality","title":"Functionality","text":"<ol> <li>Installs the Kubernetes Secrets Store CSI Driver in the <code>kube-system</code> namespace.</li> <li>Installs AWS Secrets Manager and Config Provider for Secret Store CSI Driver in the <code>kube-system</code> namespace.</li> <li>Creates an IAM access policy for scoped down to just the secrets the provided namespace should have access to.</li> <li>Updates IAM roles for service accounts <code>[team-name]-sa</code> for policies to grant read access to the provided secrets.</li> <li>Creates a SecretProviderClass <code>[team-name]-aws-secrets</code> which tells the AWS provider which secrets can be mounted in an application pod in the provided namespace.</li> </ol>"},{"location":"addons/secrets-store/#security-considerations","title":"Security Considerations","text":"<p>The AWS Secrets Manger and Config Provider provides compatibility for legacy applications that access secrets as mounted files in the pod. Security conscious applications should use the native AWS APIs to fetch secrets and optionally cache them in memory rather than storing them in the file system.</p>"},{"location":"addons/secrets-store/#renaming-mounted-kubernetes-secrets","title":"Renaming Mounted Kubernetes Secrets","text":"<p>By default, mounted Kubernetes Secrets inherit the name of their corresponding AWS Secret or SSM Parameter. If the AWS Secret or Parameter name contains slashes (\"/\" or \"\\\"), they will be replaced by underscores by the CSI Driver.  </p> <p>This can result in undesirable secret names being mounted to your pods, so as a workaround the driver also offers an aliasing feature.</p> <pre><code>{\n    secretProvider: new blueprints.LookupSsmSecretByAttrs('/path/to/my/parameter', 1),\n    kubernetesSecret: {\n        secretName: 'my_parameter', // not respected when mounting the secret directly to the pod, and will result in sync errors during pod init\n        secretAlias: 'my_parameter', // respected during mounting. File will be called \"my_parameter\" instead of \"_path_to_my_parameter\"\n    }\n}\n</code></pre> <p>NOTE: <code>secretAlias</code> is only applicable to secrets that are mounted to a pod. In these scenarios, <code>secretName</code> should match the name of the Secret or Parameter in AWS.</p>"},{"location":"addons/secrets-store/#example","title":"Example","text":"<p>After the Blueprint stack is deployed you can test consuming the secret from within a <code>deployment</code>.</p> <p>This sample <code>deployment</code> shows how to consume the secrets as mounted volumes as well as environment variables.</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; test-secrets.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-deployment\n  labels:\n    app: myapp\n  namespace: team-burnham\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      serviceAccountName: burnham-sa\n      volumes:\n      - name: secrets-store-inline\n        csi:\n          driver: secrets-store.csi.k8s.io\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: burnham-aws-secrets\n      containers:\n      - name: test-secrets\n        image: public.ecr.aws/ubuntu/ubuntu:latest\n        command: [ \"/bin/bash\", \"-c\", \"--\" ]\n        args: [ \"while true; do sleep 30; done;\" ]\n        resources:\n          limits:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n        env:\n          - name: PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: auth-password\n                key: password\n          - name: GITHUB_TOKEN\n            valueFrom:\n              secretKeyRef:\n                name: github\n                key: GITHUB_TOKEN\n        volumeMounts:\n          - name: secrets-store-inline\n            mountPath: /mnt/secrets-store\n            readOnly: true\nEOF\n</code></pre> <p>The values for <code>serviceAccountName</code> and the <code>secretProviderClass</code> shown in the example above are obtained from CloudFormation outputs of the blueprint stack shown in the screenshot below as <code>burnhamsa</code> and <code>teamburnhamsecretproviderclass</code>.</p> <p></p> <p>Apply the manifest.</p> <pre><code>$ kubectl apply -f test-secrets.yaml\ndeployment.apps/app-deployment created\n</code></pre> <p>Test that kubernetes secret <code>burnham-github-secrets</code> was created.</p> <pre><code>kubectl get secrets -n team-burnham\nNAME                     TYPE                                  DATA   AGE\nauth-password            Opaque                                1      19s\nburnham-sa-token-fqjqw   kubernetes.io/service-account-token   3      64m\ndefault-token-7fn69      kubernetes.io/service-account-token   3      64m\ngithub                   Opaque                                1      19s\n</code></pre> <p>Test that the deployment has completed and the pod is running successfully.</p> <pre><code>$ kubectl get pods -n team-burnham\nNAME                              READY   STATUS    RESTARTS   AGE\napp-deployment-6867fc6bd6-jzdwh   1/1     Running   0          46s\n</code></pre> <p>Next, test whether the secret <code>PASSWORD</code> is available as an environment variable from within the <code>app-deployment</code> pod.</p> <pre><code>$ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $PASSWORD\n\nXXXXXXXXXXXXXXXXXX\n</code></pre> <p>Test whether <code>GITHUB_TOKEN</code> is available as an environment variable from within the <code>app-deployment</code> pod.</p> <pre><code>$ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $GITHUB_TOKEN\n\nghp_XXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre>"},{"location":"addons/ssm-agent/","title":"SSM Agent Add-on","text":"<p>This add-on uses the Kubernetes DaemonSet resource type to install AWS Systems Manager Agent (SSM Agent) on all worker nodes, instead of installing it manually or replacing the Amazon Machine Image (AMI) for the nodes. DaemonSet uses a CronJob on the worker node to schedule the installation of SSM Agent.</p> <p>A common use-case for installing SSM Agent on the worker nodes is to be able open a terminal session on an instance without the need to create a bastion instance and without having to install SSH keys on the worker nodes.</p> <p>The AWS Identity and Access Management (IAM) managed role AmazonSSMManagedInstanceCore provides the required permissions for SSM Agent to run on EC2 instances. This role is automatically attached to the instances when this add-on is enabled.</p> <p>Limitations</p> <ul> <li> <p>This add-on isn't applicable to AWS Fargate, because DaemonSets aren't supported on the Fargate platform.</p> </li> <li> <p>This add-on applies only to Linux-based worker nodes.</p> </li> <li> <p>The DaemonSet pods run in privileged mode. If the Amazon EKS cluster has a webhook that blocks pods in privileged mode, the SSM Agent will not be installed.</p> </li> <li> <p>Only latest version of SSM Agent add-on can be installed.</p> </li> </ul>"},{"location":"addons/ssm-agent/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.SSMAgentAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>To validate that SSM Agent is running on worker node instance:</p> <p>Pre-Requisite: Install the Session Manager plugin for the AWS CLI as per instructions for your OS.</p> <ol> <li>Get the EC2 Instance Id of a worker node</li> </ol> <pre><code>instance_id=$(kubectl get nodes -o custom-columns=NAME:.metadata.name,INSTANCEID:.spec.providerID | awk -F/ 'FNR == 2 {print $5}')\n</code></pre> <ol> <li>Use the <code>start-session</code> api to see if you can open a terminal into the instance</li> </ol> <pre><code>aws ssm start-session --target $instance_id\n</code></pre>"},{"location":"addons/ssm-agent/#use-case-private-clusters","title":"Use Case: Private Clusters","text":"<p>If you are disabling public access for your EKS cluster endpoint such that the cluster endpoint is provisioned as private only i.e <code>endpointPublicAccess=false</code> and <code>endpointPrivateAccess=true</code>, then you can use one of the worker nodes as a TCP jump box to your EKS cluster api.</p> <p>To set up a TCP tunnel with your worker node as a jump box:</p> <ol> <li>Use SSM <code>send-command</code> api to create a TCP tunnel to Cluster API using <code>socat</code>:</li> </ol> <pre><code># Get the Cluster API endpoint first\nCLUSTER_NAME=&lt;insert your cluster name, e.g. blueprint-construct-dev&gt;\n\nCLUSTER_API=$(aws eks describe-cluster --name $CLUSTER_NAME | jq -r '.cluster.endpoint' | awk -F/ '{print $3}')\n\naws ssm send-command \\\n  --instance-ids $instance_id \\\n  --document-name \"AWS-RunShellScript\" \\\n  --comment \"tcp tunnel to cluster api\" \\\n  --parameters commands=\"nohup sudo socat TCP-LISTEN:443\\,fork TCP:$CLUSTER_API:443 &amp;\"\n</code></pre> <ol> <li>Update <code>~/.kube/config</code> to use port 8443 instead of 443 as your local host may not allow you to bind port 443 (depending on your machine network configuration you may not be able to bind to port 443. In such a case, you can bind to port 8443)</li> </ol> <pre><code>sed -i -e \"s/https:\\/\\/$CLUSTER_API/https:\\/\\/$CLUSTER_API:8443/\" ~/.kube/config\n</code></pre> <ol> <li>Update /etc/hosts so that <code>$CLUSTER_API</code> resolves to <code>127.0.0.1</code>.</li> </ol> <p><pre><code>sudo echo \"127.0.0.1 $CLUSTER_API\" &gt;&gt; /etc/hosts\n</code></pre> 4. Start an SSM session to forward remote port 443 to local port 8443:</p> <pre><code>aws ssm start-session \\\n  --target $instance_id \\\n  --document-name AWS-StartPortForwardingSession \\\n  --parameters '{\"portNumber\":[\"443\"], \"localPortNumber\":[\"8443\"]}'\n</code></pre> <p>At this point you should be able to execute <code>kubectl ...</code> commands against your cluster API from another terminal window.</p> <p>Limitations</p> <ul> <li>This approach cannot be used for Fargate or BottleRocket based providers.</li> <li><code>socat</code> is available on EKS optimized AMI out of the box but may have to be explicitly installed on others AMIs.</li> </ul>"},{"location":"addons/upbound-universal-crossplane/","title":"Upbound Universal Crossplane (UXP) Amazon EKS Add-on","text":"<p>The <code>Upbound Universal Crossplane Amazon EKS Add-on</code> allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Crossplane distribution.</p> <p>Upbound Universal Crossplane (UXP) is Upbound's official enterprise-grade Crossplane distribution. It's free, open source, and fully conformant with upstream Crossplane. UXP is hardened and tested by Upbound so customers can confidently deploy control plane architectures to production. Connect UXP to Upbound Cloud is enabled with a free Upbound account for simplified management.</p>"},{"location":"addons/upbound-universal-crossplane/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nnew blueprints.UpboundCrossplaneAddOn({\n    clusterAccessRole: blueprints.getNamedResource(\"AdminRole\")\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>iguration Options</p> <ul> <li><code>createNamespace</code>: (boolean) If you want CDK to create the namespace for you.</li> <li><code>values</code>: Arbitrary values to pass to the chart. Refer to the FluxCD Helm Chart documentation for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options).</li> </ul>"},{"location":"addons/upbound-universal-crossplane/#validation","title":"Validation","text":"<p>To validate that Upbound Crossplane add-on is installed properly: <pre><code>kubectl get all -n upbound-system\n\n# Output\nNAME                                        READY   STATUS    RESTARTS      AGE\npod/crossplane-776449cbc7-t9jnn             1/1     Running   0             25m\npod/upbound-bootstrapper-844f84fcf4-xgpj9   1/1     Running   0             25m\npod/xgql-55d7475b48-dlfgc                   1/1     Running   2 (25m ago)   25m\n\nNAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/upbound-agent   ClusterIP   172.20.131.84    &lt;none&gt;        6443/TCP   25m\nservice/xgql            ClusterIP   172.20.138.213   &lt;none&gt;        443/TCP    25m\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/crossplane             1/1     1            1           25m\ndeployment.apps/upbound-bootstrapper   1/1     1            1           25m\ndeployment.apps/xgql                   1/1     1            1           25m\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/crossplane-776449cbc7             1         1         1       25m\nreplicaset.apps/upbound-bootstrapper-844f84fcf4   1         1         1       25m\nreplicaset.apps/xgql-55d7475b48                   1         1         1       25m\n</code></pre></p>"},{"location":"addons/upbound-universal-crossplane/#functionality","title":"Functionality","text":"<p>Applies the Upbound Universal Crossplane add-on to an Amazon EKS cluster.</p>"},{"location":"addons/velero/","title":"Velero Add-On","text":"<p>The Velero(formerly Heptio Ark) is a tool to backup and restore your Kubernetes cluster resources and persistent volumes. Velero lets you :</p> <ul> <li>Take backups of your cluster and restore in case of loss.</li> <li>Migrate cluster resources to other clusters.</li> <li>Replicate your production cluster to development and testing clusters.</li> </ul> <p>Velero consists of:</p> <ul> <li>A server that runs on your cluster</li> <li>A command-line client that runs locally</li> </ul> <p>The Velero add-on installs Velero on Amazon EKS. By default it will create a private encrypted S3 Bucket to be the Velero backup destination. It leverages IAM Role for Service Accounts (IRSA) feature to enable Velero pod to make API calls with S3 and EC2 natively without the need to use kube2iam or AWS credentials. </p>"},{"location":"addons/velero/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.VeleroAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/velero/#functionality","title":"Functionality","text":"<ol> <li>By default create a private S3 bucket (blocking all public access) with SSE-KMS encryption with AWS Managed Key from KMS(Encryption At Rest) for the Velero Backup destination.</li> <li>Configure S3 Bucket policy to enable encryption in transit.</li> <li>Create the IAM Role for Service Account for Velero pod to make API calls to AWS S3 and EC2 to backup and restore.</li> <li>Preset Velero Helm Chart Values.</li> <li>Allow users to pass Velero Helm Chart Values for customization purposes. </li> <li>Supports standard helm configuration options.</li> </ol>"},{"location":"addons/velero/#limitations","title":"Limitations","text":"<ol> <li>Velero has a known bug for support of S3 with SSE-KMS encryption with Customer master key (CMK). Please refer to Velero GitHub Issue #83.</li> <li>As a result of #1, Velero is unable to leverage the S3 Bucket Key feature which requires using AWS CMK to achieve \"reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS.\"</li> </ol>"},{"location":"addons/velero/#testing-velero-functionality","title":"Testing Velero Functionality","text":"<p>The following steps will help test the backup and restore Kuberenetes resources from Velero</p> <ol> <li>Deploy a sample app as deployment into a newly created namespace</li> <li>Backup the sample app from the namespace</li> <li>Delete the sample app namespace</li> <li>Restore the sample app</li> </ol>"},{"location":"addons/velero/#deploy-a-sample-app-into-a-newly-created-namespace","title":"Deploy a sample app into a newly created namespace","text":"<ul> <li>Properly configure your <code>kubeconfig</code> to use <code>kubectl</code> command. By successfully deploying the EKS cluster, you can run commands to set up your <code>kubeconfig</code> correctly by: <code>aws eks update-kubeconfig --name &lt;EKS_Cluster_Name&gt;  --region &lt;AWS_Region&gt; --role-arn arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:role/&lt;IRSA_Role_Name&gt;</code></li> </ul> <p><pre><code># Create the test01 namespace\n$ kubectl create ns test01\nnamespace/test01 created\n\n# Deploy the Nginx Stateless applications on to namespace test01\n$ kubectl apply -f https://k8s.io/examples/application/deployment.yaml -n test01\ndeployment.apps/nginx-deployment created\n\n# Check the nginx pods\n$ kubectl get pods -n test01\nNAME                                READY   STATUS    RESTARTS   AGE\nnginx-deployment-66b6c48dd5-qf7lc   1/1     Running   0          53s\nnginx-deployment-66b6c48dd5-wvxjx   1/1     Running   0          53s\n\n# Deploy the Stateful Nginx Application with PV to namespace nginx-example\n$ kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/velero/main/examples/nginx-app/with-pv.yaml\nnamespace/nginx-example created\npersistentvolumeclaim/nginx-logs created\ndeployment.apps/nginx-deployment created\nservice/my-nginx created\n\n# Check the application and PV\n$ kubectl get pods -n nginx-example\nNAME                               READY   STATUS    RESTARTS   AGE\nnginx-deployment-66689547d-4mqsd   2/2     Running   0          106s\nhaofeif@a483e70791e6 ~ $ kubectl get pv -n nginx-example\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS   REASON   AGE\npvc-73498192-6571-4e31-b455-e7e7efbf2fb7   1Gi        RWO            Delete           Bound    nginx-example/nginx-logs   gp2                     110s\n\n## EBS Volume got created \n</code></pre> </p>"},{"location":"addons/velero/#backup-the-sample-app-from-the-namespace","title":"Backup the sample app from the namespace","text":"<ul> <li>To install Velero client Cli, please refer to the User Guide</li> <li>The backup will be created into the S3 bucket created or specified by the users. </li> </ul> <p><pre><code># Create the backup for stateless app at namespace test01\n$ velero backup create test01 --include-namespaces test01\nBackup request \"test01\" submitted successfully.\nRun `velero backup describe test01` or `velero backup logs test01` for more details.\n\n# Check for the backup\n$ velero backup get\nNAME       STATUS      ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR\ntest01     Completed   0        0          2021-09-20 12:40:37 +1000 AEST   29d       default            &lt;none&gt;\n\n# Check the logs of the backup\n$ velero backup logs test01\ntime=\"2021-09-19T02:40:37Z\" level=info msg=\"Setting up backup temp file\" backup=velero/test01 logSource=\"pkg/controller/backup_controller.go:556\"\ntime=\"2021-09-19T02:40:37Z\" level=info msg=\"Setting up plugin manager\" backup=velero/test01 logSource=\"pkg/controller/backup_controller.go:563\"\ntime=\"2021-09-19T02:40:37Z\" level=info msg=\"Getting backup item actions\" backup=velero/test01 logSource=\"pkg/controller/backup_controller.go:567\"\n...\n\n# Check the backup location, the Access mode shows the S3 bucket name and its folders.\n$ velero backup-location get\nNAME      PROVIDER   BUCKET/PREFIX                                                                         PHASE       LAST VALIDATED                   ACCESS MODE   DEFAULT\ndefault   aws        my-stack-name-mystacknamevelerobackupxxx/velero/my-stack-name   Available   2021-09-20 12:29:35 +1000 AEST   ReadWrite     true\n\n# Screenshot of the S3 bucket folder for the backup test01\n</code></pre> </p> <p><pre><code># Create the Backup with the PV at namespace nginx-example\n$ velero backup create nginx-backup --include-namespaces nginx-example\nBackup request \"nginx-backup\" submitted successfully.\nRun `velero backup describe nginx-backup` or `velero backup logs nginx-backup` for more details.\n\n## Check the backup status\n$ velero backup get\nNAME           STATUS      ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR\nnginx-backup   Completed   0        0          2021-09-20 12:37:36 +1000 AEST   29d       default            &lt;none&gt;\n\n# Screenshot of the S3 bucket folder for the backup nginx-backup (with PV)\n</code></pre> </p>"},{"location":"addons/velero/#delete-the-sample-app-namespace","title":"Delete the sample app namespace","text":"<pre><code># Delete the namespace test01\n$ kubectl delete ns test01\n\n# Delete the namespace nginx-example\n$ kubectl delete ns nginx-example\n\n# Note: Because the default reclaim policy for dynamically-provisioned PVs is \u201cDelete\u201d, these commands should trigger AWS to delete the EBS Volume that backs the PV. Deletion is asynchronous, so this may take some time. \n</code></pre>"},{"location":"addons/velero/#restore-the-sample-app","title":"Restore the sample app","text":"<p><pre><code># Restore from the backup of test01\n$ velero restore create test01 --from-backup test01\n\nRestore request \"test01\" submitted successfully.\nRun `velero restore describe test01` or `velero restore logs test01` for more details.\n\n# Check the restore status of test01\n$ velero restore get\nNAME     BACKUP   STATUS       STARTED                          COMPLETED   ERRORS   WARNINGS   CREATED                          SELECTOR\ntest01   test01   Completed   2021-09-20 12:41:38 +1000 AEST   &lt;nil&gt;       0        0          2021-09-20 12:41:36 +1000 AEST   &lt;none&gt;\n\n# Check the stateless application restore completed\n$ kubectl get pods -n test01\nNAME                                READY   STATUS    RESTARTS   AGE\nnginx-deployment-66b6c48dd5-qf7lc   1/1     Running   0          53s\nnginx-deployment-66b6c48dd5-wvxjx   1/1     Running   0          53s\n\n# Restore from the backup of nginx-backup (With PV) after confirming EBS volume has been deleted successfully\n$ velero restore create --from-backup nginx-backup\n\n# Check the Restore status\n$ velero restore get\nNAME                          BACKUP         STATUS      STARTED                          COMPLETED                        ERRORS   WARNINGS   CREATED                          SELECTOR\nnginx-backup-20210920124336   nginx-backup   Completed   2021-09-20 12:43:42 +1000 AEST   2021-09-20 12:43:45 +1000 AEST   0        0          2021-09-20 12:43:40 +1000 AEST   &lt;none&gt;\n\n# Check the status of pod and PV in namespace nginx-example\n$ kubectl get pods -n nginx-example\nNAME                               READY   STATUS    RESTARTS   AGE\nnginx-deployment-66689547d-4mqsd   2/2     Running   0          2m12s\n\n$ kubectl get pv -n nginx-example\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS   REASON   AGE\npvc-73498192-6571-4e31-b455-e7e7efbf2fb7   1Gi        RWO            Delete           Bound    nginx-example/nginx-logs   gp2                     2m22s\n\n# EBS Volume is back\n</code></pre> </p>"},{"location":"addons/vpc-cni/","title":"VPC CNI Amazon EKS Add-on","text":"<p>The <code>VPC CNI Amazon EKS Add-on</code> adds support for <code>Amazon VPC Container Network Interface (CNI)</code> plugin.</p> <p>Amazon EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. For more information, see Pod networking (CNI).</p> <p>Installing VPC CNI as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable.</p> <p>Amazon EKS automatically installs VPC CNI as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.</p> <p>Amazon EKS VPC CNI Addon now supports advanced configurations which means we can now pass configuration values as a JSON blob for setting up advanced configurations in Amazon VPC CNI. Please refer Amazon EKS add-ons: Advanced configuration for more informatoion.</p>"},{"location":"addons/vpc-cni/#prerequisite","title":"Prerequisite","text":"<ul> <li>Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.</li> </ul>"},{"location":"addons/vpc-cni/#usage","title":"Usage","text":"<p>This add-on can be used with three different patterns :</p> <p>Pattern # 1 : Simple and Easy. With all default values. This pattern wont create custom networking or setup any environment variables as part of configuration Values.</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.VpcCniAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern # 2 : Custom networking with new Secondary CIDR ranges. This pattern will first create Secondary CIDRs and Secondary Subnets with specified range of CIDRs as shown below in <code>resourceProvider</code> command. Then the VPC CNI addon will setup custom networking based on the parameters <code>awsVpcK8sCniCustomNetworkCfg</code>, <code>eniConfigLabelDef: \"topology.kubernetes.io/zone\"</code> for your Amazon EKS cluster workloads with created secondary subnet ranges to solve IP exhaustion.</p> <p>Note:  - When you are passing secondary CIDRs to the VPC resource provider, then we create secondary subnets for the customer and register them under names secondary-cidr-subnet-${order} with the resource providers.</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.VpcCniAddOn({\n  customNetworkingConfig: {\n      subnets: [\n          blueprints.getNamedResource(\"secondary-cidr-subnet-0\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-1\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-2\"),\n      ]   \n  },\n  awsVpcK8sCniCustomNetworkCfg: true,\n  eniConfigLabelDef: 'topology.kubernetes.io/zone'\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .resourceProvider(blueprints.GlobalResources.Vpc, new VpcProvider(undefined,\"100.64.0.0/24\",[\"100.64.0.0/25\",\"100.64.0.128/26\",\"100.64.0.192/26\"],))\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern # 3 : Custom networking with custom VPC and Secondary Subnets. This pattern will use the custom VPC ID and Secondary subnet IDs passed by the user to create the blueprints stack. Then the VPC CNI addon will setup custom networking based on the parameters <code>awsVpcK8sCniCustomNetworkCfg</code>, <code>eniConfigLabelDef: \"topology.kubernetes.io/zone\"</code> for your Amazon EKS cluster workloads with passed secondary subnet ranges to solve IP exhaustion. </p> <p>Note :  - When you are passing secondary subnet ids to the VPC resource provider, then we register them under names secondary-cidr-subnet-${order} with the resource providers. - When you are passing your own Secondary subnets using this pattern, Please make sure the tag <code>Key: kubernetes.io/role/internal-elb\", Value: \"1\"</code> is added to your secondary subnets. Please register your secondary subnets in any arbitary name as shown below in <code>resourceProvider</code>.Please check out Custom Networking Tutorial to learn how custome networking is manually setup on your Amazon EKS cluster.</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.VpcCniAddOn({\n  customNetworkingConfig: {\n      subnets: [\n          blueprints.getNamedResource(\"secondary-cidr-subnet-0\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-1\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-2\"),\n      ]   \n  },\n  awsVpcK8sCniCustomNetworkCfg: true,\n  eniConfigLabelDef: 'topology.kubernetes.io/zone'\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .resourceProvider(blueprints.GlobalResources.Vpc, new VpcProvider(yourVpcId))\n  .resourceProvider(\"secondary-cidr-subnet-0\", new LookupSubnetProvider(subnet1Id)\n  .resourceProvider(\"secondary-cidr-subnet-1\", new LookupSubnetProvider(subnet2Id)\n  .resourceProvider(\"secondary-cidr-subnet-2\", new LookupSubnetProvider(subnet3Id)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/vpc-cni/#vpc-cni-service-account-and-irsa-config","title":"VPC-CNI Service Account and IRSA config","text":"<p>VPC CNI add-on supports creation of an IRSA role for the add-on if the customer supplies managed policies for the add-on configuration (i.e. if the <code>serviceAccountPolicies</code> field is populated). </p> <p>Example:</p> <pre><code>const vpcCniAddOn = new VpcCniAddOn({\n  serviceAccountPolicies: [iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonEKS_CNI_Policy\")]\n});\n</code></pre> <p>The example above is using the CNI policy sufficient for ENI allocation for IPv4. Please consult the AWS documentation for IPv6 policies. </p> <p>Note: when using IRSA account with the VPC-CNI plug-in, the node instance role does not need the AmazonEKS_CNI_Policy. It can be removed from the node instance role by supplying a custom role. </p> <p>Example blueprint with Node Instance Role without CNI policy:</p> <pre><code>const nodeRole = new blueprints.CreateRoleProvider(\"blueprint-node-role\", new iam.ServicePrincipal(\"ec2.amazonaws.com\"),\n  [\n      iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonEKSWorkerNodePolicy\"),\n      iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonEC2ContainerRegistryReadOnly\"),\n      iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonSSMManagedInstanceCore\")\n  ]);\n\nconst clusterProvider = new blueprints.GenericClusterProvider({\n  version: KubernetesVersion.V1_29,\n  managedNodeGroups: [\n    {\n      id: \"mng1\",\n      instanceTypes: [new ec2.InstanceType('m5.4xlarge')],\n      nodeRole: blueprints.getNamedResource(\"node-role\") as iam.Role\n    }\n  ]\n});\n\nblueprints.EksBlueprint.builder()\n    .addOns(...addOns)\n    .resourceProvider(\"node-role\", nodeRole)\n    .clusterProvider(clusterProvider)\n    .build(scope, \"blueprint\", props);\n</code></pre>"},{"location":"addons/vpc-cni/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>version</code>: Pass in the optional vpc-cni plugin version compatible with kubernetes-cluster version as shown below <pre><code># Assuming cluster version is 1.19, below command shows versions of the vpc-cni add-on available for the specified cluster's version.\naws eks describe-addon-versions \\\n    --addon-name vpc-cni \\\n    --kubernetes-version 1.25 \\\n    --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\\n    --output text\n# Output\nv1.13.2-eksbuild.1\nFalse\nv1.13.0-eksbuild.1\nFalse\nv1.12.6-eksbuild.2\nFalse\nv1.12.6-eksbuild.1\nFalse\nv1.12.5-eksbuild.2\nFalse\nv1.12.5-eksbuild.1\nFalse\nv1.12.2-eksbuild.1\nTrue\nv1.12.0-eksbuild.2\nFalse\nv1.11.5-eksbuild.1\nFalse\nv1.11.4-eksbuild.3\nFalse\nv1.10.4-eksbuild.3\nFalse\nv1.0.0-eksbuild.preview\nFalse\n</code></pre></li> <li>enableNetworkPolicy - set to <code>true</code> enables Kubernetes Network policy. This setting is introduced on <code>1.14.0</code> or latter.</li> </ul> <p>For example: <pre><code>new blueprints.addons.VpcCniAddOn({ enableNetworkPolicy: true, version: 'v1.15.0-eksbuild.2' }) \n</code></pre>    - enableWindowsIpam - set to <code>true</code> enables Windows support for EKS cluster. </p> <p>For example: <pre><code>new blueprints.addons.VpcCniAddOn({ enableWindowsIpam: true, version: 'v1.15.0-eksbuild.2' }) \n</code></pre></p>"},{"location":"addons/vpc-cni/#validation","title":"Validation","text":"<p>To validate that vpc-cni add-on is running, ensure that the pod is in Running state.</p> <pre><code>$ kubectl -n kube-system get ds aws-node -oyaml|grep AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG -A1\n</code></pre> <pre><code># Assuming cluster-name is my-cluster, below command shows the version of vpc-cni add-on installed. Check if it is same as the version installed via EKS add-on\naws eks describe-addon \\\n    --cluster-name my-cluster \\\n    --addon-name vpc-cni \\\n    --query \"addon.addonVersion\" \\\n    --output text\n# Output\nv1.13.0-eksbuild.1\n</code></pre>"},{"location":"addons/vpc-cni/#functionality","title":"Functionality","text":"<p>Applies VPC CNI add-on to Amazon EKS cluster. </p>"},{"location":"addons/vpc-cni/#custom-networking","title":"Custom Networking","text":"<p>We are installing VPC CNI addon of CDK EKS blueprints ahead of your data plane node creation so we dont have a need to cordon and drain the nodes to gracefully shutdown the Pods and then terminate the nodes. VPC CNI addon will be first installed on Amazon EKS Control Plane, then data plan nodes will be deployed and then all the other day 2 operational addons you have opted in will be installed. This solves IP exhaustion via custom networking of VPC CNI addon out of the box without any manual intervention.</p> <p>Please check our Amazon EKS Best Practices Guide for Networking for more information on custom networking.</p>"},{"location":"addons/vpc-cni/#references","title":"References","text":"<ul> <li>Reference amazon-vpc-cni-k8s to learn more about different VPC CNI configuration Values</li> <li>Reference VpcCniAddon to learn more about this addon</li> <li>Reference Amazon EKS Best Practices Guide for Networking to learn about Amazon EKS networking best practices</li> <li>Reference Custom Networking Tutorial to learn how custome networking is manually setup on your Amazon EKS cluster.</li> <li>Reference Configure your cluster for Kubernetes network policies to learn about how to enable and use kubernetes Network policies</li> </ul>"},{"location":"addons/xray-adot-addon/","title":"AWS X-Ray ADOT Add-on","text":"<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for X-Ray which receives traces from the application and sends the same to X-Ray console. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy.</p> <p>This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup X-Ray for remote write traces.</p> <p>For more information on the add-on, please review the user guide.</p>"},{"location":"addons/xray-adot-addon/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>cert-manager</code> Blueprints add-on.</li> <li><code>adot</code> EKS Blueprints add-on.</li> </ul>"},{"location":"addons/xray-adot-addon/#usage","title":"Usage","text":"<p>This add-on can used with two different patterns :</p> <p>Pattern # 1 : Simple and Easy - Using all default property values. This pattern deploys an ADOT collector in the <code>default</code> namespace with <code>deployment</code> as the mode to write traces to X-Ray console.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.XrayAdotAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>With the same pattern, to deploy ADOT collector in non-default namespace:</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.XrayAdotAddOn({\n                ampPrometheusEndpoint: ampWorkspace.attrPrometheusEndpoint,\n                namespace: 'adot'\n              }),\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Pattern # 2 : Overriding Property value for different deployment Modes. This pattern deploys an ADOT collector on the namespace specified in <code>namespace</code>, name specified in <code>name</code> with <code>daemonset</code> as the mode to X-Ray console. Deployment mode can be overridden to any of these values - <code>deployment</code>, <code>daemonset</code>, <code>statefulset</code>, <code>sidecar</code>.</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.XrayAdotAddOn({\n    deploymentMode: XrayDeploymentMode.DAEMONSET,\n    namespace: 'default',\n    name: 'adot-collector-xray'\n});\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"addons/xray-adot-addon/#validation","title":"Validation","text":"<p>To validate that X-Ray add-on is installed properly, ensure that the required kubernetes resources are running in the cluster</p> <pre><code>kubectl get all -n default\n</code></pre>"},{"location":"addons/xray-adot-addon/#output","title":"Output","text":"<pre><code>NAME                                                 READY   STATUS        RESTARTS   AGE\npod/otel-collector-xray-collector-6fc44d9bbf-xdfpg   1/1     Running       0          6m44s\npod/traffic-generator-86f86d84cc-s78wv               0/1     Terminating   0          128m\n\nNAME                                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nservice/kubernetes                                 ClusterIP   172.20.0.1      &lt;none&gt;        443/TCP             3d\nservice/otel-collector-xray-collector              ClusterIP   172.20.83.240   &lt;none&gt;        4317/TCP,4318/TCP   6m46s\nservice/otel-collector-xray-collector-headless     ClusterIP   None            &lt;none&gt;        4317/TCP,4318/TCP   6m46s\nservice/otel-collector-xray-collector-monitoring   ClusterIP   172.20.2.85     &lt;none&gt;        8888/TCP            6m46s\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/otel-collector-xray-collector   1/1     1            1           6m44s\n\nNAME                                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/otel-collector-xray-collector-6fc44d9bbf   1         1         1       6m44s\n</code></pre>"},{"location":"addons/xray-adot-addon/#functionality","title":"Functionality","text":"<p>Applies the X-Ray ADOT add-on to an Amazon EKS cluster. </p>"},{"location":"addons/xray/","title":"AWS X-Ray Add-on","text":"<p>This add-on is currently deprecated since the underlying manifests are incompatible with the latest versions of EKS. Please use XRay Adot Add-on.</p> <p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. The X-Ray addon provisions X-Ray daemon into an EKS cluster. This daemon exposes receives X-Ray SDK metrics and pushes the traces to X-Ray console. </p> <p>For instructions on getting started with X-Ray on EKS refer to the X-Ray Documentation.</p>"},{"location":"addons/xray/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.XrayAddOn();\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .build(app, 'my-stack-name');\n</code></pre> <p>Once deployed, it allows applications to be instrumented with X-Ray by leveraging the X-Ray SDK.  Examples of such integration can be found on AWS Docs.</p>"},{"location":"addons/xray/#functionality","title":"Functionality","text":"<ol> <li>Creates the <code>amazon-cloudwatch</code> namespace.</li> <li>Deploys the <code>xray-daemon</code> manifests into the cluster.</li> <li>Configures Node role with policies (<code>AWSXRayDaemonWriteAccess</code>) for communication between the cluster and the X-Ray service.</li> </ol>"},{"location":"api/media/readme-internal/","title":"Description","text":"<p>This is an internal readme for development processes that should be followed for this repository.</p>"},{"location":"api/media/readme-internal/#local-development","title":"Local Development","text":"<p>This project leverage Makefiles for project automation. We currently support the following commands.</p> <p>Lint the project with <code>ESLint</code>. </p> <pre><code>make lint\n</code></pre> <p>Build the project with <code>Typescript</code>. </p> <pre><code>make build.\n</code></pre>"},{"location":"api/media/readme-internal/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>The below instructions apply regardless of whether PR is submitted from a fork or a branch.</p> <ol> <li>Make sure you IDE is configured to format modified lines only. Submitting fully formatted files makes it very hard to review, and such will be rejected.</li> <li>Tab size is 4 and spaces (be mindful that VSCode may not be the only IDE used). </li> <li>he following commands produce no errors and/or warnings:</li> </ol> <pre><code>npm i\nmake build\nmake lint\nmake run-test\ncdk list\n</code></pre>"},{"location":"api/media/readme-internal/#triggering-e2e-testing","title":"Triggering E2E Testing","text":"<p>The CI system attached to the project will run all stacks under examples as end-to-end integration testing. </p> <p>Currently it works the following way:</p> <ul> <li>A human maintainer reviews the pr code to ensure it is not malicious </li> <li>If the code is trusted and the maintainer wishes to run e2e tests, they comment on the pr with /do-e2e-tests. This will trigger the build and test. Any visibility into the state can only occur through AWS maintainers.</li> <li>If job succeeds, the CI bot approves the PR. If it fails it requests changes. Details on what failed will need to be manually shared with external contributors.</li> <li>At present shapirov103, kcoleman731 and askulkarni2 have rights to invoke the bot. </li> </ul>"},{"location":"api/media/readme-internal/#publishing","title":"Publishing","text":"<p>At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs.</p> <ol> <li>Change version in package.json. We are currently using .., e.g. 0.1.5 <li>Patch version increment must be used for bug fixes, including changes in code and missing documentation.</li> <li>Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes.</li> <li>Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation).</li> <li>Publishing (if not applied through CI):</li> <li><code>make build</code> (compile)</li> <li><code>npm publish</code> (this will require credentials to npm)</li>"},{"location":"api/media/readme-internal/#submitting-changes","title":"Submitting Changes","text":"<p>For   direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch.  3. After review if approved changes will be merged.</p> <p>For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following:    1. Clear description of the feature    2. Test coverage    3. Validation instructions</p>"},{"location":"api/media/readme-internal/#validations-framework-link-to-readme","title":"Validations framework link to readme","text":"<p>See this document for more details. </p>"},{"location":"builders/","title":"Builders","text":"<p>The <code>eks-blueprints</code> framework allows customers to use builders to configure required addons as they prepare a blueprint for setting EKS cluster with required day 2 operational tooling</p> <p>The framework currently provides support for the following Builders:</p> Builder Description <code>BedrockBuilder</code> The <code>BedrockBuilder</code> allows you to get started with a builder class to configure required addons as you prepare a blueprint for setting up an EKS cluster with access to Amazon Bedrock. <code>GpuBuilder</code> The <code>GpuBuilder</code> allows you to get started with a builder class to configure with required setup as you prepare a blueprint for setting up EKS cluster with GPU Operator to run your GPU workloads. <code>GravitonBuilder</code> The <code>GravitonBuilder</code> allows you to get started with a builder class to configure with required setup as you prepare a blueprint for setting up EKS cluster with Graviton worker nodes to run your ARM64 workloads. <code>ObservabilityBuilder</code> The <code>ObservabilityBuilder</code> allows you to get started with a builder class to configure required addons as you prepare a blueprint for setting up Observability on an existing EKS cluster or a new EKS cluster. <code>WindowsBuilder</code> The <code>WindowsBuilder</code> allows you to get started with a builder class to configure with required setup as you prepare a blueprint for setting up EKS cluster with windows to run your windows workloads."},{"location":"builders/bedrock-builder/","title":"Bedrock Builder","text":"<p>The <code>BedrockBuilder</code> allows you to get started with a builder class to configure required addons as you prepare a blueprint for setting up EKS cluster with access to Bedrock.</p> <p>The <code>BedrockBuilder</code> creates the following:</p> <ul> <li>An EKS Cluster` with passed k8s version and cluster tags.</li> <li>A nodegroup to schedule Gen AI workloads with parameters passed.</li> <li>Sets up IRSA with access to Bedrock service.</li> </ul>"},{"location":"builders/bedrock-builder/#bedrock-on-eks-cluster","title":"Bedrock on EKS Cluster","text":"<p>The below usage helps you with a demonstration to use <code>BedrockBuilder</code> to setup required addons as you prepare a blueprint for setting up Bedrock access to a new EKS cluster.</p> <pre><code>import { ApplicationTeam, BedrockBuilder, ClusterInfo } from \"@aws-quickstart/eks-blueprints\";\nimport * as blueprints from \"@aws-quickstart/eks-blueprints\";\nimport * as spi from '@aws-quickstart/eks-blueprints/dist/spi';\nimport { Construct } from \"constructs\";\nimport { loadYaml, readYamlDocument } from \"@aws-quickstart/eks-blueprints/dist/utils\";\nimport { KubectlProvider, ManifestDeployment } from \"@aws-quickstart/eks-blueprints/dist/addons/helm-addon/kubectl-provider\";\n\nexport default class GenAIShowcase {\n    constructor(scope: Construct, id: string) {\n        const account = process.env.CDK_DEFAULT_ACCOUNT!;\n        const region = process.env.CDK_DEFAULT_REGION!;\n        const stackID = `${id}-blueprint`;\n\n        const bedrockTeamProps: blueprints.teams.BedrockTeamProps = {\n            name: blueprints.utils.valueFromContext(scope, \"bedrock.pattern.name\", \"showcase\"),\n            namespace: blueprints.utils.valueFromContext(scope, \"bedrock.pattern.namespace\", \"bedrock\"),\n            createNamespace: true,\n            serviceAccountName: 'bedrock-service-account',\n            extensionFunction: extensionFunction\n        }; \n\n        BedrockBuilder.builder()\n            .account(account)\n            .region(region)\n            .version('auto')\n            .addBedrockTeam(bedrockTeamProps)\n            .build(scope, stackID);\n    }\n}\n\nfunction extensionFunction(team: ApplicationTeam, clusterInfo: ClusterInfo) {\n    const values: spi.Values = {\n        namespace: team.teamProps.namespace,\n        imageName: blueprints.utils.valueFromContext(clusterInfo.cluster, \"bedrock.pattern.image.name\", undefined),\n        imageTag: blueprints.utils.valueFromContext(clusterInfo.cluster, \"bedrock.pattern.image.tag\", undefined)\n    };\n\n    // Apply manifest\n    const doc = readYamlDocument(__dirname + '/deployment/showcase-deployment.ytpl');\n    const manifest = doc.split(\"---\").map((e: any) =&gt; loadYaml(e));\n\n    const manifestDeployment: ManifestDeployment = {\n        name: team.teamProps.name,\n        namespace: team.teamProps.namespace!,\n        manifest,\n        values\n    };\n    const manifestConstruct = new KubectlProvider(clusterInfo).addManifest(manifestDeployment);\n    manifestConstruct.node.addDependency(team.serviceAccount);\n}\n</code></pre>"},{"location":"builders/gpu-builder/","title":"GPU Builder","text":"<p>The <code>GpuBuilder</code> allows you to get started with a builder class to configure with required setup as you prepare a blueprint for setting up EKS cluster with GPU Operator to run your GPU workloads. </p> <p>The <code>GpuBuilder</code> creates the following: - An EKS Cluster` with passed k8s version and cluster tags. - A nodegroup to schedule GPU workloads with parameters passed.</p>"},{"location":"builders/gpu-builder/#input-parameters","title":"Input Parameters","text":"<p><code>GpuOptions</code> which takes inputs to <code>GpuBuilder</code> supports following parameters:</p> <ul> <li><code>kubernetesVersion</code> : Required field, Kubernetes version to use for the cluster</li> <li><code>instanceClass</code>: Required field, Instance class to use for the cluster</li> <li><code>instanceSize</code>:  Required field, Instance size to use for the cluster</li> <li><code>nodeRole</code>: optional, Node IAM Role to be attached to nodes.</li> <li><code>gpuAmiType</code>: Required field, AMI Type for GPU Nodes. For example <code>AL2_X86_64_GPU</code>.</li> <li><code>desiredNodeSize</code>: Optional field, Desired number of nodes to use for the cluster</li> <li><code>minNodeSize</code>: Optional field, Minimum number of nodes to use for the cluster</li> <li><code>maxNodeSize</code>: Optional field, Maximum number of nodes to use for the cluster</li> <li><code>blockDeviceSize</code>: Optional field, Block device size</li> <li><code>clusterProviderTags</code>: Optional field, Cluster Provider Tags</li> <li><code>nodeGroupTags</code>: Optional field,  Node Group Tags for nodes which run standard cluster software.</li> </ul>"},{"location":"builders/gpu-builder/#demonstration-running-gpus-on-eks-cluster","title":"Demonstration - Running GPUs on EKS Cluster","text":"<p>The below usage helps you with a demonstration to use <code>GpuBuilder</code> to configure a required setup as you prepare a blueprint for setting up GPU nodes on a new EKS cluster.</p> <pre><code>import * as blueprints from \"@aws-quickstart/eks-blueprints\";\nimport * as ec2 from \"aws-cdk-lib/aws-ec2\";\nimport * as eks from \"aws-cdk-lib/aws-eks\";\nimport * as iam from \"aws-cdk-lib/aws-iam\";\nimport { Construct } from \"constructs\";\nimport { GpuBuilder, GpuOptions } from '../common/gpu-builder';\n\nexport default class GpuConstruct {\n    build(scope: Construct, id: string) {\n        const account = process.env.CDK_DEFAULT_ACCOUNT!;\n        const region = process.env.CDK_DEFAULT_REGION!;\n        const stackID = `${id}-eks-blueprint`;\n\n        const options: GpuOptions = {\n            kubernetesVersion: eks.KubernetesVersion.of(\"1.28\"),\n            instanceClass: ec2.InstanceClass.G5,\n            instanceSize: ec2.InstanceSize.XLARGE\n        };\n\n        const values = {\n                    driver: {\n                      enabled: true\n                    },\n                    mig: {\n                      strategy: 'mixed'\n                    },\n                    devicePlugin: {\n                      enabled: true,\n                      version: 'v0.13.0'\n                    },\n                    migManager: {\n                      enabled: true,\n                      WITH_REBOOT: true\n                    },\n                    toolkit: {\n                      version: 'v1.13.1-centos7'\n                    },\n                    operator: {\n                      defaultRuntime: 'containerd'\n                    },\n                    gfd: {\n                      version: 'v0.8.0'\n                    }\n                }\n\n\n        GpuBuilder.builder(options)\n            .account(account)\n            .region(region)\n            .enableGpu({values})\n            .build(scope, stackID);\n    }\n}\n</code></pre>"},{"location":"builders/graviton-builder/","title":"Graviton Builder","text":"<p>The <code>GravitonBuilder</code> allows you to get started with a builder class to configure with required setup as you prepare a blueprint for setting up EKS cluster with Graviton worker nodes to run your ARM64 workloads.</p> <p>The <code>GravitonBuilder</code> creates the following:</p> <ul> <li>An EKS Cluster` with passed k8s version and cluster tags.</li> <li>A nodegroup to schedule ARM64 workloads with parameters passed.</li> </ul>"},{"location":"builders/graviton-builder/#input-parameters","title":"Input Parameters","text":"<p><code>Partial&lt;MngClusterProviderProps&gt;</code> parameters can be used as inputs to <code>GravitonBuilder</code>. Few parameters shown below:</p> <ul> <li><code>version</code> : Kubernetes version to use for the cluster</li> <li><code>instanceTypes</code>: Instance Type to use for the cluster</li> </ul>"},{"location":"builders/graviton-builder/#demonstration-running-graviton-nodes-on-eks-cluster","title":"Demonstration - Running Graviton Nodes on EKS Cluster","text":"<p>The below usage helps you with a demonstration to use <code>GravitonBuilder</code> to configure a required setup as you prepare a blueprint for setting up Graviton nodes on a new EKS cluster.</p> <pre><code>import * as blueprints from \"@aws-quickstart/eks-blueprints\";\nimport { GravitonBuilder } from \"@aws-quickstart/eks-blueprints\";\nimport { CfnWorkspace } from \"aws-cdk-lib/aws-aps\";\nimport * as ec2 from \"aws-cdk-lib/aws-ec2\";\nimport * as eks from \"aws-cdk-lib/aws-eks\";\nimport { Construct } from \"constructs\";\n\nexport default class GravitonConstruct {\n    build(scope: Construct, id: string) {\n        const account = process.env.CDK_DEFAULT_ACCOUNT!;\n        const region = process.env.CDK_DEFAULT_REGION!;\n        const stackID = `${id}-blueprint`;\n\n        const ampWorkspaceName = \"graviton-amp-workspaces\";\n        const ampWorkspace: CfnWorkspace =\n            blueprints.getNamedResource(ampWorkspaceName);\n\n        const options: Partial&lt;blueprints.MngClusterProviderProps&gt; = {\n            version: eks.KubernetesVersion.of(\"1.28\"),\n            instanceTypes: [ec2.InstanceType.of(ec2.InstanceClass.M7G, ec2.InstanceSize.XLARGE)],\n            desiredSize: 3,\n            minSize: 2,\n            maxSize: 5,\n        };\n\n        GravitonBuilder.builder(options)\n            .account(account)\n            .region(region)\n            .resourceProvider(\n                blueprints.GlobalResources.Vpc,\n                new blueprints.VpcProvider()\n            )\n            .resourceProvider(\n                \"efs-file-system\",\n                new blueprints.CreateEfsFileSystemProvider({\n                    name: \"efs-file-systems\",\n                })\n            )\n            .resourceProvider(\n                ampWorkspaceName,\n                new blueprints.CreateAmpProvider(\n                    ampWorkspaceName,\n                    ampWorkspaceName\n                )\n            )\n            .addOns(\n                new blueprints.addons.IstioBaseAddOn(),\n                new blueprints.addons.IstioControlPlaneAddOn(),\n                new blueprints.addons.KubeStateMetricsAddOn(),\n                new blueprints.addons.MetricsServerAddOn(),\n                new blueprints.addons.PrometheusNodeExporterAddOn(),\n                new blueprints.addons.ExternalsSecretsAddOn(),\n                new blueprints.addons.SecretsStoreAddOn(),\n                new blueprints.addons.CalicoOperatorAddOn(),\n                new blueprints.addons.CertManagerAddOn(),\n                new blueprints.addons.AdotCollectorAddOn(),\n                new blueprints.addons.AmpAddOn({\n                    ampPrometheusEndpoint: ampWorkspace.attrPrometheusEndpoint\n                }),\n                new blueprints.addons.CloudWatchLogsAddon({\n                    logGroupPrefix: \"/aws/eks/graviton-blueprint\",\n                }),\n                new blueprints.addons.EfsCsiDriverAddOn(),\n                new blueprints.addons.FluxCDAddOn(),\n                new blueprints.addons.GrafanaOperatorAddon(),\n                new blueprints.addons.XrayAdotAddOn()\n            )\n            .build(scope, stackID);\n    }\n}\n</code></pre>"},{"location":"builders/observability-builder/","title":"Observability Builder","text":"<p>The <code>ObservabilityBuilder</code> allows you to get started with a builder class to configure required addons as you prepare a blueprint for setting up observability on an existing EKS cluster or a new EKS cluster.</p>"},{"location":"builders/observability-builder/#supported-methods","title":"Supported Methods","text":"<p><code>ObservabilityBuilder</code> supports following methods for setting up observability on Amazon EKS :</p> <ul> <li><code>enableNativePatternAddOns</code>: This method helps you prepare a blueprint for setting up observability with AWS native services</li> <li><code>enableMixedPatternAddOns</code>: This method helps you prepare a blueprint for setting up observability with AWS managed open source services</li> <li><code>enableOpenSourcePatternAddOns</code>: This method helps you prepare a blueprint for setting up observability with a combination of AWS native and AWS managed open source services</li> <li><code>enableControlPlaneLogging</code>: This method activates all the control plane logging features for EKS Clusters and feeds them into CloudWatch. This is an in-place change and should work for new and existing deployments, please check the AWS documentation for Control Plane Logging for more information on Control Plane logging.</li> </ul>"},{"location":"builders/observability-builder/#usage","title":"Usage","text":"<p>The framework provides a couple of convenience methods to instantiate the `` by leveraging the SDK API calls.</p>"},{"location":"builders/observability-builder/#usage-1-observability-for-a-new-eks-cluster","title":"Usage 1 - Observability For a New EKS Cluster","text":"<p>The below usage helps you with a demonstration to use <code>ObservabilityBuilder</code> to setup required addons as you prepare a blueprint for setting up observability on a new EKS cluster.</p> <pre><code>import { Construct } from 'constructs';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { ObservabilityBuilder } from '@aws-quickstart/eks-blueprints';\n\nexport default class SingleNewEksConstruct {\n    constructor(scope: Construct, id: string) {\n        const stackId = `${id}-observability-accelerator`;\n\n        const account = process.env.COA_ACCOUNT_ID! || process.env.CDK_DEFAULT_ACCOUNT!;\n        const region = process.env.COA_AWS_REGION! || process.env.CDK_DEFAULT_REGION!;\n\n        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.VpcCniAddOn(),\n            new blueprints.addons.XrayAddOn()\n        ];\n\n        ObservabilityBuilder.builder()\n            .account(account)\n            .region(region)\n            .enableNativePatternAddOns()\n            .enableControlPlaneLogging()\n            .addOns(...addOns)\n            .build(scope, stackId);\n    }\n}\n</code></pre>"},{"location":"builders/observability-builder/#usage-2-observability-for-an-existing-eks-cluster","title":"Usage 2 - Observability For an existing EKS Cluster","text":"<p>The below usage helps you with a demonstration to use <code>ObservabilityBuilder</code> to setup required addons as you prepare a blueprint for setting up observability on an existing EKS cluster.</p> <pre><code>import { ImportClusterProvider, utils } from '@aws-quickstart/eks-blueprints';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { cloudWatchDeploymentMode } from '@aws-quickstart/eks-blueprints';\nimport { ObservabilityBuilder } from '@aws-quickstart/eks-blueprints';\nimport * as cdk from \"aws-cdk-lib\";\nimport * as eks from 'aws-cdk-lib/aws-eks';\n\nexport default class ExistingEksMixedobservabilityConstruct {\n    async buildAsync(scope: cdk.App, id: string) {\n        // AddOns for the cluster\n        const stackId = `${id}-observability-accelerator`;\n\n        const clusterName = utils.valueFromContext(scope, \"existing.cluster.name\", undefined);\n        const kubectlRoleName = utils.valueFromContext(scope, \"existing.kubectl.rolename\", undefined);\n\n        const account = process.env.COA_ACCOUNT_ID! || process.env.CDK_DEFAULT_ACCOUNT!;\n        const region = process.env.COA_AWS_REGION! || process.env.CDK_DEFAULT_REGION!;\n\n        const sdkCluster = await blueprints.describeCluster(clusterName, region); // get cluster information using EKS APIs\n        const vpcId = sdkCluster.resourcesVpcConfig?.vpcId;\n\n        /**\n         * Assumes the supplied role is registered in the target cluster for kubectl access.\n         */\n\n        const importClusterProvider = new ImportClusterProvider({\n            clusterName: sdkCluster.name!,\n            version: eks.KubernetesVersion.of(sdkCluster.version!),\n            clusterEndpoint: sdkCluster.endpoint,\n            openIdConnectProvider: blueprints.getResource(context =&gt;\n                new blueprints.LookupOpenIdConnectProvider(sdkCluster.identity!.oidc!.issuer!).provide(context)),\n            clusterCertificateAuthorityData: sdkCluster.certificateAuthority?.data,\n            kubectlRoleArn: blueprints.getResource(context =&gt; new blueprints.LookupRoleProvider(kubectlRoleName).provide(context)).roleArn,\n            clusterSecurityGroupId: sdkCluster.resourcesVpcConfig?.clusterSecurityGroupId\n        });\n\n        const cloudWatchAdotAddOn = new blueprints.addons.CloudWatchAdotAddOn({\n            deploymentMode: cloudWatchDeploymentMode.DEPLOYMENT,\n            namespace: 'default',\n            name: 'adot-collector-cloudwatch',\n            metricsNameSelectors: ['apiserver_request_.*', 'container_memory_.*', 'container_threads', 'otelcol_process_.*'],\n        });\n\n        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.VpcCniAddOn(),\n            cloudWatchAdotAddOn,\n            new blueprints.addons.XrayAdotAddOn(),\n        ];\n\n        ObservabilityBuilder.builder()\n            .account(account)\n            .region(region)\n            .enableMixedPatternAddOns()\n            .enableControlPlaneLogging()\n            .clusterProvider(importClusterProvider)\n            .resourceProvider(blueprints.GlobalResources.Vpc, new blueprints.VpcProvider(vpcId)) \n            .addOns(...addOns)\n            .build(scope, stackId);\n    }\n}\n</code></pre>"},{"location":"builders/windows-builder/","title":"Windows Builder","text":"<p>The <code>WindowsBuilder</code> allows you to get started with a builder class to configure with required setup as you prepare a blueprint for setting up EKS cluster with windows to run your windows workloads. </p> <p>The <code>WindowsBuilder</code> creates the following: - An EKS Cluster` with passed k8s version and cluster tags. - A non-windows nodegroup for standard software with parameters passed. - A windows nodegroup to schedule windows workloads with parameters passed.</p>"},{"location":"builders/windows-builder/#input-parameters","title":"Input Parameters","text":"<p><code>WindowsOptions</code> which takes inputs to <code>WindowsBuilder</code> supports following parameters:</p> <ul> <li><code>kubernetesVersion</code> : Required field, Kubernetes version to use for the cluster</li> <li><code>instanceClass</code>: Required field, Instance class to use for the cluster</li> <li><code>instanceSize</code>:  Required field, Instance size to use for the cluster</li> <li><code>nodeRole</code>: optional, Node IAM Role to be attached to Windows and Non-windows nodes.</li> <li><code>windowsAmiType</code>: Required field, AMI Type for Windows Nodes. For example <code>WINDOWS_FULL_2022_X86_64</code>.</li> <li><code>desiredNodeSize</code>: Optional field, Desired number of nodes to use for the cluster</li> <li><code>minNodeSize</code>: Optional field, Minimum number of nodes to use for the cluster</li> <li><code>maxNodeSize</code>: Optional field, Maximum number of nodes to use for the cluster</li> <li><code>blockDeviceSize</code>: Optional field, Block device size</li> <li><code>noScheduleForWindowsNodes</code>: Optional field, No Schedule for Windows Nodes, this allows Windows nodes to be marked as no-schedule by default to prevent any linux workloads from scheduling.</li> <li><code>clusterProviderTags</code>: Optional field, Cluster Provider Tags</li> <li><code>genericNodeGroupTags</code>: Optional field, Generic Node Group Tags for non-windows nodes which run standard cluster software.</li> <li><code>windowsNodeGroupTags</code>: Optional field, Windows Node Group Tags.</li> </ul>"},{"location":"builders/windows-builder/#demonstration-building-windows-on-eks-cluster","title":"Demonstration - Building Windows on EKS Cluster","text":"<p>The below usage helps you with a demonstration to use <code>WindowsBuilder</code> to configure a required setup as you prepare a blueprint for setting up windows nodes on a new EKS cluster.</p> <pre><code>import * as blueprints from \"@aws-quickstart/eks-blueprints\";\nimport * as ec2 from \"aws-cdk-lib/aws-ec2\";\nimport * as eks from \"aws-cdk-lib/aws-eks\";\nimport * as iam from \"aws-cdk-lib/aws-iam\";\nimport { Construct } from \"constructs\";\nimport { WindowsBuilder, WindowsOptions } from '../common/windows-builder';\nimport { WindowsVpcCni } from \"./vpc-cni\";\n\nexport default class WindowsConstruct {\n    build(scope: Construct, id: string) {\n        const account = process.env.CDK_DEFAULT_ACCOUNT!;\n        const region = process.env.CDK_DEFAULT_REGION!;\n        const stackID = `${id}-eks-blueprint`;\n\n        const nodeRole = new blueprints.CreateRoleProvider(\"blueprint-node-role\", new iam.ServicePrincipal(\"ec2.amazonaws.com\"),\n            [\n                iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonEKSWorkerNodePolicy\"),\n                iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonEC2ContainerRegistryReadOnly\"),\n                iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonSSMManagedInstanceCore\"),\n                iam.ManagedPolicy.fromAwsManagedPolicyName(\"AmazonEKS_CNI_Policy\")\n            ]);\n\n        const options: WindowsOptions = {\n            kubernetesVersion: eks.KubernetesVersion.of(\"1.28\"),\n            instanceClass: ec2.InstanceClass.M5,\n            instanceSize: ec2.InstanceSize.XLARGE4\n        };\n\n        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new WindowsVpcCni()\n        ];\n\n        WindowsBuilder.builder(options)\n            .addOns(...addOns)\n            .account(account)\n            .region(region)\n            .resourceProvider(\"node-role\", nodeRole)\n            .resourceProvider(\n                blueprints.GlobalResources.Vpc,\n                new blueprints.VpcProvider()\n            )\n            .build(scope, stackID);\n    }\n}\n</code></pre>"},{"location":"cluster-providers/","title":"Cluster Providers","text":"<p>The <code>eks-blueprints</code> framework allows customers to easily configure the underlying EKS clusters that it provisions. This is done via Cluster Providers. Customers can leverage the Cluster Providers that the framework supports, or supply their own. </p> <p>The framework currently provides support for the following Cluster Providers:</p> Cluster Provider Description <code>GenericClusterProvider</code> Provisions an EKS cluster with one or more managed or Auto Scaling groups as well as Fargate Profiles. <code>AsgClusterProvider</code> Provisions an EKS cluster with an Auto Scaling group used for compute capacity. <code>MngClusterProvider</code> Provisions an EKS cluster with a Managed Node group for compute capacity. <code>FargateClusterProvider</code> Provisions an EKS cluster which leverages AWS Fargate to run Kubernetes pods. <code>ImportClusterProvider</code> Imports an existing EKS cluster into the blueprint allowing capabilities to add (certain) add-ons and teams. <p>By default, the framework will leverage the <code>MngClusterProvider</code> which creates a single managed node group.</p> <p>If you would like to add more node groups to a single cluster, you can leverage <code>GenericClusterProvider</code>, which allows multiple managed node groups or autoscaling (self-managed) node groups along with Fargate profiles.</p> <p>The version property that sets the Kubernetes Version for the Control Plane is required to be set either in the Cluster Provider, or in the Blueprint Properties.  In either spot, it can be set to a <code>KubernetesVersion</code> or <code>\"auto\"</code>.  If set to auto, the cluster version will be set to the latest Kubernetes Version. Auto versioning is not recommended in production clusters, as clusters will be updated as new Kubernetes versions release.</p>"},{"location":"cluster-providers/asg-cluster-provider/","title":"Auto Scaling Group Cluster Provider","text":"<p>The <code>AsgClusterProvider</code> allows you to provision an EKS cluster which leverages EC2 Auto Scaling groups(ASGs) for compute capacity. An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.</p>"},{"location":"cluster-providers/asg-cluster-provider/#usage","title":"Usage","text":"<pre><code>const props: AsgClusterProviderProps = {\n    minSize: 1,\n    maxSize: 10,\n    desiredSize: 4,\n    version: \"auto\",\n    instanceType: new InstanceType('m5.large'),\n    machineImageType: eks.MachineImageType.AMAZON_LINUX_2,\n    updatePolicy: UpdatePolicy.Rolling\n}\nconst clusterProvider = new blueprints.AsgClusterProvider(props);\nnew blueprints.EksBlueprint(scope, { id: 'blueprint', [], [], clusterProvider });\n</code></pre>"},{"location":"cluster-providers/asg-cluster-provider/#configuration","title":"Configuration","text":"<p><code>AsgClusterProvider</code> supports the following configuration options.</p> Prop Description name The name for the cluster. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default <code>min-size</code>). version Kubernetes version for the control plane. Required in cluster props or blueprint props. instanceType Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") machineImageType Machine Image Type for the Autoscaling Group. updatePolicy Update policy for the Autoscaling Group. vpcSubnets The subnets for the cluster. privateCluster If <code>true</code> Kubernetes API server is private. tags Tags to propagate to Cluster. <p>There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations.</p> <p>Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option):</p> <ul> <li><code>eks.default.min-size</code></li> <li><code>eks.default.max-size</code></li> <li><code>eks.default.desired-size</code></li> <li><code>eks.default.instance-type</code></li> <li><code>eks.default.private-cluster</code></li> <li><code>eks.default.isolated-cluster</code></li> </ul> <p>Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass <code>AsgClusterProviderProps</code> to each cluster blueprint.</p> <p>You can find more details on the supported configuration options in the API documentation for the AsgClusterProviderProps.</p>"},{"location":"cluster-providers/asg-cluster-provider/#bottlerocket-asg","title":"Bottlerocket ASG","text":"<p>Bottlerocket is a Linux-based open-source operating system that is purpose-built by Amazon Web Services for running containers. Customers can leverage the <code>AsgClusterProvider</code> to provision EKS clusters with Bottlerocket nodes. To do so, set the <code>machineImageType</code> property to <code>eks.MachineImageType.BOTTLEROCKET</code>.</p> <pre><code>const props: AsgClusterProviderProps = {\n    minSize: 1,\n    maxSize: 10,\n    desiredSize: 4,\n    version: \"auto\",\n    instanceType: new InstanceType('m5.large'),\n    machineImageType: eks.MachineImageType.BOTTLEROCKET,\n    updatePolicy: UpdatePolicy.Rolling\n}\nconst clusterProvider = new blueprints.AsgClusterProvider(props);\nnew blueprints.EksBlueprint(scope, { id: 'blueprint', teams, addOns, clusterProvider });\n</code></pre>"},{"location":"cluster-providers/fargate-cluster-provider/","title":"Fargate Cluster Provider","text":"<p>The <code>FargateClusterProvider</code> allows you to provision an EKS cluster which runs Kubernetes pods on AWS Fargate. To create a Fargate cluster, you must provide Fargate Profiles, which allows cluster operators to specify which Pods should be run on Fargate.</p>"},{"location":"cluster-providers/fargate-cluster-provider/#usage","title":"Usage","text":"<p>In the example below, the Fargate profile indicates that all Pods in the <code>dynatrace</code> namespace having the label <code>example.com/fargate</code> set to <code>true</code> (the string, not the boolean value) should run on Fargate. (The label may be omitted if you want all Pods in the namespace to run on Fargate.)</p> <pre><code>const fargateProfiles = {\n    dynatrace: {\n        selectors: [\n            {\n                namespace: 'dynatrace',\n                labels: { // optional\n                    'example.com/fargate': 'true'\n                }\n            }\n        ]\n    }\n};\nconst tags = {\n    \"Name\": \"blueprints-example-cluster\",\n    \"Type\": \"fargate-cluster\"\n}\nconst clusterProvider = new blueprints.FargateClusterProvider({\n    version: KubernetesVersion.V1_28,\n    fargateProfiles,\n    tags\n});\n\nnew blueprints.EksBlueprint(scope, { id: 'blueprint', [], [], clusterProvider });\n</code></pre>"},{"location":"cluster-providers/fargate-cluster-provider/#configuration","title":"Configuration","text":"<p><code>FargateClusterProvider</code> supports the following configuration options.</p> Prop Description name The name for the cluster. version Kubernetes version for the control plane. Required in cluster props or blueprint props. fargateProfiles A map of Fargate profiles to use with the cluster. vpcSubnets The subnets for the cluster. tags Tags to propagate to Cluster. privateCluster Public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations <p>You can find more details on the supported configuration options in the API documentation for the FargateClusterProviderProps.</p>"},{"location":"cluster-providers/generic-cluster-provider/","title":"Generic Cluster Provider","text":"<p>The <code>GenericClusterProvider</code> allows you to provision an EKS cluster which leverages one or more EKS managed node groups(MNGs), or one or more autoscaling groupsEC2 Auto Scaling groups for its compute capacity. Users can also configure multiple Fargate profiles along with the EC2 based compute cpacity.</p> <p>Today it is not possible for an Amazon EKS Cluster to propagate tags to EC2 instance worker nodes directly when you create an EKS cluster. You can create a launch template with custom tags on <code>managedNodeGroups</code> with <code>GenericClusterProvider</code> as shown in <code>mng2-launchtemplate</code>. This will allow you to propagate custom tags to your EC2 instance worker nodes.</p> <p>Note: If <code>launchTemplate</code> is passed with <code>managedNodeGroups</code>, <code>diskSize</code> is not allowed.</p>"},{"location":"cluster-providers/generic-cluster-provider/#configuration","title":"Configuration","text":"<p>Full list of configuration options:</p> <ul> <li>Generic Cluster Provider</li> <li>Managed Node Group</li> <li>Autoscaling Group</li> <li>Fargate Cluster</li> </ul>"},{"location":"cluster-providers/generic-cluster-provider/#usage","title":"Usage","text":"<pre><code>const windowsUserData = ec2.UserData.forWindows();\nwindowsUserData.addCommands(`\n    $ErrorActionPreference = 'Stop'\n    $EKSBootstrapScriptPath = \"C:\\\\\\\\Program Files\\\\\\\\Amazon\\\\\\\\EKS\\\\\\\\Start-EKSBootstrap.ps1\"\n    Try {\n    &amp; $EKSBootstrapScriptPath -EKSClusterName '&lt;YOUR_CLUSTER_NAME&gt;'\n    } Catch {\n    Throw $_\n    }\n`);\nconst ebsDeviceProps: ec2.EbsDeviceProps = {\n    deleteOnTermination: false,\n    volumeType: ec2.EbsDeviceVolumeType.GP2\n};\nconst clusterProvider = new blueprints.GenericClusterProvider({\n    version: KubernetesVersion.V1_29,\n    tags: {\n        \"Name\": \"blueprints-example-cluster\",\n        \"Type\": \"generic-cluster\"\n    },\n    serviceIpv4Cidr: \"10.43.0.0/16\",\n    // if needed use this to register an auth role integrate with RBAC\n    mastersRole: blueprints.getResource(context =&gt; {\n        return new iam.Role(context.scope, 'AdminRole', { assumedBy: new AccountRootPrincipal() });\n    }),\n    managedNodeGroups: [\n        {\n            id: \"mng1\",\n            amiType: NodegroupAmiType.AL2_X86_64,\n            instanceTypes: [new InstanceType('m5.2xlarge')],\n            desiredSize: 2,\n            maxSize: 3,\n            nodeGroupSubnets: { subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS },\n            launchTemplate: {\n                // You can pass Custom Tags to Launch Templates which gets propagated to worker nodes.\n                tags: {\n                    \"Name\": \"Mng1\",\n                    \"Type\": \"Managed-Node-Group\",\n                    \"LaunchTemplate\": \"Custom\",\n                    \"Instance\": \"ONDEMAND\"\n                }\n            }\n        },\n        {\n            id: \"mng2-launchtemplate\",\n            instanceTypes: [new ec2.InstanceType('m5.2xlarge')],\n            nodeGroupCapacityType: CapacityType.SPOT,\n            desiredSize: 0,\n            minSize: 0,\n            launchTemplate: {\n                machineImage: ec2.MachineImage.genericLinux({\n                    'us-east-1': 'ami-08e520f5673ee0894',\n                    'us-west-2': 'ami-0403ff342ceb30967',\n                    'us-east-2': 'ami-07109d69738d6e1ee',\n                    'us-west-1': 'ami-07bda4b61dc470985',\n                    'us-gov-west-1': 'ami-0e9ebbf0d3f263e9b',\n                    'us-gov-east-1':'ami-033eb9bc6daf8bfb1'\n                }),\n                userData: userData,\n                // You can pass Custom Tags to Launch Templates which gets propagated to worker nodes.\n                tags: {\n                    \"Name\": \"Mng2\",\n                    \"Type\": \"Managed-Node-Group\",\n                    \"LaunchTemplate\": \"Custom\",\n                    \"Instance\": \"SPOT\"\n                }\n            }\n        },\n        // Below is a Managed Windows Node Group Sample.\n        {\n            id: \"mng3-windowsami\",\n            amiType: NodegroupAmiType.AL2_X86_64,\n            instanceTypes: [new ec2.InstanceType('m5.4xlarge')],\n            desiredSize: 0,\n            minSize: 0,\n            nodeRole: blueprints.getNamedResource(\"node-role\") as iam.Role,\n            launchTemplate: {\n                blockDevices: [\n                    {\n                        deviceName: \"/dev/sda1\",\n                        volume: ec2.BlockDeviceVolume.ebs(50, ebsDeviceProps),\n                    }\n                ],\n            machineImage: ec2.MachineImage.genericWindows({\n                'us-east-1': 'ami-0e80b8d281637c6c1',\n                'us-east-2': 'ami-039ecff89038848a6',\n                'us-west-1': 'ami-0c0815035bf1efb6e',\n                'us-west-2': 'ami-029e1340b254a7667',\n                'eu-west-1': 'ami-09af50f599f7f882c',\n                'eu-west-2': 'ami-0bf1fec1eaef78230',\n            }),\n                securityGroup: blueprints.getNamedResource(\"my-cluster-security-group\") as ec2.ISecurityGroup,\n                tags: {\n                    \"Name\": \"Mng3\",\n                    \"Type\": \"Managed-WindowsNode-Group\",\n                    \"LaunchTemplate\": \"WindowsLT\",\n                    \"kubernetes.io/cluster/&lt;YOUR_CLUSTER_NAME&gt;\": \"owned\"\n                },\n                userData: windowsUserData,\n            }\n        }\n    ],\n    fargateProfiles: {\n        \"fp1\": {\n            fargateProfileName: \"fp1\",\n            selectors:  [{ namespace: \"serverless1\" }]\n        }\n    }\n});\n\nEksBlueprint.builder()\n    .clusterProvider(clusterProvider)\n    .build(app, blueprintID);\n</code></pre> <p>The Cluster configuration and node group configuration exposes a number of options that require to supply an actual CDK resource. For example cluster allows passing <code>mastersRole</code>, <code>securityGroup</code>, etc. to the cluster, while managed node group allow specifying <code>nodeRole</code>.</p> <p>All of such cases can be solved with Resource Providers.</p> <p>Example: <pre><code>const clusterProvider = new blueprints.GenericClusterProvider({\n    version: KubernetesVersion.V1_29,\n    tags: {\n        \"Name\": \"blueprints-example-cluster\",\n        \"Type\": \"generic-cluster\"\n    },\n    // if needed use this to register an auth role to integrate with RBAC\n    mastersRole: blueprints.getResource(context =&gt; {\n        return new iam.Role(context.scope, 'AdminRole', { assumedBy: new AccountRootPrincipal() });\n    }),\n    securityGroup: blueprints.getNamedResource(\"my-cluster-security-group\") as ec2.ISecurityGroup, // assumed to be register as a resource provider under name my-cluster-security-group\n    managedNodeGroups: [\n        {\n            id: \"mng1\",\n            nodeRole: blueprints.getResource(context =&gt; {\n                const role = new iam.Role(context.scope, 'NodeRole', { assumedBy: new iam.ServicePrincipal(\"ec2.amazonaws.com\")});\n                ... add policies such as AmazonEKSWorkerNodePolicy and AmazonEC2ContainerRegistryReadOnly\n                return role;\n            })\n        }\n});\n\nEksBlueprint.builder()\n    .resourceProvider(\"my-cluster-security-group\", {\n        provide(context: blueprints.ResourceContext) : ec2.ISecurityGroup {\n            return ec2.SecurityGroup.fromSecurityGroupId(context.scope, 'SG', 'sg-12345', { mutable: false }); // example for look up\n        }\n    })\n    .clusterProvider(clusterProvider)\n    .build(app, blueprintID);\n</code></pre></p>"},{"location":"cluster-providers/generic-cluster-provider/#configuration_1","title":"Configuration","text":"<p>The <code>GenericClusterProvider</code> supports the following configuration options.</p> Prop Description clusterName The name for the cluster. managedNodeGroups Zero or more managed node groups. autoscalingNodeGroups Zero or more autoscaling node groups (mutually exclusive with managed node groups). fargateProfiles Zero or more Fargate profiles. version Kubernetes version for the control plane. Required in cluster props or blueprint props. vpc VPC for the cluster. vpcSubnets The subnets for control plane ENIs (subnet selection). privateCluster If <code>true</code> Kubernetes API server is private. tags Tags to propagate to Cluster. <p>There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations.</p> <p>Default configuration for managed and autoscaling node groups can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option):</p> <ul> <li><code>eks.default.min-size</code></li> <li><code>eks.default.max-size</code></li> <li><code>eks.default.desired-size</code></li> <li><code>eks.default.instance-type</code></li> <li><code>eks.default.private-cluster</code></li> <li><code>eks.default.isolated-cluster</code></li> </ul> <p>Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass individual <code>GenericProviderClusterProps</code> to each cluster blueprint.</p> <p>You can find more details on the supported configuration options in the API documentation for the GenericClusterProviderProps.</p>"},{"location":"cluster-providers/generic-cluster-provider/#upgrading-control-plane","title":"Upgrading Control Plane","text":"<p>Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. Worker nodes in-place upgrade requires explicit update of the individual node groups. The property that controls it for managed node groups is <code>amiReleaseVersion</code>. The following demonstrates how to do so.</p> <pre><code>const clusterProvider = new blueprints.GenericClusterProvider({\n    version: KubernetesVersion.V1_29,\n    managedNodeGroups: [\n        {\n            id: \"managed-1\",\n            amiType: NodegroupAmiType.AL2_X86_64,\n            amiReleaseVersion: \"1.20.4-20210519\"\n        }\n    ]\n});\n</code></pre> <p>Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions.</p>"},{"location":"cluster-providers/import-cluster-provider/","title":"Import Cluster Provider","text":"<p>The <code>ImportClusterProvider</code> allows you to import an existing EKS cluster into your blueprint. Importing an existing cluster at present will allow adding certain add-ons and limited team capabilities. </p>"},{"location":"cluster-providers/import-cluster-provider/#usage","title":"Usage","text":"<p>The framework provides a couple of convenience methods to instantiate the <code>ImportClusterProvider</code> by leveraging the SDK API call to describe the cluster. </p>"},{"location":"cluster-providers/import-cluster-provider/#option-1","title":"Option 1","text":"<p>Recommended option is to get the cluster information through the <code>DescribeCluster</code> API (requires <code>eks:DescribeCluster</code> permission at build-time) and then use it to instantiate the <code>ImportClusterProvider</code> and (very important) to set up the blueprint VPC. </p> <p>Make sure VPC is set to the VPC of the imported cluster, otherwise the blueprint by default will create a new VPC, which will be redundant and cause problems with some of the add-ons. </p> <p>Note: <code>blueprints.describeCluster()</code> is an asynchronous function, you should either use <code>await</code> or handle promise resolution chain. </p> <pre><code>const clusterName = \"quickstart-cluster\";\nconst region = \"us-east-2\";\n\nconst kubectlRoleName = \"MyClusterAuthConfigRole\"; // this is the role registered in the aws-auth config map in the target cluster \nconst sdkCluster = await blueprints.describeCluster(clusterName, region); // get cluster information using EKS APIs\n\n/**\n * Assumes the supplied role is registered in the target cluster for kubectl access.\n */\nconst importClusterProvider = blueprints.ImportClusterProvider.fromClusterAttributes(\n    sdkCluster, \n    blueprints.getResource(context =&gt; new blueprints.LookupRoleProvider(kubectlRoleName).provide(context))\n);\n\nconst vpcId = sdkCluster.resourcesVpcConfig?.vpcId;\n\nblueprints.EksBlueprint.builder()\n    .clusterProvider(importClusterProvider)\n    .resourceProvider(blueprints.GlobalResources.Vpc, new blueprints.VpcProvider(vpcId)) // this is required with import cluster provider\n</code></pre>"},{"location":"cluster-providers/import-cluster-provider/#option-2","title":"Option 2","text":"<p>This option is convenient if you already know the VPC Id of the target cluster. It also requires <code>eks:DescribeCluster</code> permission at build-time:</p> <pre><code>const clusterName = \"quickstart-cluster\";\nconst region = \"us-east-2\";\n\nconst kubectlRole: iam.IRole = blueprints.getNamedResource('my-role');\n\nconst importClusterProvider2 = await blueprints.ImportClusterProvider.fromClusterLookup(clusterName, region, kubectlRole); // note await here\n\nconst vpcId = ...; // you can always get it with blueprints.describeCluster(clusterName, region);\n\nblueprints.EksBlueprint.builder()\n    .clusterProvider(importClusterProvider2)\n    .resourceProvider('my-role', new blueprints.LookupRoleProvider('my-role'))\n    .resourceProvider(blueprints.GlobalResources.Vpc, new blueprints.VpcProvider(vpcId)) \n</code></pre>"},{"location":"cluster-providers/import-cluster-provider/#option-3","title":"Option 3","text":"<p>Unlike the other options, this one does not require any special permissions at build time, however it requires passing all the required information to the import cluster provider.  OIDC provider is expected to be passed in as well if you are planning to leverage IRSA with your blueprint. The OIDC provider is expected to be registered in the imported cluster already, otherwise IRSA won't work.</p> <pre><code>const importClusterProvider3 = new ImportClusterProvider({\n    clusterName: 'my-existing-cluster',\n    version: KubernetesVersion.V1_26,\n    clusterEndpoint: 'https://B792B88BC60999B1A37D.gr7.us-east-2.eks.amazonaws.com',\n    openIdConnectProvider: getResource(context =&gt;\n        new LookupOpenIdConnectProvider('https://oidc.eks.us-east-2.amazonaws.com/id/B792B88BC60999B1A37D').provide(context)),\n    clusterCertificateAuthorityData: 'S0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCasdd234................',\n    kubectlRoleArn: 'arn:...',\n    clusterSecurityGroupId: 'sg...';\n});\n\nconst vpcId = ...; \n\nblueprints.EksBlueprint.builder()\n    .clusterProvider(importClusterProvider3)\n    .resourceProvider(blueprints.GlobalResources.Vpc, new blueprints.VpcProvider(vpcId)) \n</code></pre>"},{"location":"cluster-providers/import-cluster-provider/#configuration","title":"Configuration","text":"<p>The <code>ImportClusterProvider</code> supports the following configuration options:</p> Prop Description clusterName Cluster name version EKS version of the target cluster clusterEndpoint The API Server endpoint URL openIdConnectProvider An Open ID Connect provider for this cluster that can be used to configure service accounts. You can either import an existing provider using <code>LookupOpenIdConnectProvider</code>, or create a new provider using new custom resource provider to call <code>new eks.OpenIdConnectProvider</code> clusterCertificateAuthorityData The certificate-authority-data for your cluster. kubectlRoleArn An IAM role with cluster administrator and \"system:masters\" permissions."},{"location":"cluster-providers/import-cluster-provider/#known-limitations","title":"Known Limitations","text":"<p>The following add-ons will not work with the <code>ImportClusterProvider</code> due to the inability (at present) of the imported clusters to modify <code>aws-auth</code> ConfigMap and mutate cluster authentication: * <code>ClusterAutoScalerAddOn</code> * <code>AwsBatchAddOn</code> * <code>EmrEksAddOn</code> * <code>KarpenterAddOn</code></p> <p>Teams can be added to the cluster and will perform all of the team functionality except cluster access due to the same inability to mutate cluster access. </p> <p>At the moment, there are no examples to add extra capacity to the imported clusters like node groups. </p>"},{"location":"cluster-providers/mng-cluster-provider/","title":"Managed Node Group Cluster Provider","text":"<p>The <code>MngClusterProvider</code> allows you to provision an EKS cluster which leverages EKS managed node groups(MNGs) for compute capacity. MNGs automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters.</p>"},{"location":"cluster-providers/mng-cluster-provider/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as ec2 from 'aws-cdk-lib/aws-ec2';\nimport * as eks from 'aws-cdk-lib/aws-eks';\nimport * as bp from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst props: bp.MngClusterProviderProps = {\n    minSize: 1,\n    maxSize: 10,\n    desiredSize: 4,\n    instanceTypes: [new ec2.InstanceType('m5.large')],\n    amiType: eks.NodegroupAmiType.AL2023_X86_64_STANDARD,\n    nodeGroupCapacityType: eks.CapacityType.ON_DEMAND,\n    amiReleaseVersion: \"1.30.0-20240615\" // this will upgrade kubelet to 1.30.0\n};\n\nconst clusterProvider = new bp.MngClusterProvider(props);\nnew bp.EksBlueprint(app, { id: 'blueprint-1', addOns:[], teams: [], clusterProvider, version: eks.KubernetesVersion.V1_30 });\n</code></pre>"},{"location":"cluster-providers/mng-cluster-provider/#configuration","title":"Configuration","text":"<p>The <code>MngClusterProvider</code> supports the following configuration options.</p> Prop Description name The name for the cluster. @Deprecated clusterName Cluster name version Kubernetes version for the control plane. Required in cluster props or blueprint props. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default <code>min-size</code>). instanceTypes Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") amiType The AMI type for the managed node group. amiReleaseVersion The AMI Kubernetes release version for the node group. customAmi The custom AMI and the userData for the node group, <code>amiType</code> and <code>amiReleaseVersion</code> will be ignored if this is set. nodeGroupCapacityType The capacity type for the node group (on demand or spot). vpcSubnets The subnets for the cluster. privateCluster If <code>true</code> Kubernetes API server is private. isolatedCluster If <code>true</code> EKS Cluster is configured to deploy in an isolated subnet. tags Tags to propagate to Cluster. nodeGroupTags Tags to propagate to Node Group. <p>There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations.</p> <p>Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option):</p> <ul> <li><code>eks.default.min-size</code></li> <li><code>eks.default.max-size</code></li> <li><code>eks.default.desired-size</code></li> <li><code>eks.default.instance-type</code></li> <li><code>eks.default.private-cluster</code></li> <li><code>eks.default.isolated-cluster</code></li> </ul> <p>Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass <code>MngClusterProviderProps</code> to each cluster blueprint.</p> <p>You can find more details on the supported configuration options in the API documentation for the MngClusterProviderProps.</p>"},{"location":"cluster-providers/mng-cluster-provider/#upgrading-worker-nodes","title":"Upgrading Worker Nodes","text":"<p>Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, you must also update the <code>amiReleaseVersion</code> property. The following demonstrates how to do so.</p> <pre><code>const props: MngClusterProviderProps = {\n    version: KubernetesVersion.V1_29,\n    amiReleaseVersion: \"1.25.7-20230509\" // this will upgrade kubelet to 1.25\n}\n</code></pre> <p>Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions.</p>"},{"location":"cluster-providers/mng-cluster-provider/#creating-clusters-with-spot-capacity-type","title":"Creating Clusters with Spot Capacity Type","text":"<p>To create clusters which leverage Spot capacity, set the <code>nodeGroupCapacityType</code> value to <code>CapacityType.SPOT</code></p> <pre><code>const props: MngClusterProviderProps = {\n    nodeGroupCapacityType: CapacityType.SPOT,\n    version: KubernetesVersion.V1_29,\n    instanceTypes: [new InstanceType('t3.large'), new InstanceType('m5.large')],\n    amiReleaseVersion: \"1.25.7-20230509\" // this will upgrade kubelet to 1.25\n}\n</code></pre> <p>Note that two attributes in this configuration are relevant for Spot: <code>nodeGroupCapacityType</code> and <code>instaceTypes</code>. The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability.</p>"},{"location":"cluster-providers/mng-cluster-provider/#creating-clusters-with-custom-ami-for-the-node-group","title":"Creating Clusters with custom AMI for the node group","text":"<p>To create clusters using custom AMI for the worker nodes, set the <code>customAmi</code> to your custom image and provide your <code>userData</code> for node bootstrapping.</p> <pre><code>const userData = UserData.forLinux();\nuserData.addCommands(`/etc/eks/bootstrap.sh ${cluster.clusterName}`);\n\nconst props: MngClusterProviderProps = {\n    nodeGroupCapacityType: CapacityType.ON_DEMAND,\n    version: KubernetesVersion.V1_29,\n    instanceTypes: [new InstanceType('t3.large')],\n    customAmi: {\n        machineImage: MachineImage.genericLinux({'us-east-1': 'ami-0be34337b485b2609'}),\n        userData: userData,\n    },\n}\n</code></pre>"},{"location":"internal/ci/","title":"CodeBuild CI","text":"<p>This example shows how to enable a CodeBuild based Continuous Integration process for the Blueprints blueprint. The CodeBuild project is provisioned using a CDK application.</p> <p>The buildspec.yml provided deploys the sample blueprint stacks provided in examples. The buildspec can be used directly if you wish to setup the CodeBuild project manually through the console or via the CLI.</p> <p>Optionally, you can also provide an S3 bucket location for a <code>cdk.context.json</code> that contains key-values for any context you want to provide to your application such as route 53 domain, domain account, subzone, IAM users, etc.</p>"},{"location":"internal/ci/#deploy-codebuild-project","title":"Deploy CodeBuild Project","text":"<p>First, clone this project.</p> <pre><code>git clone https://github.com/aws-quickstart/cdk-eks-blueprints.git\n\ncd cdk-eks-blueprints\n</code></pre> <p>Install CDK (please review and install any missing pre-requisites for your environment)</p> <pre><code>npm install -g aws-cdk@2.173.4\n</code></pre> <p>Install the dependencies for this project.</p> <pre><code>npm install\n</code></pre> <p>Bootstrap CDK into the target AWS account and region.</p> <pre><code>env CDK_NEW_BOOTSTRAP=1  cdk bootstrap \\\n  --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\naws://&lt;ACCOUNT_ID&gt;/&lt;AWS_REGION&gt;\n</code></pre> <p>Connect GitHub organization to CodeBuild (as described here).</p> <pre><code>export GITHUB_TOKEN=&lt;personal_access_token&gt;\n\naws codebuild import-source-credentials \\\n  --server-type GITHUB \\\n  --auth-type PERSONAL_ACCESS_TOKEN \\\n  --token $GITHUB_TOKEN\n</code></pre> <p>Optionally, upload a cdk.context.json file into an S3 bucket which can be accessed by the CodeBuild project.</p> <p><pre><code>aws s3 cp cdk.context.json \\\n  s3://&lt;s3bucket&gt;/cdk.context.json\n</code></pre> Deploy the CodeBuild Project. Use the <code>--parameters GitHubOwner=&lt;value&gt;</code> to override the value for <code>owner</code> used in the CodeBuild project. If you do not specify the input parameter we will try to use <code>aws-quickstart</code> by default.</p> <pre><code>cdk deploy -a \"npx ts-node ci/index.ts\" \\\n  --parameters GitHubOwner=&lt;GitHubOwner&gt; \\\n  --context eks.default.context-location=s3://&lt;s3bucket&gt;/cdk.context.json\"\n</code></pre> <p>After the deployment is completed the CodeBuild project will be configured to build and deploy the blueprint stack on every pull-request merge to the <code>main</code> branch.</p> <p>Note: Update build badge url the top level README. The badge url can be obtained by running the following AWS cli command.</p> <pre><code>aws codebuild batch-get-projects \\\n  --names QuickstartSspAmazonEksBuild | \\\n  jq -r '.projects[0].badge.badgeRequestUrl'\n</code></pre>"},{"location":"internal/input-validations-framework-readme/","title":"How to use the framework","text":"<p>The constraints framework implementation is located in the utils/constraints-utils.ts module</p> <pre><code>import * from 'utils/constraints-utils';\n</code></pre>"},{"location":"internal/input-validations-framework-readme/#what-can-you-use-with-the-framework","title":"What can you use with the framework","text":"<p>The constraints framework provides a set of generic classes and interfaces as well as the invocation framework to validate constraints on arbitrary objects.</p>"},{"location":"internal/input-validations-framework-readme/#validations-in-constraints-utilsts","title":"Validations in constraints-utils.ts","text":"<p>This file holds the supported constraints and function(s) to validate constraints defined below in the rest of this document.</p>"},{"location":"internal/input-validations-framework-readme/#stringconstraint","title":"StringConstraint","text":"<p>Constructor:  <pre><code>new StringConstraint(minValue, maxValue);\n</code></pre></p> <p>API reference 'here'</p> <p>If given string length falls outside of these inclusive bounds throws detailed Zod error</p>"},{"location":"internal/input-validations-framework-readme/#urlstringconstraint","title":"UrlStringConstraint","text":"<p>Constructor:  <pre><code>new UrlStringConstraint(minValue, maxValue);\n</code></pre></p> <p>API reference 'here'</p> <p>If given string length falls outside of these inclusive bounds, or does not follow a proper URL format it throws detailed Zod error</p>"},{"location":"internal/input-validations-framework-readme/#numberconstraint","title":"NumberConstraint","text":"<p>Constructor:  <pre><code>new NumberConstraint(minValue, maxValue);\n</code></pre></p> <p>API reference 'here'</p> <p>If given number falls outside of these inclusive bounds throws detailed Zod error.</p>"},{"location":"internal/input-validations-framework-readme/#arrayconstraint","title":"ArrayConstraint","text":"<p>Constructor: <pre><code>new utils.ArrayConstraint(minValue, maxValue);\n</code></pre></p> <p>API reference 'here'</p> <p>If given array length falls outside of these inclusive bounds throws detailed Zod error.</p>"},{"location":"internal/input-validations-framework-readme/#genericregexstringcontraint","title":"GenericRegexStringContraint","text":"<p>Constructor: <pre><code>new GenericRegexStringConstraint(new RegExp(regexString));\n</code></pre></p> <p>If given string does not match the regular expression throws detailed Zod error.</p>"},{"location":"internal/input-validations-framework-readme/#compositeconstraint","title":"CompositeConstraint","text":"<p>Constructor: <pre><code>new CompositeConstraint(...constraints);\n</code></pre></p> <p>If given value does not comply with each of the constraints in the list throws detailed Zod error for first failure.</p>"},{"location":"internal/input-validations-framework-readme/#internethoststringconstraint","title":"InternetHostStringConstraint","text":"<p>Constructor: <pre><code>new InternetHostStringConstraint();\n</code></pre></p> <p>If given string does not match RFC 1123 standards throws detailed Zod error.</p>"},{"location":"internal/input-validations-framework-readme/#validateconstraints-function","title":"validateConstraints Function","text":"<p><pre><code>validateConstraints&lt;T&gt;(constraints: ConstraintsType&lt;T&gt;, context: string, ...object: any)\n</code></pre> This is the entry point to use the framework. This function can validate either a single object or an array of objects against the provided constraints.</p>"},{"location":"internal/input-validations-framework-readme/#how-to-use-the-constraints-utilsts","title":"How to use the constraints-utils.ts","text":"<p>You need two things when utilizing constraints-utils.ts and the following examples are from 'here'</p> <p>First you need a class with specified keys assigned to given constraints. </p> <p>Example with two keys: </p> <pre><code>export class BlueprintPropsConstraints implements ConstraintsType&lt;EksBlueprintProps&gt; {\n    id = new StringConstraint(1, 63);\n    name = new StringConstraint(1, 63);\n</code></pre> <p>Second you need to call the <code>validateConstraints</code> function:</p> <p>Example (note: punctuation, formatting):</p> <pre><code>validateConstraints(new BlueprintPropsConstraints, EksBlueprintProps.name, blueprintProps);\n</code></pre>"},{"location":"internal/input-validations-framework-readme/#limitations","title":"Limitations","text":"<p>Currently, constraints can be defined for flat objects, and nested structures will require individual validations.</p>"},{"location":"internal/readme-internal/","title":"Description","text":"<p>This is an internal readme for development processes that should be followed for this repository.</p>"},{"location":"internal/readme-internal/#local-development","title":"Local Development","text":"<p>This project leverage Makefiles for project automation. We currently support the following commands.</p> <p>Lint the project with <code>ESLint</code>. </p> <pre><code>make lint\n</code></pre> <p>Build the project with <code>Typescript</code>. </p> <pre><code>make build.\n</code></pre>"},{"location":"internal/readme-internal/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>The below instructions apply regardless of whether PR is submitted from a fork or a branch.</p> <ol> <li>Make sure you IDE is configured to format modified lines only. Submitting fully formatted files makes it very hard to review, and such will be rejected.</li> <li>Tab size is 4 and spaces (be mindful that VSCode may not be the only IDE used). </li> <li>he following commands produce no errors and/or warnings:</li> </ol> <pre><code>npm i\nmake build\nmake lint\nmake run-test\ncdk list\n</code></pre>"},{"location":"internal/readme-internal/#triggering-e2e-testing","title":"Triggering E2E Testing","text":"<p>The CI system attached to the project will run all stacks under examples as end-to-end integration testing. </p> <p>Currently it works the following way:</p> <ul> <li>A human maintainer reviews the pr code to ensure it is not malicious </li> <li>If the code is trusted and the maintainer wishes to run e2e tests, they comment on the pr with /do-e2e-tests. This will trigger the build and test. Any visibility into the state can only occur through AWS maintainers.</li> <li>If job succeeds, the CI bot approves the PR. If it fails it requests changes. Details on what failed will need to be manually shared with external contributors.</li> <li>At present shapirov103, kcoleman731 and askulkarni2 have rights to invoke the bot. </li> </ul>"},{"location":"internal/readme-internal/#publishing","title":"Publishing","text":"<p>At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs.</p> <ol> <li>Change version in package.json. We are currently using .., e.g. 0.1.5 <li>Patch version increment must be used for bug fixes, including changes in code and missing documentation.</li> <li>Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes.</li> <li>Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation).</li> <li>Publishing (if not applied through CI):</li> <li><code>make build</code> (compile)</li> <li><code>npm publish</code> (this will require credentials to npm)</li>"},{"location":"internal/readme-internal/#submitting-changes","title":"Submitting Changes","text":"<p>For   direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch.  3. After review if approved changes will be merged.</p> <p>For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following:    1. Clear description of the feature    2. Test coverage    3. Validation instructions</p>"},{"location":"internal/readme-internal/#validations-framework-link-to-readme","title":"Validations framework link to readme","text":"<p>See this document for more details. </p>"},{"location":"resource-providers/","title":"Resource Providers","text":""},{"location":"resource-providers/#terminology","title":"Terminology","text":"<p>Resource A resource is a CDK construct that implements <code>IResource</code> interface from <code>aws-cdk-lib</code> which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 <code>IHostedZone</code>, an ACM certificate <code>ICertificate</code>, a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams.</p> <p>ResourceProvider A resource provider is a core Blueprints concept that enables customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint.</p>"},{"location":"resource-providers/#use-cases","title":"Use Cases","text":"<p><code>ClusterAddOn</code> and <code>Team</code> implementations require AWS resources that can be shared across several constructs. For example, <code>ExternalDnsAddOn</code> requires an array of hosted zones that will be used for integration with Route53. <code>NginxAddOn</code> requires a certificate and hosted zone (for DNS validation) in order to use TLS termination. VPC may be used inside add-ons and team constructs to look up VPC CIDR and subnets.</p> <p>The Blueprints framework provides ability to register a resource provider under an arbitrary name and make it available in the resource context, which is available to all add-ons and teams. With this capability, customers can either use existing resource providers or create their own and reference the provided resources inside add-ons, teams or other resource providers.</p> <p>Resource providers may depend on resources provided by other resource providers. For example, <code>CertificateResourceProvider</code> relies on a hosted zone resource, which is expected to be supplied by another provider.</p> <p>Example use cases:</p> <ol> <li> <p>As a platform user, I must create a VPC using my enterprise standards and leverage it for the EKS Blueprint. Solution: create an implementation of <code>ResourceProvider&lt;IVpc&gt;</code> (or leverage an existing one) and register it with the blueprint (see Usage).</p> </li> <li> <p>As a platform user, I need to use an existing hosted zone for all external DNS names used with ingress objects of my workloads. Solution: use a predefined <code>ImportHostedZoneProvider</code> or <code>LookupHostedZoneProvider</code> to reference the existing hosted zone.</p> </li> <li> <p>As a platform user, I need to create an S3 bucket and use it in one or more <code>Team</code> implementations. Solution: create an implementation for an S3 Bucket resource provider and use the supplied resource inside teams.</p> </li> </ol>"},{"location":"resource-providers/#contracts","title":"Contracts","text":"<p>The API contract for a resource provider is represented by the <code>ResourceProvider</code> interface from the <code>spi/resource-contracts</code> module.</p> <pre><code>export declare interface ResourceProvider&lt;T extends IResource = IResource&gt; {\n    provide(context: ResourceContext): T;\n}\n</code></pre> <p>Example implementations:</p> <pre><code>class VpcResourceProvider implements ResourceProvider&lt;IVpc&gt; {\n    provide(context: ResourceContext): IVpc {\n        const scope = context.scope; // stack\n        ...\n    }\n}\n\nclass DynamoDbTableResourceProvider implements ResourceProvider&lt;ITable&gt; {\n    provide(context: ResourceContext): ITable {\n        ...\n    }\n}\n\n/**\n * Example implementation of a VPC Provider that creates a NAT Gateway \n * which is available in all 3 AZs of the VPC while only being in one\n */\nclass OtherVpcResourceProvider implements ResourceProvider&lt;IVpc&gt; {\n    provide(context: ResourceContext): IVpc {\n        return new Vpc(context.scope, '&lt;vpc-name&gt;', {\n            availabilityZones: ['us-east-1a', 'us-east-1b', 'us-east-1c'], // VPC spans all AZs\n            subnetConfiguration: [{\n                cidrMask: 24,\n                name: 'private',\n                subnetType: SubnetType.PRIVATE_WITH_EGRESS\n            }, {\n                cidrMask: 24,\n                name: 'public',\n                subnetType: SubnetType.PUBLIC\n            }],\n            natGatewaySubnets: {\n                availabilityZones: ['us-east-1b'] // NAT gateway only in 1 AZ \n                subnetType: SubnetType.PUBLIC\n            }\n        });\n    }\n}\n</code></pre> <p>Access to registered resources from other resource providers and/or add-ons and teams:</p> <pre><code>/**\n * Provides API to register resource providers and get access to the provided resources.\n */\nexport class ResourceContext {\n\n    /**\n     * Adds a new resource provider and specifies the name under which the provided resource will be registered,\n     * @param name Specifies the name key under which the provided resources will be registered for subsequent look-ups.\n     * @param provider Implementation of the resource provider interface\n     * @returns the provided resource\n     */\n    public add&lt;T extends cdk.IResource = cdk.IResource&gt;(name: string, provider: ResourceProvider&lt;T&gt;) : T {\n        ...\n    }\n\n    /**\n     * Gets the provided resource by the supplied name. \n     * @param name under which the resource provider was registered\n     * @returns the resource or undefined if the specified resource was not found\n     */\n    public get&lt;T extends cdk.IResource = cdk.IResource&gt;(name: string) : T | undefined {\n        ...\n    }\n}\n</code></pre> <p>Convenience API to access registered resources from add-ons:</p> <pre><code>/**\n * Cluster info supplies all contextual information on the cluster configuration, registered resources and add-ons \n * which could be leveraged by the framework, add-on implementations and teams.\n */\nexport class ClusterInfo {\n    ...\n\n    /**\n     * Provides the resource context object associated with this instance of the EKS Blueprint.\n     * @returns resource context object\n     */\n    public getResourceContext(): ResourceContext {\n        return this.resourceContext;\n    }\n\n    /**\n     * Provides the resource registered under supplied name\n     * @param name of the resource to be returned\n     * @returns Resource object or undefined if no resource was found\n     */\n    public getResource&lt;T extends cdk.IResource&gt;(name: string): T | undefined {\n        ...\n    }\n\n    /**\n     * Same as {@link getResource} but will fail if the specified resource is not found\n     * @param name of the resource to be returned\n     * @returns Resource object (fails if not found)\n     */\n    public getRequiredResource&lt;T extends cdk.IResource&gt;(name: string): T {\n        ...\n    }\n}\n</code></pre>"},{"location":"resource-providers/#usage","title":"Usage","text":"<p>Registering Resource Providers for a Blueprint</p> <p>Note: <code>GlobalResources.HostedZone</code> and <code>GlobalResources.Certificate</code> are provided for convenience as commonly referenced constants. Full list of Resource Providers can be found here.</p> <pre><code>const myVpcId = ...;  // e.g. app.node.tryGetContext('my-vpc', 'default)  will look up property my-vpc in the cdk.json\n\nblueprints.EksBlueprint.builder()\n    //  Specify VPC for the cluster (if not set, a new VPC will be provisioned as per EKS Best Practices)\n    .resourceProvider(GlobalResources.VPC, new VpcProvider(myVpcId))\n    //  Specify KMS Key as cluster secrets encryption key\n    .resourceProvider(GlobalResources.KmsKey, new CreateKmsKeyProvider('my-alias-name'))    \n    //  Register hosted zone and give it a name of GlobalResources.HostedZone\n    .resourceProvider(GlobalResources.HostedZone, new ImportHostedZoneProvider('hosted-zone-id1', 'my.domain.com'))\n    .resourceProvider(\"internal-hosted-zone\", new ImportHostedZoneProvider('hosted-zone-id2', 'myinternal.domain.com'))\n    // Register certificate GlobalResources.Certificate name and reference the hosted zone registered in the previous step\n    .resourceProvider(GlobalResources.Certificate, new CreateCertificateProvider('domain-wildcard-cert', '*.my.domain.com', GlobalResources.HostedZone))\n    .resourceProvider(\"private-ca\", new CreateCertificateProvider('internal-wildcard-cert', '*.myinternal.domain.com', \"internal-hosted-zone\"))\n    // Create EFS file system and register it under the name of efs-file-system\n    .resourceProvider(\"efs-file-system\", new CreateEfsFileSystemProvider('efs-file-system'))\n    // Create an S3 bucket and register it\n    .resourceProvider('blueprint-s3', new blueprints.CreateS3BucketProvider({\n        name: `bucket-name`, // This bucket name must be globally unique \n        id: 'blueprints-s3-bucket-id',\n        s3BucketProps: { removalPolicy: cdk.RemovalPolicy.DESTROY }\n    }))\n    .addOns(new AwsLoadBalancerControllerAddOn())\n    // Use hosted zone for External DNS\n    .addOns(new ExternalDnsAddOn({hostedZoneResources: [GlobalResources.HostedZone]}))\n    // Use certificate registered before with NginxAddon\n    .addOns(new NginxAddOn({\n        certificateResourceName: GlobalResources.Certificate,\n        externalDnsHostname: 'my.domain.com'\n    }))\n    .teams(...)\n    .version(\"auto\")\n    .build(app, 'stack-with-resource-providers');\n</code></pre> <p>Registering Multiple Hosted Zones</p> <pre><code>blueprints.EksBlueprint.builder()\n    //  Register hosted zone1 under the name of MyHostedZone1\n    .resourceProvider(\"MyHostedZone1\", new ImportHostedZoneProvider('hosted-zone-id1', 'my.domain.com'))\n    // Register zone2 under the name of MyHostedZone2\n    .resourceProvider(\"MyHostedZone2\", new ImportHostedZoneProvider('hosted-zone-id2', 'my.otherdomain.com'))\n    // Register certificate and reference the hosted zone1 registered in the previous steps\n    .resourceProvider(\"MyCert\", new CreateCertificateProvider('domain-wildcard-cert', '*.my.domain.com', \"MyHostedZone1\"))\n    .addOns(new AwsLoadBalancerControllerAddOn())\n    // Use hosted zones for External DNS\n    .addOns(new ExternalDnsAddOn({hostedZoneResources: [\"MyHostedZone1\", \"MyHostedZone2\"]}))\n    // Use certificate registered before with NginxAddon\n    .addOns(new NginxAddOn({\n        certificateResourceName: \"MyCert\",\n        externalDnsHostname: 'my.domain.com'\n    }))\n    .teams(...)\n    .version(\"auto\")\n    .build(app, 'stack-with-resource-providers');\n</code></pre>"},{"location":"resource-providers/#using-resource-providers-with-cdk-constructs","title":"Using Resource Providers with CDK Constructs","text":"<p>Some constructs used in the <code>EKSBlueprint</code> stack are standard CDK constructs that accept CDK resources.</p> <p>For example, <code>GenericClusterProvider</code> (which is the basis for all cluster providers) allows passing resources like <code>IRole</code>, <code>SecurityGroup</code> and other properties that customers may find inconvenient to define with a builder pattern.</p> <p>Blueprints provide a convenience API to register such resources in a declarative manner.</p> <p>Example with an anonymous resource:</p> <pre><code>const clusterProvider = new blueprints.GenericClusterProvider({\n    version: KubernetesVersion.V1_29,\n    mastersRole: blueprints.getResource(context =&gt; { // will generate a unique name for resource. designed for cases when resource is defined once and needed in a single place.\n        return new iam.Role(context.scope, 'AdminRole', { assumedBy: new AccountRootPrincipal() });\n    }),\n    managedNodeGroups: [\n        ...\n    ]\n});\n\nblueprints.EksBlueprint.builder()\n    .addOns(...addOns)\n    .clusterProvider(clusterProvider)\n    .version(\"auto\")\n    .build(scope, blueprintID, props);\n</code></pre> <p>Example with a named resource:</p> <pre><code>const clusterProvider = new blueprints.GenericClusterProvider({\n    version: KubernetesVersion.V1_29,\n    mastersRole: blueprints.getNamedResource(\"my-role\") as iam.Role,\n    managedNodeGroups: [\n        ...\n    ]\n});\n\nblueprints.EksBlueprint.builder()\n    .resourceProvider(\"my-role\", new blueprints.LookupRoleProvider(\"SomeExistingRole\")) // enables to look up this role from ClusterInfo under \"my-role\" in add-ons, etc.\n    .addOns(...addOns)\n    .clusterProvider(clusterProvider)\n    .version(\"auto\")\n    .build(scope, blueprintID, props);\n</code></pre>"},{"location":"resource-providers/#implementing-custom-resource-providers","title":"Implementing Custom Resource Providers","text":"<ol> <li>Select the type of the resource that you need. Let's say it will be an FSx File System. Note: it must be one of the derivatives/implementations of <code>IResource</code> interface.</li> <li>Implement <code>ResourceProvider</code> interface:</li> </ol> <pre><code>class MyResourceProvider implements blueprints.ResourceProvider&lt;fsx.IFileSystem&gt; {\n    provide(context: blueprints.ResourceContext): s3.IBucket {\n        return new fsx.LustreFileSystem(context.scope, \"FsxLustreFileSystem\");\n    }\n}\n</code></pre> <ol> <li>Register your resource provider under an arbitrary name which must be unique in the current scope across all resource providers:</li> </ol> <pre><code>blueprints.EksBlueprint.builder()\n    .resourceProvider(\"FsxLustreFileSystem\" ,new MyResourceProvider())\n    .addOns(...)\n    .teams(...)\n    .version(\"auto\")\n    .build();\n</code></pre> <ol> <li>Use the resource inside a custom add-on:</li> </ol> <pre><code>class MyCustomAddOn implements blueprints.ClusterAddOn {\n    deploy(clusterInfo: ClusterInfo): void | Promise&lt;cdk.Construct&gt; {\n        const myFsxfileSystem: fsx.LustreFileSystem = clusterInfo.getRequiredResource('FsxLustreFileSystem'); // will fail if the file system does not exist\n        // do something with the file system\n    }\n}\n</code></pre>"},{"location":"resource-providers/amp-provider/","title":"AMP Resource Provider","text":""},{"location":"resource-providers/amp-provider/#amp-resource-provider","title":"Amp Resource Provider","text":""},{"location":"resource-providers/amp-provider/#createampprovider","title":"CreateAmpProvider","text":"<p>Creates a new AMP Workspace with the provided AMP Workspace name and tags and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(\"amp-workspace\", new blueprints.CreateAmpProvider(\"amp-workspace\", \"&lt;workspace-name&gt;\", [{key:'key', value:'value'}, ...]))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/certificate-providers/","title":"Certificate Resource Providers","text":""},{"location":"resource-providers/certificate-providers/#certificate-resource-providers","title":"Certificate Resource Providers","text":""},{"location":"resource-providers/certificate-providers/#importcertificateprovider","title":"ImportCertificateProvider","text":"<p>Imports certificate by its ARN and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(\"test-cert\", new blueprints.ImportCertificateProvider(\"arn:aws:acm:&lt;region&gt;:&lt;account&gt;:certificate/&lt;cert-id&gt;\", \"test-cert\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/certificate-providers/#createcertificateprovider","title":"CreateCertificateProvider","text":"<p>Creates a new certificate for the specified domain and makes it available to the blueprint constructs under the provided name.  Depends on a hosted zone to be registered for validation.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(\"internal-zone\", new blueprints.ImportHostedZoneProvider('hosted-zone-id', \"internal.domain.com\"))\n  .resourceProvider(\"test-cert\", new blueprints.CreateCertificateProvider(\"test-cert\", \"*.internal.domain.com\", \"internal-zone\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/efs-providers/","title":"EFS Resource Providers","text":""},{"location":"resource-providers/efs-providers/#efs-resource-providers","title":"EFS Resource Providers","text":""},{"location":"resource-providers/efs-providers/#createefsfilesystemprovider","title":"CreateEfsFileSystemProvider","text":"<p>Creates a new EFS file system with the provided properties and makes it available to the blueprint constructs under the provided name. Depends on a VPC resource provider.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(GlobalResources.Vpc, new blueprints.VpcProvider())\n  .resourceProvider(\"efs-file-system\", new blueprints.CreateEfsFileSystemProvider({name: \"efs-file-system\"}))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/efs-providers/#lookupefsfilesystem","title":"LookupEfsFileSystem","text":"<p>Looks for an existing EFS file system by the given name and id and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(\"efs-file-system\", new blueprints.LookupEfsFileSystem({name: \"efs-file-system\", fileSystemId: \"efs-fs-id\"}))\n  ...\n  build();\n</code></pre></p>"},{"location":"resource-providers/hosted-zone-providers/","title":"Hosted Zone Resource Providers","text":""},{"location":"resource-providers/hosted-zone-providers/#hosted-zone-resource-providers","title":"Hosted Zone Resource Providers","text":""},{"location":"resource-providers/hosted-zone-providers/#lookuphostedzoneprovider","title":"LookupHostedZoneProvider","text":"<p>Looks up a hosted zone based on name and optional id and makes it available to the blueprint constructs under the provided name. </p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(\"internal-zone-1\", new blueprints.LookupHostedZone('internal-zone', \"hosted-zone-id\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/hosted-zone-providers/#importhostedzoneprovider","title":"ImportHostedZoneProvider","text":"<p>Directly imports a hosted zone based on id and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(\"internal-zone-2\", new blueprints.ImportHostedZoneProvider('hosted-zone-id', \"internal.domain.com\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/hosted-zone-providers/#delegatinghostedzoneprovider","title":"DelegatingHostedZoneProvider","text":"<p>Convenient approach to create a global hosted zone record in a centralized account and subdomain records in workload accounts and make it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>const props: blueprints.DelegatingHostedZoneProviderProps = {\n  parentDomain: \"domain.com\", \n  subdomain: \"sub.domain.com\", \n  parentDnsAccountId: \"&lt;account-id&gt;\",\n  delegatingRoleName: \"&lt;IAM-Role-Name&gt;\", // must have trust relationship with workload account where blueprint is provisioned\n};\nblueprints.EksBlueprint.builder()\n  .resourceProvider(\"internal-zone-3\", new blueprints.DelegationHostedZoneProvider(props))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/iam-role-providers/","title":"IAM Role Resource Providers","text":""},{"location":"resource-providers/iam-role-providers/#iam-role-resource-providers","title":"IAM Role Resource Providers","text":""},{"location":"resource-providers/iam-role-providers/#createroleprovider","title":"CreateRoleProvider","text":"<p>Creates a new role based on the given role id, principal, and policies and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider('service-role', \n    new blueprints.CreateRoleProvider(\n      'ServiceRole', \n      new iam.ServicePrincipal('ec2.amazonaws.com'), \n      [iam.ManagedPolicy.fromAwsManagedPolicyName(\"AWSEC2SpotServiceRolePolicy\")])\n  )\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/iam-role-providers/#lookuproleprovider","title":"LookupRoleProvider","text":"<p>Looks up role by name and makes it available to the blueprint constructs under the provided name.  The role will be looked up in the account where the blueprint is deployed, so if a blueprint is deployed in multiple accounts, each account must have the role defined.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider('service-role', new blueprints.LookupRoleProvider(\"RoleName\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/kms-key-providers/","title":"KMS Key Resource Providers","text":""},{"location":"resource-providers/kms-key-providers/#kms-key-resource-providers","title":"KMS Key Resource Providers","text":""},{"location":"resource-providers/kms-key-providers/#createkmskeyprovider","title":"CreateKmsKeyProvider","text":"<p>Creates a KMS Key with the optionally provided aliasName and properties and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(GlobalResources.KmsKey, new blueprints.CreateKmsKeyProvider(\"alias-name\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/kms-key-providers/#lookupkmskeyprovider","title":"LookupKmsKeyProvider","text":"<p>Looks up a KMS Key based on the alias provided and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(GlobalResources.KmsKey, new blueprints.LookupKmsKeyProvider(\"alias-name\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/s3-providers/","title":"S3 Bucket Resource Providers","text":""},{"location":"resource-providers/s3-providers/#s3-bucket-resource-providers","title":"S3 Bucket Resource Providers","text":""},{"location":"resource-providers/s3-providers/#creates3bucketprovider","title":"CreateS3BucketProvider","text":"<p>Creates a new S3 bucket and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprints.builder()\n  .resourceProvider(\"s3-bucket\", new blueprints.CreateS3BucketProvider({\n    name: 'unique-bucket-name',\n    id: 's3-bucket-id',\n    s3BucketProps: {removalPolicy: cdk.RemovalPolicy.DESTROY},\n  }))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/s3-providers/#imports3bucketprovider","title":"ImportS3BucketProvider","text":"<p>Looks up a bucket by name and optional id and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprints.builder()\n  .resourceProvider(\"s3-bucket\", new blueprints.ImportS3BucketProvider(\"bucket-name\", 'bucket-id'))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/vpc-providers/","title":"VPC Resource Providers","text":""},{"location":"resource-providers/vpc-providers/#vpc-resourcer-providers","title":"VPC Resourcer Providers","text":""},{"location":"resource-providers/vpc-providers/#vpcprovider","title":"VpcProvider","text":"<p>If given a VPC id, looks up the corresponding VPC and provides it. Otherwise, creates a new VPC with the optionally provided CIDRs and subnet CIDRs or the default values and makes it available to the blueprint constructs under the provided name.  If the supplied id is default, it will look up the default vpc.</p> <p>Example Implementations: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(GlobalResources.Vpc, new blueprints.VpcProvider({primaryCidr: \"10.0.0.0/16\"}))\n  ...\n  .build();\n</code></pre></p> <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider(GlobalResources.Vpc, new blueprints.VpcProvider(\"&lt;vpc-id&gt;\"))\n  ...\n  .build();\n</code></pre>"},{"location":"resource-providers/vpc-providers/#directvpcprovider","title":"DirectVpcProvider","text":"<p>Provides the given VPC to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>const app = new cdk.App();\n\nexport class VPCStack extends cdk.Stack {\n  public readonly vpc: ec2.Vpc;\n\n  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n\n    this.vpc = new ec2.Vpc(this, 'eks-blueprint-vpc');\n  }\n}\n\nconst vpcStack = new VPCStack(app, 'eks-blueprint-vpc', { env: { account, region } });\n\nblueprints.EksBlueprint.builder()\n  .resourceProvider(GlobalResources.Vpc, new blueprints.DirectVpcProvider(vpcStack.vpc))\n  ...\n  .build();\n</code></pre></p>"},{"location":"resource-providers/vpc-providers/#lookupsubnetprovider","title":"LookupSubnetProvider","text":"<p>Directly imports a secondary subnet provider based on id and makes it available to the blueprint constructs under the provided name.</p> <p>Example Implementation: <pre><code>blueprints.EksBlueprint.builder()\n  .resourceProvider('my-subnet', new blueprints.LookupSubnetProvider(\"subnet-id\"))\n  ...\n  .build();\n</code></pre></p>"},{"location":"teams/aws-batch-on-eks-team/","title":"AWS Batch on EKS Team","text":"<p>The <code>AWS Batch on EKS Team</code> extends the <code>ApplicationTeam</code> and allows the Batch on EKS team to manage the namespace where the Batch Jobs are deployed. This team MUST be used in conjuction with EMR on EKS AddOn.</p> <p>The AWS Batch on EKS Team allows you to create a Compute Environment and a job queue to attach to the compute environment. Job queues are where jobs are submitted, and where they reside until they can be scheduled in the compute environment.</p>"},{"location":"teams/aws-batch-on-eks-team/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.AwsBatchAddOn();\n\nconst batchTeam: BatchEksTeamProps = {\n    name: 'batch-a',\n    namespace: 'aws-batch',\n    envName: 'batch-a-comp-env',\n    computeResources: {\n        envType: BatchEnvType.EC2,\n        allocationStrategy: BatchAllocationStrategy.BEST,\n        priority: 10,\n        minvCpus: 0,\n        maxvCpus: 128,\n        instanceTypes: [\"m5\", \"t3.large\"]\n    },\n    jobQueueName: 'team-a-job-queue',\n};\n\nconst blueprint = blueprints.EksBlueprint.builder()\n    .version(\"auto\")\n    .addOns(addOn)\n    .teams(new blueprints.BatchEksTeam(batchTeam))\n    .build(app, 'my-stack-name');\n</code></pre>"},{"location":"teams/aws-batch-on-eks-team/#create-a-job-definition","title":"Create a Job definition","text":"<p>Once you deploy the addon and the team, to run a batch job on EKS, you must first define a job. AWS Batch job definitions specify how jobs are to be run. The following is an example job definition you can set using AWS CLI:</p> <pre><code>cat &lt;&lt;EOF &gt; ./batch-eks-job-definition.json\n{\n  \"jobDefinitionName\": \"MyJobOnEks_Sleep\",\n  \"type\": \"container\",\n  \"eksProperties\": {\n    \"podProperties\": {\n      \"hostNetwork\": true,\n      \"containers\": [\n        {\n          \"image\": \"public.ecr.aws/amazonlinux/amazonlinux:2\",\n          \"command\": [\n            \"sleep\",\n            \"60\"\n          ],\n          \"resources\": {\n            \"limits\": {\n              \"cpu\": \"1\",\n              \"memory\": \"1024Mi\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\nEOF\naws batch register-job-definition --cli-input-json file://./batch-eks-job-definition.json\n</code></pre>"},{"location":"teams/aws-batch-on-eks-team/#submit-a-job","title":"Submit a Job","text":"<p>Using the job definition, you can define and deploy a specific job using the following example AWS CLI command:</p> <pre><code>aws batch submit-job --job-queue team-a-job-queue \\\n    --job-definition MyJobOnEks_Sleep --job-name My-Eks-Job1\n</code></pre> <p>You will get an output that lists the Job ID:</p> <pre><code>{\n    \"jobArn\": \"arn:aws:batch:us-west-2:123456789012:job/9518c9eb-b261-4732-a38d-54caf1d22229\",\n    \"jobName\": \"My-Eks-Job1\",\n    \"jobId\": \"9518c9eb-b261-4732-a38d-54caf1d22229\"\n}\n</code></pre>"},{"location":"teams/aws-batch-on-eks-team/#verify-job-completion","title":"Verify Job completion","text":"<p>You can see the job by running the following command:</p> <pre><code>aws batch describe-jobs --job &lt;jobId-from-submit-response&gt; \n</code></pre> <pre><code>{\n    \"jobs\": [\n        {\n            \"jobArn\": \"arn:aws:batch:us-west-2:123456789012:job/9518c9eb-b261-4732-a38d-54caf1d22229\",\n            \"jobName\": \"My-Eks-Job1\",\n            \"jobId\": \"9518c9eb-b261-4732-a38d-54caf1d22229\",\n            \"jobQueue\": \"arn:aws:batch:us-west-2:123456789012:job-queue/team-a-job-queue\",\n            \"status\": \"RUNNABLE\",\n            \"attempts\": [],\n            \"createdAt\": 1676581820093,\n            \"dependsOn\": [],\n            \"jobDefinition\": \"arn:aws:batch:us-west-2:123456789012:job-definition/MyJobOnEks_Sleep:1\",\n            \"parameters\": {},\n            \"tags\": {},\n            \"platformCapabilities\": [],\n            \"eksProperties\": {\n                \"podProperties\": {\n                    \"hostNetwork\": true,\n                    \"containers\": [\n                        {\n                            \"image\": \"public.ecr.aws/amazonlinux/amazonlinux:2\",\n                            \"command\": [\n                                \"sleep\",\n                                \"60\"\n                            ],\n                            \"args\": [],\n                            \"env\": [],\n                            \"resources\": {\n                                \"limits\": {\n                                    \"memory\": \"1024Mi\",\n                                    \"cpu\": \"1\"\n                                }\n                            },\n                            \"volumeMounts\": []\n                        }\n                    ],\n                    \"volumes\": []\n                }\n            },\n            \"eksAttempts\": []\n        }\n    ]\n}\n</code></pre> <p>After a while, you can check that the pod has been created (and eventually deleted after job completion) under the <code>aws-batch</code> namespace to run the job:</p> <pre><code>kubectl get pod -n aws-batch \n</code></pre> <p>You can also check that the job has been completed by running the describe job command again and seeing the output. There should be description of the pod and node assignment for the job under <code>eksAttempts</code>:</p> <pre><code>.......................\n            \"eksAttempts\": [\n                {\n                    \"containers\": [\n                        {\n                            \"exitCode\": 0,\n                            \"reason\": \"Completed\"\n                        }\n                    ],\n                    \"podName\": \"aws-batch.fa024ec9-4232-3e82-b09b-4ba6ba396ec2\",\n                    \"nodeName\": \"ip-10-0-81-102.us-west-2.compute.internal\",\n                    \"startedAt\": 1676581976000,\n                    \"stoppedAt\": 1676582036000\n                }\n            ]\n.......................\n</code></pre>"},{"location":"teams/bedrock-team/","title":"Bedrock Team","text":"<p>The <code>BedrockTeam</code> extends the <code>ApplicationTeam</code> and allows the Bedrock team to manage the namespace where the generative AI workloads can be deployed. This team MUST be used in conjuction with Bedrock Builder.</p> <p>The Bedrock Team allows you to create an IRSA to allow pods in namespace specified by the user to access Amazon Bedrock.</p>"},{"location":"teams/bedrock-team/#usage","title":"Usage","text":"<pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst bedrockTeamProps: blueprints.BedrockTeamProps = {\n  namespace: 'bedrock',\n  createNamespace: true,\n  serviceAccountName: 'bedrock-service-account',\n};\n\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .teams(new blueprints.BedrockTeam(bedrockTeamProps))\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"teams/emr-eks-team/","title":"EMR on EKS Team","text":"<p>The <code>EMR on EKS Team</code> extends the <code>ApplicationTeam</code> and allows the EMR on EKS team to manage the namespace where the virtual cluster is deployed. This team MUST be used in conjuction with EMR on EKS AddOn.</p> <p>The EMR on EKS Team allows you to create a Virtual Cluster and job Execution Roles that are used by the job to access data in Amazon S3, AWS Glue Data Catalog or any other AWS resources that you need to interact with. The job execution roles are scoped with IRSA to be only assumed by pods deployed by EMR on EKS in the namespace of the virtual cluster. You can learn more about the condition applied here. The IAM roles will have the following format: <code>NAME-AWS_REGION-EKS_CLUSTER_NAME</code>.</p>"},{"location":"teams/emr-eks-team/#usage","title":"Usage","text":"<pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst addOn = new blueprints.addons.EmrEksAddOn();\n\n//The policy to be attached to the EMR on EKS execution role \nconst executionRolePolicyStatement: PolicyStatement [] = [\n            new PolicyStatement({\n              resources: ['*'],\n              actions: ['s3:*'],\n            }),\n            new PolicyStatement({\n              resources: ['*'],   \n              actions: ['glue:*'],\n            }),\n            new PolicyStatement({\n              resources: ['*'],\n              actions: [\n                'logs:*',\n              ],\n            }),\n          ];\n\n      const dataTeam: EmrEksTeamProps = {\n              name:'dataTeam',\n              virtualClusterName: 'batchJob',\n              virtualClusterNamespace: 'batchjob',\n              createNamespace: true,\n              users: [\n                new ArnPrincipal(`arn:aws:iam::${YOUR_IAM_ACCOUNT}:user/user1`),\n                new ArnPrincipal(`arn:aws:iam::${YOUR_IAM_ACCOUNT}:user/user2`)\n              ],\n              userRoleArn: new ArnPrincipal(`arn:aws:iam::${YOUR_IAM_ACCOUNT}:role/role1`),\n              executionRoles: [\n                  {\n                      executionRoleIamPolicyStatement: executionRolePolicyStatement,\n                      executionRoleName: 'myBlueprintExecRole'\n                  }\n              ]\n          };\n\n\nconst blueprint = blueprints.EksBlueprint.builder()\n  .version(\"auto\")\n  .addOns(addOn)\n  .teams(new blueprints.EmrEksTeam(dataTeam))\n  .build(app, 'my-stack-name');\n</code></pre>"},{"location":"teams/emr-eks-team/#submit-a-job","title":"Submit a job","text":"<p>Once you deploy the blueprint you will have as output the Virtual Cluster <code>id</code>. You can use the <code>id</code> and the execution role for which you supplied a policy to submit jobs. Below you can find an example of a job you can submit with AWS CLI.</p> <pre><code>aws emr-containers start-job-run \\\n  --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \\\n  --name=pi-2 \\\n  --execution-role-arn=$EMR_ROLE_ARN \\\n  --release-label=emr-6.8.0-latest \\\n  --job-driver='{\n    \"sparkSubmitJobDriver\": {\n      \"entryPoint\": \"local:///usr/lib/spark/examples/src/main/python/pi.py\",\n      \"sparkSubmitParameters\": \"--conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1\"\n    }\n  }'\n</code></pre>"},{"location":"teams/emr-eks-team/#verify-job-submission","title":"Verify job submission","text":"<p>Once you submit a job you can verify that it is running from the AWS console on the EMR service that the job is running. Below you can see a screenshot of the console.</p> <p></p>"},{"location":"teams/teams/","title":"Teams","text":"<p>The <code>eks-blueprints</code> framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two <code>Team</code> types: <code>ApplicationTeam</code> and <code>PlatformTeam</code>. <code>ApplicationTeam</code> represent teams managing workloads running in cluster namespaces and <code>PlatformTeam</code> represents platform administrators who have admin access (masters group) to clusters.</p> <p>You are also able to create your own team implementations by creating classes that inherits from <code>Team</code>.</p>"},{"location":"teams/teams/#applicationteam","title":"ApplicationTeam","text":"<p>To create an <code>ApplicationTeam</code> for your cluster, simply implement a class that extends <code>ApplicationTeam</code>. You will need to supply a team name, an array of users, and (optionally) a directory where you may optionally place any policy definitions and generic manifests for the team. These manifests will be applied by the platform and will be outside of the team control NOTE: When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings in the yaml files.</p> <pre><code>export class TeamAwesome extends ApplicationTeam {\n    constructor(app: App) {\n        super({\n            name: \"team-awesome\",\n            users: [\n                new ArnPrincipal(`arn:aws:iam::${YOUR_IAM_ACCOUNT}:user/user1`),  \n                new ArnPrincipal(`arn:aws:iam::${YOUR_IAM_ACCOUNT}:user/user2`)\n            ],\n            teamManifestDir: './examples/teams/team-awesome/'\n        });\n    }\n}\n</code></pre> <p>The <code>ApplicationTeam</code> will do the following:</p> <ul> <li>Create a namespace</li> <li>Register quotas</li> <li>Register IAM users for cross-account access</li> <li>Create a shared role for cluster access. Alternatively, an existing role can be supplied.</li> <li>Register provided users/role in the <code>awsAuth</code> map for <code>kubectl</code> and console access to the cluster and namespace.</li> <li>(Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and applies them.</li> </ul>"},{"location":"teams/teams/#platformteam","title":"PlatformTeam","text":"<p>To create an <code>PlatformTeam</code> for your cluster, simply implement a class that extends <code>PlatformTeam</code>. You will need to supply a team name and an array of users.  </p> <p><pre><code>export class TeamAwesome extends PlatformTeam {\n    constructor(app: App) {\n        super({\n            name: \"team-awesome\",\n            users: [\n                new ArnPrincipal(`arn:aws:iam::${YOUR_IAM_ACCOUNT}:user/user1`),  \n                new ArnPrincipal(`arn:aws:iam::${YOUR_IAM_ACCOUNT}:user/user2`)\n            ]\n\n        });\n    }\n}\n</code></pre> The <code>PlatformTeam</code> class does the following:</p> <ul> <li>registers IAM users for admin access to the cluster (<code>kubectl</code> and console)</li> <li>registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role</li> </ul> <p>To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example:</p> <p><pre><code>const adminTeam = new PlatformTeam( {\n    name: \"second-adminteam\", // make sure this is unique within organization\n    userRoleArn: `${YOUR_ROLE_ARN}`;\n})\n</code></pre> NOTE: <code>YOUR_ROLE_ARN</code> should be stripped of any path component, as explained in the User Guide. So, for example, <code>arn:aws:iam::111122223333:role/team/developers/eks-admin</code> should be changed to <code>arn:aws:iam::111122223333:role/eks-admin</code>.</p>"},{"location":"teams/teams/#defaultteamroles","title":"DefaultTeamRoles","text":"<p>The <code>DefaultTeamRoles</code> class provides default RBAC configuration for <code>ApplicationTeams</code>:</p> <ul> <li>Cluster role, group identity and cluster role bindings to view nodes and namespaces</li> <li>Namespace role and role binding for the group to view pods, deployments, daemonsets, services</li> </ul>"},{"location":"teams/teams/#team-benefits","title":"Team Benefits","text":"<p>By managing teams via infrastructure as code, we achieve the following benefits:</p> <ol> <li>Self-documenting code</li> <li>Centralized logic related to the team</li> <li>Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets</li> <li>IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name.</li> </ol> <p>The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access. </p>"},{"location":"teams/teams/#cluster-access-kubectl","title":"Cluster Access (<code>kubectl</code>)","text":"<p>The stack output will contain the <code>kubeconfig</code> update command, which should be shared with the development and platform teams.</p> <pre><code>${teamname}teamrole arn:aws:iam::${account}:role/west-dev-${teamname}AccessRole3CDA6927-1QA4S3TYMY36N\n\nplatformteamadmin   arn:aws:iam::${account}:role/west-dev-${platform-team-name}AccessRole57468BEC-8JYMM0HZZ2CE  \n\nteamtroisaiamrole   arn:aws:iam::${account}:role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46\n\nwestdevConfigCommand1AE70258    aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam::${account}:role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08\n</code></pre> <p>Note the last command is to update <code>kubeconfig</code> with the proper context to access cluster using <code>kubectl</code>. The last argument of this command is <code>--role-arn</code> which by default is set to the cluster master role. </p> <p>Developers (members of each team) should use the role name for the team role, such as <code>burnhamteamrole</code> for team name <code>burnham</code>.  Platform administrators must use the role output for their team name, such as platformteamadmin in the above example.</p>"},{"location":"teams/teams/#console-access","title":"Console Access","text":"<p>Provided that each team has received the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. </p> <p>To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team. </p>"},{"location":"teams/teams/#examples","title":"Examples","text":"<p>There are a few team examples under /teams folder.</p> <p>The example for team-burnham includes a way to specify IAM users through a local or project CDK context.  Project context is defined in <code>cdk.json</code> under context key and local context is defined in <code>~/.cdk.json</code> under context key. </p> <p>Example:</p> <pre><code>\u279c cat ~/.cdk.json \n{\n    \"context\": {\n        \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\"\n    }\n}\n</code></pre>"}]}